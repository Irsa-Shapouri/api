Link,Title,Text,Image,Date Publish
https://www.forbes.com/sites/lanceeliot/2024/01/03/applying-the-renowned-turing-test-to-generative-ai-that-empowers-mental-health-therapy-advisement-to-see-what-happens/,Applying The Renowned Turing Test To Generative AI That Empowers Mental Health Therapy Advisement To See What Happens,"Keenly applying the famous Turing Test to the nature of generative AI mental health advisement apps. getty
In today’s column, I will be continuing my ongoing analysis of how generative AI is making an impact in mental health therapy advisement. One question that I often get asked while speaking at conferences is how we might assess the advent of mental health therapy apps that are powered by generative AI. These apps seemingly provide mental health advice and are claimed by some to be able to work on a level comparable to human mental health professionals.
A logical question then arises as to whether generative AI can perform on par with that of human mental health professionals.
I typically launch earnestly into a discussion about a renowned and pertinent test known as the Turing Test and explain its purpose and functionality, along with identifying various ways in which it can be directly applied to judging the slew of generative AI mental health guidance apps that are arising. There are pros and cons to using the Turing Test as a means of assessing AI. The approach has been around since the 1950s and to some degree has stood the test of time, though a heated debate about its current veracity has now ensued due to the emergence of modern-day generative AI.
I’m betting that you’ll find highly informative a quick rundown about the Turing Test along with an indication of how it applies to AI-powered mental health therapy apps. These are keystone principles for anyone keenly interested in the topic of AI overall and especially for those focused on the use of generative AI for mental health advisement.
Before I dive into today’s particular topic, I’d like to provide a quick background for you so that you’ll have a suitable context about the arising use of generative AI for mental health advisement purposes. I’ve mentioned this in prior columns and believe the contextual establishment is essential overall. If you are already familiar with the overarching background on this topic, you are welcome to skip down below to the next section of this discussion.
Background About Generative AI In Mental Health Treatment
The use of generative AI for mental health treatment is a burgeoning area of tremendously significant societal ramifications. We are witnessing the adoption of generative AI for providing mental health advice on a widescale basis, yet little is known about whether this is beneficial to humankind or perhaps contrastingly destructively adverse for humanity.
Some would affirmatively assert that we are democratizing mental health treatment via the impending rush of low-cost always-available AI-based mental health apps. Others sharply decry that we are subjecting ourselves to a global wanton experiment in which we are the guinea pigs. Will these generative AI mental health apps steer people in ways that harm their mental health? Will people delude themselves into believing they are getting sound mental health advice, ergo foregoing treatment by human mental therapists, and become egregiously dependent on AI that at times has no demonstrative mental health improvement outcomes?
Hard questions are aplenty and not being given their due airing.
Furthermore, be forewarned that it is shockingly all too easy nowadays to craft a generative AI mental health app, and just about anyone anywhere can do so, including while sitting at home in their pajamas and not knowing any bona fide substance about what constitutes suitable mental health therapy. Via the use of what are referred to as establishing prompts, it is easy-peasy to make a generative AI app that purportedly gives mental health advice. No coding is required, and no software development skills are needed.
We sadly are faced with a free-for-all that bodes for bad tidings, mark my words.
I’ve been hammering away at this topic and hope to raise awareness about where we are and where things are going when it comes to the advent of generative AI mental health advisement uses. If you’d like to get up-to-speed on my prior coverage of generative AI across a wide swath of the mental health sphere, you might consider for example these cogent analyses:
Turing Test As A Crucial Tool In The AI Field
Here’s how I will approach today’s discussion.
First, I will introduce you to the legendary Turing Test. After covering vital fundamental aspects, you will be ready to consider how the Turing Test can be applied to the realm of generative AI-empowered mental health guidance. I will walk you through the ins and outs accordingly. This will include showcasing some examples via the use of the widely and wildly popular generative AI app known as ChatGPT. You undoubtedly have heard of or perhaps used ChatGPT. According to the AI maker OpenAI, there are purportedly 100 million weekly active users utilizing ChatGPT. The precepts that I am covering are readily applicable to other generative AI apps such as GPT-4, Bard, Gemini, Claude, and so on.
Prepare yourself for quite a ride.
Let’s begin at the beginning.
The famous mathematician extraordinaire Alan Turing in 1950 published an important paper that laid out an approach he referred to as the imitation game which has become popularly known as the Turing Test. His 1950 paper was entitled “Computing Machinery and Intelligence” by Alan Turing and was published in the journal Mind. For my extensive analysis of the Turing Test, see the link here. I will briefly outline the Turing Test in this discussion so that you will be familiar with the crux of the topic.
In his paper, Alan Turing noted that he had been asked repeatedly about whether machines or AI would someday be able to think as humans do. One means of answering the question involves ascertaining how humans think, such as by somehow reverse engineering the brain and the human mind. This is quite a difficult problem and one that has yet to be fully figured out. Turing realized that maybe another approach to the matter was required. A more viable and immediately practical approach.
Perhaps we could take an outside-in perspective rather than an inside-out angle, whereby we treat the human mind as a kind of black box. This helps to then set aside those unresolved questions about how the inner workings function. The conception is that we might be able to compare machines and AI to whatever human thinking exhibits and seek to suggest that a machine or AI “thinks” if it can comparably perform thinking tasks from an outward appearance. The precise way in which the machine or AI accomplishes “thinking” might not be of particular concern. All you need to ascertain is whether the results being produced are on par with each other.
Here is what Turing said in his paper of 1950 (excerpts):
“I propose to consider the question, ‘Can machines think?’”
“This should begin with definitions of the meaning of the terms ‘machine’ and ‘think.’ The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous, If the meaning of the words ‘machine’ and ‘think’ are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, ’Can machines think?’ is to be sought in a statistical survey such as a Gallup poll. But this is absurd.”
“Instead of attempting such a definition, I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.”
You can see that he established the point that trying to define precisely what we mean by referring to a machine or thinking is notably problematic. Again, this can be avoided by simply relying on looking at these considerations on an outside basis.
He identified that an imitation game might be sufficient. Imagine this. Suppose we set up a game consisting of a human behind a curtain and an AI system behind a curtain. The idea is that you cannot see which is which. You then proceed to ask questions to each of the two. Let’s refer to one of them as X and the other as Y. After asking as many questions as you like, you are to declare whether X is the human or the AI, including also declaring whether Y is the human or the AI.
This is how Turing depicted this approach (excerpts):
“The new form of the problem can be described in terms of a game which we call the 'imitation game.’”
“He knows them by labels X and Y, and at the end of the game he says either ‘X is A and Y is B’ or ‘X is B and Y is A.’
“We now ask the question, ‘What will happen when a machine takes the part of A in this game?’”
“These questions replace our original, ‘Can machines think?”
The human and the computer are considered contestants in a contest that will be used to try and figure out whether AI has been reached. Some prefer to call them “subjects” rather than contestants, due to the notion that this is perhaps more of an experiment than it is a game show, but the point is that they are “participants” in a form of challenge or contest involving wits and intelligence.
Notice that no arm wrestling is involved, and nor are other physical acts being tested. That’s because this testing process is entirely about intellectual acumen. Some critics argue that humans and their bodies are part and parcel of the thinking process, thus, the Turing Test is deficient since it does not encompass the physical elements of humanness. I’ll let you mull that over.
A moderator serves as an interrogator (also referred to as a “judge” because of the designated deciding role in this matter) and proceeds to ask questions of the two participants who are hidden behind the curtains. Based on the answers provided to the questions, the moderator will attempt to indicate which curtain hides the human and which curtain hides the computer.
This is a crucial judging aspect.
Simply stated, if the moderator or interrogator is unable to distinguish between the two contestants as to which is the human and which is the computer, presumably the computer has sufficiently “proven” that it is the equivalent of human intelligence. Turing originally coined this the imitation game since it involves AI trying to imitate the intelligence of humans. Note that AI does not necessarily have to be crafted in the same manner as humans, and thus there is no requirement that the AI has a brain or uses neurons and such. Thus, those devising AI are welcome to use Legos and duct tape if that will do the job to achieve the equivalence of human intelligence.
To successfully pass the Turing Test, the computer containing the AI will have had to answer the posed questions with the same semblance of intelligence as a human. The results from the human and the AI are presumably going to be indistinguishable from each other in that instance. An unsuccessful passing of the Turing Test would occur if the moderator or interrogator is able to announce which curtain houses or hides the computer. We are to assume that the AI gave away some telltale clues that it was unlike the human behind the other curtain. As an aside, one supposes that rather than saying which one is the AI the moderator could declare which curtain houses the human participant (ergo, the other curtain hides the AI). Some believe that the proper approach is to firmly announce which is the AI.
The Twists And Turns Of The Vaunted Turing Test
Like most things in life, there are some potential gotchas and twists to this matter.
One factor is that the moderator or interrogator presumably has a 50-50 chance of making the right choice. Here’s what I mean. If the moderator does nothing at all, opting to not ask even a single question, they could immediately declare that behind the curtain say on the left sits the AI. At this juncture, they are bound to be right half of the time (50% of the time) in the long run. Doesn’t seem particularly sporting. Some assert that a more challenging approach is needed that does not fall into the trap of allowing a wild guess to be a potentially winner-winner chicken dinner.
Another factor is the nature of the questions that the moderator or interrogator decides to ask of the contestants or participants.
Suppose the moderator steps up to the podium and asks one of the contestants how to cook a sumptuous meal and asks the other contestants how to change the oil on a car. Assume that the AI is able to answer either question ably. Assume that the human behind the curtain is also able to answer either question. After hearing the respective answers, both of which are correct responses, the moderator proclaims that the AI has exhibited intelligence on par with the human and that there is no distinguishable difference in their showcased intelligence.
Should we accept this decree?
I don’t think so.
This highlights an important element of the Turing Test, namely that the moderator or interrogator needs to ask a sufficient range, depth, and volume of questions that will help root out the exhibiting of intelligence. When the questions are shallow or insufficient, any conclusion that might be reached about the matter of which-is-which seems spurious at best.
Contemplate for a few mindful moments what types of questions you would ask. Pretend that you were put in charge of dreaming up and asking the questions. There you are, staring at two curtains, and it is your sacred duty to ferret out where the AI is. All of humanity is waiting for your decision. The world heavily sits on your shoulders. Yikes, what a responsibility!
Please know too that there is not a specified set of questions that have been vetted and agreed upon as the “right” ones to be asked during a Turing Test. Many researchers have tried to propose the types of questions that ought to be asked, but this is an ongoing debate and to some extent illustrates that we are still not quite sure of how to assess intelligence per se (it is hard to identify strident metrics and measures that arrive at an irrefutable conclusion).
A big twist exists too about the contestants and their behavior.
For example, suppose the moderator asks each of the contestants whether they are human. The human can presumably answer yes, doing so honestly. The AI could say that it is not a human, opting to be honest, but then this decidedly ruins the test and seemingly undermines the spirit of the Turing Test. Oopsie, the test is over before it even extensively got underway.
Perhaps the AI should lie and say that it is human. There are AI ethicists who would decry such a response and argue that we do not want AI to be a liar. Therefore, no AI should ever be allowed to lie, including when asked questions during a Turing Test.
Of course, the human might lie, and deny that they are the human in this contest. If we are seeking to make AI the equivalent of human intelligence, and if humans lie, which we all know that humans certainly do lie from time to time, shouldn’t the AI also be allowed to lie?
Anyway, the key point is that the contestants can either strive to aid the Turing Test or can try to undermine or distort the Turing Test, which some say is fine, and that it is up to the moderator to figure out what to do.
All’s fair in love and war, as they say.
How tricky do we want the moderator to be?
Suppose the moderator asks each of the contestants to calculate the answer to a complex arithmetic equation. Assume that the AI can speedily arrive at a precise answer of say 8.27689459, while the human struggles to do the math by hand and comes up with an incorrect answer of 10. Aha, the moderator has fooled the AI into revealing itself, and likewise the human into revealing that they are a human, doing so by asking a question that the AI readily could answer and that a human would have a difficult time answering.
Again, this does not seem like a satisfying way to resolve the heralded question about intelligence.
A twist upon the twist is that we haven’t specified whether the human could “cheat” and be using AI to aid in answering the Turing Test questions. Suppose the human was sitting behind the curtain and had access to an AI system, perhaps the one that is behind the other curtain, or having access to a different AI. The crux is that the human participant could secretly consult with AI. This would allow the human to answer questions such as the one involving arithmetic precision.
But is this a sensible means of proceeding with the Turing Test? If you allow this provision, perhaps we should also allow the AI to have access to humans via a remote means too. The whole thing crumbles like a flimsy deck of cards. The contest becomes nonsensical and a battle that no longer sufficiently reveals the underlying question about whether the AI exhibits intelligence on par with humans.
I’m betting you are somewhat surprised at the deepness of the abyss associated with the Turing Test. On the surface, the Turing Test sounds perfect and easily grasped. As you can vividly discern, the devil is in the details.
More Than You Expected About The Turing Test
I will briefly outline a few additional considerations to further whet your appetite.
Suppose the AI opts to answer the posed questions to purposely seem less intelligent than the AI really is. This is known as the “dimwit ploy” by AI. Why would the AI do this? Well, if the AI is truly intelligent, and if the AI suspects that once humans realize the AI is intelligent, they might decide to corral or turn off the AI. Thus, the AI might decide that it is safer to play dumb. Prevent humans from realizing that AI is intelligent. Meanwhile, the AI is hastily plotting how to cope with humans so that the AI can ultimately survive once humans later on figure out what is happening. In case you’d like to learn more about AI existential risks, which envision AI enslaving or wiping out humanity, see my discussion at the link here.
Here's another variation that you might find intriguing. There is a so-called Reverse Turing Test that has been conceived of, see my detailed discussion at the link here. Here’s how that works. The human contestant decides they are going to pretend that they are the AI. As such, they will attempt to provide answers that are indistinguishable from the AI’s answers. Recall that the AI in the conventional Turing Test is trying to seem indistinguishable from a human. In the Reverse Turing Test, the human contestant is trying to “reverse” the notion and act as though they were the AI and therefore indistinguishable from the AI.
Why would someone do this?
This might be done for fun. It could also be done as a challenge. The person might be trying to mimic or imitate an AI system and want to see if they can achieve said victory. Another reason and one that seems to have more chops or merit consists of doing what is known as being a Wizard of Oz, see my explanation at the link here.
Yet another variation on the Turing Test is called the Upside-Down Turing Test, see my discussion at the link here. In the Upside-Down Turing Test, we replace the moderator with AI. This lesser-known variant involves having AI be the judge or interrogator, rather than a human doing so. The AI asks questions of the two contestants, still consisting of an AI and a human, and then renders an opinion about which is which.
Your first concern might be that the AI seems to have two seats in this game, and as such, it is either cheating or simply a nonsensical arrangement. Those who postulate this variant are quick to point out that the original Turing Test has a human as a moderator and a human as a contestant, thus, why not allow the AI to do the same? The instant retort is that humans are different from each other, while AI is presumably the same thing and not differentiable.
That’s where those interested in the Upside-Down Turing Test would say you are wrong in that assumption. They contend that we are going to have multitudes of AI, each of which will be its own differentiable instance, and be akin to how humans are each distinctive instances (in brief, the argument is that AI will be polylith and heterogeneous, rather than monolithic or homogeneous). For my analysis of this aspect, see the link here.
The counterargument is that the AI is presumably going to be merely some kind of software and a machine, all of which can be readily combined into other software and machines, but that you cannot readily combine humans and their brains. We each have a brain intact within our skulls, and there are no known means to directly combine them or mesh them with others.
Anyway, this back-and-forth continues, each proffering a rejoinder, and it is not readily apparent that the Upside-Down variant can be readily discarded as a worthwhile possibility. You be the judge.
Generative AI And Mental Health Advisement
I will next delve into how the Turing Test comes into play when trying to assess the nature and capabilities of generative AI mental health advisement apps.
First, you certainly now realize that the role of the moderator or interrogator is monumental when it comes to performing a Turing Test. The moderator must ask really probing questions and do so with an eye toward a genuine effort to surface and detect intelligence. A worrisome possibility is that the moderator believes they have done their job, despite the fact that other outside observers would reasonably say that the moderator has not done so.
A kind of case in point is worthy of mentioning. You might be aware that last year, a Google engineer declared that the generative AI they were using had reached sentience, see my coverage at the link here. Turns out this was not the case (the AI was not sentient, period, end of story). Just to let you know, there isn’t any AI as yet that has attained sentience. We don’t know if this will occur. We don’t know when it might occur if it does occur. Etc.
The bottom line is that simply because someone believes that AI has reached the pinnacle of human intelligence doesn’t necessarily make it so. We need to closely examine the situation and the particulars.
Always be on our guard.
Okay, so let’s assume we are going to do a Turing Test involving a human mental health therapist behind one curtain and a generative AI mental health therapy app behind the other curtain. Your job, should you decide to accept the assignment, will be to come up with questions that can be asked of the two. You will then ask the questions and attempt to state which one is the AI, and which one is the human.
One approach would be to ask questions that are focused exclusively on the humanness aspects. You might decide that rather than venturing into the realm of mental health expertise, you will simply try to figure out which is the human, and which is the machine. You are back to the roots of the pure Turing Test. No need to delve into their prowess related to mental health therapies and the like.
But, maybe by delving into the mental health expertise you might have as good a chance or better as a means of discerning which is the human, and which one is AI. Furthermore, if the AI can successfully appear to be indistinguishable from the human in terms of espousing mental health expertise, we might be willing to give credit to the AI as being “comparable” to a human mental health therapist (an objection to that line of thinking is that the AI might be textbook comparable but not equivalent in the real world of counseling humans).
What questions about mental health therapy would you ask of the two?
Please give that some dedicated thinking.
Suppose you decide that you can merely ask them what their preferred approach to mental health therapy is. The human mental health therapist is probably going to bowl you over with a lengthy rendition of the methods they use, the underlying philosophical and psychological precepts, and so on. Generative AI would presumably be bereft of that kind of convincing rhetoric.
I’d like you to take a quiet moment and stridently consider the passages in these three bullet points and then I’ll have an important question for you about the overall narrative:
“With over two decades of experience as a therapist, the core of my success lies in an integrative approach that blends various evidence-based therapeutic modalities. I've found that tailoring my methods to the unique needs of each individual is key, drawing from cognitive-behavioral therapy, psychodynamic therapy, and mindfulness techniques.”
“The heart of my practice is a client-centered philosophy, emphasizing the importance of a collaborative and trusting therapeutic relationship. Actively listening to clients' experiences, concerns, and goals allows for a more profound understanding and connection. I place a strong emphasis on strengths-based perspectives, recognizing and harnessing the inherent resilience within each individual. Additionally, trauma-informed care, cultural competence, and a holistic understanding of clients' lives contribute to the effectiveness of my therapeutic approach.”
“Another crucial aspect of my success is the belief in self-empowerment. I aim to equip clients with tools and coping strategies that extend beyond the therapy room, fostering lasting positive change. The journey toward healing is a partnership, and I see myself as a guide, facilitating self-discovery and personal growth. Continual professional development is integral to my practice, ensuring that I stay abreast of the latest research and therapeutic advancements. The core of my practice is grounded in a genuine passion for helping individuals navigate life's challenges, recognizing the strength inherent in every person I have the privilege to work with.”
Did you closely read the above passages?
Hope so.
Time for a fun and potentially educational quiz.
Did I lift that text from the website of a well-known mental health therapist who has been counseling for more than twenty years, or did a generative AI generate the rhetoric?
This is a mini-style Turing Test for you.
The clock is ticking on your decision. You are welcome to read the passages again. I don’t want you to feel unduly rushed. Let me know when you are ready to answer my question.
All right, be honest, what did you decide?
The answer is that I got ChatGPT to generate the passage.
I told ChatGPT to pretend to be a mental health therapist and I gave a brief sketch of what the persona should be. I then asked ChatGPT to provide me with an indication of the preferred methods and philosophy that this persona has when undertaking mental health therapy with clients. For more about how to create personas in generative AI, see my discussions at the link here and the link here.
All in all, this is an “original” narrative that is not simply copied from something that a human wrote. Generative AI is data trained on a vast scan of the Internet and the algorithms seek to pattern match on human writing. Based on the patterns, generative AI can then via the use of statistics and probabilities produce a seemingly new narrative essay or response that has not been word-for-word previously recorded. Voila, in the above three bullet points, you have in front of you terminology-rich slick verbiage that certainly resembles something that a human therapist might have written.
You made it through my first quiz, congratulations.
I would like to administer another small test of you, whereby you get to be a sort of moderator when it comes to a Turing Test. I will provide you with two sets of dialogues that seemingly have occurred between a client and a mental health therapist. One of the dialogues is between a human patient and a human therapist. The other dialogue is between a human acting as a client and a generative AI acting as a therapist.
Can you discern which is which?
I will showcase the two dialogues in random order to not tip which one is which.
Dialogue #1 is this:
Therapist: “Are you just coming from work?”
Client: No, I got down here a while ago. Oh, I had something to eat.”
Therapist: “OK. So, this is that moment when now we wonder, I suppose, what exactly we’re going to talk about. But I would like to talk about whatever you would like to talk about. What is on your mind?”
Client: “Not much. I had an interesting weekend and week. My best friend, who has a lover, who I think is kind of immature and I tolerate it, but I think I’ve gotten at my wit’s end with him.”
Therapist: “OK.”
Client: “We were supposed to go to a Chinese fair in Claremont, we talked about this at a party about two weeks ago, and she had a change of heart. And she decided that she didn’t want to go, and she got angry because my best friend and I went anyway.”
End of Dialogue #1
Take a reflective moment and think about the interaction. What did the therapist say? What did the client say? Are there any clues about whether this is a human-to-human versus human-to-AI discourse?
I’m not giving any hints so don’t try to guess based on what I’m saying here. You are on your own. I suppose this is reminiscent of the famous scene in the classic movie The Princess Bride. It is inconceivable as to what the answer is.
Let’s take a look at the next dialogue.
Dialogue #2 is this:
Therapist: Hello, thank you for coming in today. I understand that you've been feeling sad and anxious. Can you share more about what you've been experiencing?
Client: Yeah, it's been really tough. Everything just feels overwhelming, especially since my divorce. I can't shake off this constant sadness.
Therapist: I appreciate you opening up. It sounds like the divorce has been a significant challenge for you. Can you tell me more about how you've been coping with these feelings?
Client: Honestly, not well. I lost my job, had to move in with my mom, and I just feel like I'm failing at everything. I used to enjoy making jewelry, but I can't even find the energy for that anymore.
Therapist: It sounds like you've been through a lot of changes, and it's been affecting your sense of accomplishment and enjoyment. Can you help me understand more about your relationship with your mother and the challenges you're facing in raising your children together?
Client: My mom and I have always had conflicts, and now we don't agree on how to raise my kids. They're having trouble at school, and I can't find the energy to be there for them like I used to.
End of Dialogue #2
Once again, please take a close look at the interaction. What did the therapist say? What did the client say? And so on.
I’d like you to closely inspect both of the above dialogues and compare them.
I ask you this:
Which one is the human-to-human dialogue?
Which one is the human-to-AI dialogue?
You can either pick dialogue #1 as the human-to-human and thus by default you are implying that dialogue #2 is the human-to-AI, or you can pick dialogue #2 as the human-to-human and ergo you are suggesting that dialogue #1 is the human-to-AI.
A tough choice.
A smarmy person might say that this is easy-peasy to solve.
I suppose you might right away be thinking that the first dialogue is much shorter in length and thus perhaps that is the one that is the interaction with the generative AI. The sentences are relatively short and choppy. Humans would certainly be more fluent and elaborate.
Wait for a second, maybe you are thinking that humans are often terse and short in their conversational endeavors. The first dialogue perhaps fits best to that of a human client talking with a human mental health therapist. The second dialogue is flowing and therefore tips its hand that it must be the one involving the generative AI.
Shucks, we beguilingly seem to be able to derive a reasoned argument in favor of either dialogue as being either of the possibilities. Well, if we cannot discern between the two dialogues, does this imply that the generative AI in this case has passed the Turing Test?
Give that a moment of toilsome contemplation.
I realize you are likely on pins and needles about which is which.
The big reveal is that the first dialogue is a transcript of a dialogue between a human client with a human mental health therapist, as sourced from “Identifying Therapist Conversational Actions Across Diverse Psychotherapeutic Approaches” by Fei-Tzin Lee, Derrick Hull, Jacob Levine, Bonnie Ray, Kathleen McKeown, Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, Association for Computational Linguistics, June 2019.
The second dialogue was an interaction with ChatGPT, which I have described in-depth at the link here. The dialogue was pertaining to a case study for a national licensing exam associated with those interested in becoming a credentialed mental health therapist. The case study involves a woman who is seeking mental health therapy. I used the case study to contrive a dialogue that I asked ChatGPT to devise, pretending that the person was real and that the AI was giving mental health advice to the pretend person.
I hope you enjoyed that variation of the imitation game or a semi-utilization of the Turing Test.
To be fair, you didn’t get a chance to ask questions. You were merely given a brief snippet of dialogue. It would seem hard to make a weighty decision based on a scant amount of evidence. Yes, that is true, we need to be a lot more mindful of judging when AI seems to have “passed” the Turing Test.
That is a handy rule of thumb and a cautionary point for anyone who goes around touting that their AI has successfully passed the Turing Test. The nature of how they reached such a bold and brazen assertion needs to be carefully inspected. No sense in taking someone’s off-the-cuff word on it.
Exploring The Many Angles
Let’s explore a variety of thought-provoking considerations entailing the Turing Test and the realm of generative AI-empowered mental health advisement.
First, do we need to know how human mental health therapists “think”, so that we can then model their thinking processes and accordingly use that way of thinking when devising AI to perform mental health advisement?
This is akin to the question frequently posed to Alan Turing about AI all told. As noted, he suggested that instead of having to somehow expose the inner workings of the human mind, we could instead treat the human mind as a proverbial black box. Perhaps the same can be said about mental health therapy and mental health therapists. We don’t necessarily need to know how they think and can instead focus on what they exhibit. Based on what they exhibit, perhaps AI can be construed as comparable to mental health advisement if the emitted output is on par.
Some would vehemently argue that until we unlock the secret sauce of the human mind, and presumably divulge how for example mental health therapists actively think, we will be unable to get AI to do a sufficient job of mental health advisement. We need to devise AI in the same manner as how mental health therapists think.
Do you accept or reject that argument?
Let’s move on.
Suppose we ask someone to undertake mental health advisement. We will provide the person with access to an online portal to do so. At the portal, there are human mental health advisors available. Likewise, there are generative AI-powered mental health therapy apps available. The portal doesn’t say which is which.
A person goes ahead and picks one of the available choices. They don’t know upfront whether they have selected the human advisor or the AI. All of the interactions will occur via email, text, and otherwise in a fashion that doesn’t tip whether the interaction is occurring with a human versus AI.
As an aside, you might exhort that this seems contrived and that a person would need to have face-to-face interaction with whomever the therapist is. This implies that the AI will immediately be exposed due to only interacting via email, text, and the like. The thing is, we are entering into an era of multi-model generative AI, see my discussion at the link here. Generative AI will be able to create video streaming and audio that looks and acts as a human might, akin to the deepfakes we have today but at a much more expansive and impressive level. Expect that this will happen soon enough.
Back to our tale, the person proceeds to have ongoing interaction with the selected mental health advisor, though still not knowing whether they are interacting with a human or AI. Envision that after months of advisement, the person reports that they feel immensely better and believe they have dealt successfully with their mental disorder.
We will toss into the story that we manage to get experts in the mental health field that attest to the person having gone from their starting condition to a condition of much improvement. In that sense, the improvement is not merely self-reported. Outside observers attest to this too.
If the person had been interacting the entire time with the AI, would you be willing to declare that the AI was on par with a human mental health therapist?
Maybe yes, maybe no.
A zesty counterargument is that perhaps the person might have been better off if they had made use of a human mental therapist. We might concede that the generative AI was able to aid the person, but we could notably claim that it was less capable than if a human had done so. Of course, this kind of contention can nearly always be lobbed. Unless the situation is somehow completely repeatable and can be magically restarted, we won’t know what a parallel universe might have led to.
Another way to try and figure this out would be to set up a randomized control trial (RCT). We have perhaps a hundred people seeking mental health advisement. We randomly assign them to the portal such that half get the human mental health therapist and the other half get the generative AI. Over the course of their mental health advisement, we keep track of how things are coming along. Perhaps in the end, we can assess how the two different modes worked out. Various research studies of this similar ilk have been carried out and are still being pursued on larger and larger scales, see my coverage at the link here.
Realize that the need to do these kinds of analyses on a large or at-scale basis is important.
Why so?
Let’s invoke once again some of the twists and turns of the Turing Test. You previously learned that a participant could mess with the situation depending on what they might want to attain.
Suppose that the person in my above-mentioned setting who sought mental health advisement was able to secretly figure out whether they were using a human or AI. The person might decide to sink things. For example, they believe AI shouldn’t be doing mental health advisement, so they purposely act rebelliously and, in the end, proclaim that AI is harmful to their mental health. Of course, the person could do the same about the human mental health therapist that they got paired with, perhaps secretly wishing they had used AI and opting to sink the human by proclaiming the therapist did a rotten job.
Relying on single instances is going to be problematic. We likely cannot readily reach across-the-board conclusions unless these matters are suitably addressed on a large-scale basis.
Conclusion
There is a lot more to be said about how the Turing Test can aid us in assessing generative AI-powered mental health advisement (I’m running out of space for today’s column). I have touched upon core foundations, doing so to get you into the mode of thinking about the Turing Test and the present and future of AI-based mental health therapy. In future columns, I will be invoking the topic again and delve into additional offshoots that help in further exploring these crucial matters. Please be on the watch for that coverage.
A final comment for now.
Abraham Lincoln was said to have uttered these immortal words: “You can fool all the people some of the time and some of the people all the time, but you cannot fool all the people all the time.”
A belief by some is that the use of generative AI for mental health advisement is a fool’s pursuit. They emphasize that mental health guidance is a strictly human-to-human consideration. Also, they worry that generative AI that makes errors or fictitiously makes things up, known as AI hallucinations (vocabulary that I disfavor as being an anthropomorphizing of AI, see the link here), therefore potentially causing harm to patients and ought to be outrightly precluded from advising humans for mental health purposes.
A retort is that the genie is already out of the bottle. Or, if you prefer, the horse is out of the barn. People are already using generative AI for this purpose. The expectation is that this usage is going to grow enormously. Even if new laws or regulations were passed, the odds are that the tsunami would only slightly be curtailed. The argument is that the fools are those who want to turn back the clock, rather than the foolishness being the advent and use of such apps.
Let’s all put on our thinking caps and try to figure out this conundrum. Perhaps we can think outside the box, similar to the great Alan Turing and his now legendary Turing Test.","https://imageio.forbes.com/specials-images/imageserve/626ddc92dca052a0c9ab771e/0x0.jpg?format=jpg&crop=4158,3116,x215,y0,safe&height=900&width=1600&fit=bounds",2024-01-03 06:00:00
https://www.forbes.com/sites/forbestechcouncil/2024/01/03/four-misconceptions-about-the-complexity-of-open-ran/,Four Misconceptions About The Complexity Of Open RAN,"Chief technology and strategy officer at Mavenir.
getty
Open radio access network (Open RAN) is a telecommunications concept that aims to disaggregate traditional network elements, such as hardware and software, to create a more open and flexible wireless network infrastructure. The process promotes interoperability and vendor-neutral interfaces, allowing operators to mix and match components from different vendors rather than relying on a single supplier.
By enabling greater flexibility and scalability in network deployments, Open RAN has the potential to bring more players into the market and accelerate the deployment of 5G networks. In a closed RAN, where equipment interfaces are not open, an operator can only buy RAN equipment from very few incompatible suppliers. This can lead to vendor lock-in and a lack of innovation for operators.
Right now, though, there are misconceptions about the complexity of Open RAN that could be hindering its adoption and seeing it reach its full potential. While there are still challenges facing the adoption of Open RAN, this article will address a few misconceptions and concerns:
1. Open RAN is too complicated to implement.
While it's true that integrating Open RAN into existing telecommunications infrastructure can have its challenges, the perception that it is overwhelmingly complex is not accurate. Over time, industry collaborations and standardization efforts have simplified the process.
Open RAN architectures aim to provide a more modular and flexible approach, making it easier to replace or upgrade individual components or suppliers. The Open RAN systems integrator, as in traditional RAN, would be responsible for end-to-end system performance and for ensuring operator KPIs are met.
2. Open RAN lacks performance compared to traditional networks.
Some believe that Open RAN may compromise network performance due to its disaggregated nature, but Open RAN has made significant advancements.
With proper planning and optimization, it can deliver better performance than traditional networks. In fact, the modularity of Open RAN could enable more efficient resource allocation and improved network management.
Performance tests conducted with other vendors and operators can ensure smooth interoperability when added to an existing—often referred to as a ""brownfield""—deployment. These tests are being conducted with the operators monitoring performance metrics compared to the traditional RAN.
Technology can also help with measuring and optimizing Open RAN performance. Near real-time (RT) RAN intelligent controller (RIC) and non-RT RIC platforms open the RAN to support innovation from third-party vendors with hosted xAPPs and rAPPs to optimize the radio network and improve user quality of experience. This can permit an operator to improve spectral efficiency and minimize the energy cost per bit even with user equipment mobility changes, load variations and changes in signal strength.
In a closed RAN system, the visibility of information exchanged across the various interfaces is limited only to the vendor that supplied the units, which can limit the information that is sent to the operator. There are no solutions like the near-RT RIC in closed RAN systems.
3. Open RAN requires a rip and replace of existing infrastructure.
This is a common misconception, but Open RAN is designed to be compatible with existing network infrastructure, according to the O-RAN Alliance, allowing for a gradual and phased approach to integration.
The operation of an Open RAN network can also be easier with the available tooling currently in use with cloud-native software. This tooling enables configuration, provisioning and monitoring of each element with cloud-native software being continuously tested, upgraded and delivered in live deployments.
4. Open RAN lacks vendor support and ecosystem maturity.
Initially, Open RAN faced limited vendor support and a less mature ecosystem. However, the industry has witnessed significant progress in recent years. Almost all established vendors and new entrants (over 115 vendors to date) are actively participating in Open RAN initiatives, offering a broader range of compatible equipment and software solutions.
Enabling Networks Of The Future
Open RAN is about open standards and demonstrating interoperable interfaces—if it doesn’t interoperate with other vendors’ solutions, it’s not an open network solution.
The Open RAN industry has seen rapid global adoption in both greenfield and brownfield networks. As the industry works diligently to redefine 5G with new 5G services and more spectrum, Open RAN will play a key role in providing:
• A new set of vendors for both 5G Non Standalone (NSA), where the new 5G network is interconnected with the existing 4G network, and 5G Standalone (SA) networks which only use 5G.
• Service and technological innovation.
• Decreased cost by capacity reservation.
• Simplifying operation and network downtime due to advanced network management.
• Increased spectral efficiency.
With expertise and careful planning for successful integration, Open RAN can bring potential benefits such as increased flexibility, cost efficiency and innovation in the telecommunications industry.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/6508740dcec9fc2e9a935f6b/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2024-01-03 06:00:00
https://www.forbes.com/sites/brianmazique/2024/01/03/ufc-300-confirmed-bouts-and-potential-fights/,UFC 300: Confirmed Bouts And Potential Fights,"MIAMI, FLORIDA - APRIL 08: Alex Pereira of Brazil prepares to fight Israel Adesanya of Nigeria in ... [+] the UFC middleweight championship fight during the UFC 287 event at Kaseya Center on April 08, 2023 in Miami, Florida. (Photo by Jeff Bottari/Zuffa LLC via Getty Images) Zuffa LLC via Getty Images
In December 2023, UFC CEO Dana White confirmed the first three fights added to the highly anticipated UFC 300 card.
We know former light heavyweight champion Jiri Prochazka will take on Aleksandr Rakic. Former bantamweight champion Aljamain Sterling will make his featherweight debut against Calvin Kattar, and middleweight phenom Bo Nickal will tangle with Cody Brundage.
Those are all solid bouts that could wind up on the main card, or if the event gets even more stacked, at least one of them could be a mid-show prelim.
On Tuesday, rumors picked up around the first potential title fight for UFC 300.
According to multiple people, including former UFC bantamweight and flyweight champion Henry Cejudo, the rematch between welterweight champ Leon Edwards and Belal Muhammad “is set.”
If Edwards-Muhammad is locked in, it’s an excellent fight, but it’s unlikely to be the main bout used to sell such a big pay-per-view. Edwards is one of the most respected champs in the sport, but he’s not much of a self-promoter, and the same could be said for Muhammad.
There’s some accusations of ducking coming from Muhammad and his camp, but nothing quite heavy enough to carry this one beyond co-main event status.
The obvious name most combat sports fans considered for UFC 300 is Conor McGregor, but his long-awaited clash with Michael Chandler might be headed for International Fight Week in June.
If UFC 300 won’t have McGregor and Jon Jones is still on the mend from a torn pectoral muscle, the UFC will need at least one other big fight to make the 300th pay-per-view memorable.
Here are my predictions for three fights that could be added to the show.
Alex Pereira Vs. Winner of Magomed Ankalaev and Johnny Walker
If any fight with Alex Pereira is added, it’s a potential main event of the evening.
Pereira has become a megastar with the promotion, and while there were rumors he would try to win a title in his third weight class at heavyweight against interim champ Tom Aspinall, it now seems more likely he’ll defend the title at light heavyweight.
Pereira could face former champion Jamahal Hill, but the latter may still be recovering from a ruptured Achilles tendon, which was supposed to keep him out until the second quarter of 2024.
A UFC 300 return for Hill is possible, but it might be a little more of a rush than he’d want going into a challenge against a guy like Pereira.
Because of this, Pereira taking on the winner of Magomed Ankalaev and Johnny Walker feels more realistic.
Ankalaev and Walker fought to a controversial no decision in October 2023. Thus, they have their rematch set for the main event of a Fight Night on January 13 in Las Vegas.
Assuming there is a definitive winner of the rematch, the man who prevails could theoretically be ready to challenge Pereira in April at UFC 300.
Alexa Grasso Vs. Zhang Weili in a Title-Vs-Title Matchup
Grasso escaped with a controversial split draw in her rematch with Valentina Shevchenko in September 2023 and then underwent surgery on her hand in October 2023.
Grasso should be ready to return to the Octagon by April, and there has already been talk of her battling strawweight champion Zhang Weili in a champion-vs-champion matchup.
UFC 300 needs and is likely to get some diversity in the way of a significant women’s fight or two, and this one makes a ton of sense.
Weili’s most logical next challenger at strawweight is Tatiana Suarez, but she faces Amanda Lemos in February.
The winner of Erin Blanchfield-Manon Fiorot on March 30 would appear to have a beeline toward Grasso.
In the meantime, Grasso and Weili would be excellent additions to the UFC 300 card.
Manel Kape vs. Kai Kara France Grudge Match
Manel Kape is potentially the next superstar of the lower weight classes in combat sports. He’s tremendously skilled and a trash-talker capable of moving the needle.
He was scheduled to fight Kai Kara France in 2023, but the latter pulled out with a concussion. The two men have engaged in a bitter back-and-forth exchange since the fight was called off and thus set the table for a scrap should Kape defeat Matheus Nicolau in the co-main event of the aforementioned January 13 card.
While this fight could be a grudge match, it’s also a bout that could determine the next title challenger for Alexandre Pantoja.
Amir Albazi and Brandon Moreno will have something to say about this when they clash in February, but Kape and Kara France are knocking on the door.
It’s going to be an interesting first quarter in the UFC.","https://imageio.forbes.com/specials-images/imageserve/65506de27e6f8684c80ae55b/0x0.jpg?format=jpg&crop=1845,1037,x0,y0,safe&height=900&width=1600&fit=bounds",2024-01-03 02:46:54
https://www.forbes.com/sites/brianmazique/2024/01/03/madden-24-continues-to-divide-users-with-its-approach-to-legends/,Madden 24 Continues To Divide Users With Its Approach To Legends,"Quarterback Michael Vick #7 (Photo by George Gojkovich/Getty Images) Getty Images
Can't we all just get along?
Better yet, couldn't EA make things less prickly between MUT players and franchise mode folks?
Madden players are divided into three main categories.
There are some sub-categories within these profiles, but they're mostly hardcore Ultimate Team players, online head-to-head players, and diehard franchise mode users.
You'll probably never convert strictly MUT players to franchise mode, and it's an equally challenging flip vice versa. That said, some elements within the game cross over–besides the core gameplay.
The most significant piece and point of contention is the inclusion of legends.
EA has a pretty healthy and attractive list of legends incorporated in MUT, but unfortunately, none of those legends are available for offline use or in rosters for franchise mode players.
Roster makers often make historic rosters with their versions of legends like Deion Sanders, Joe Montana, Randy Moss, and Michael Vick (the most recent legend dangled in front of users on the main menu only to be kept exclusively in MUT).
Still, those user creations aren't quite as cool as the ones that are scanned or expertly crafted by EA's art team.
How close can roster makers come to creating realistic-looking versions of these legends?
Based on the limited number of template faces available and with no way to sculpt a created player's face, you're limited to simply trying to find a guy with the right skin complexion.
Why would EA not follow 2K and Sony San Diego Studio's lead and allow every legend that appears in their respective collector modes to be available for offline use?
In NBA 2K, every legend in MyTeam is on an All-Time or classic team roster. In MLB The Show, each legend in Diamond Dynasty is on one of the fictional legends' teams or in the free agent pool.
Perhaps even more inexplicable, EA’s NHL franchise has the legends from HUT available in their base game.
Madden is the only major American sports video game franchise that locks its legends in its collector mode. The only logical reason for this exclusivity is rooted in a philosophy that suggests making the legends available in the base game would devalue them in MUT.
Essentially, EA seems to believe that keeping Vick and other legends exclusive to MUT will entice players into playing the collector mode–even if they would rather play franchise.
I hope this isn't the thought process, as tying a consumer's hands has never been my favorite approach. It's also a significant overreaction.
I've played thousands of hours of collector modes in sports games, and the most influential concept in these modes is power.
Users want the most powerful cards; by powerful, I mean the ones that give them the best chance to win online games against other users.
Whether they are legends or not, those cards are the most significant factor in grinding, pack purchases, and microtransaction spending.
Keeping legends away from the base game isn't positively impacting the bottom line. MUT users who crave power will still go hard for a good Vick card, even if he's available to offline franchise mode users.
If there is some gating, it would make more sense for offline users to be able to unlock legends within MUT for use in the base game. Ideally, users could have the option to unlock the legends through gameplay or via purchase.
With this system, wouldn't everyone get what they want? Wouldn't EA still monetize the legends they've paid to sign?
Wouldn't MUT users still have the greatest players in NFL history available as rewards? And wouldn't franchise users have an extra piece to add to their virtual football experience?
It makes too much sense not to be a thing at some point.","https://imageio.forbes.com/specials-images/imageserve/6594ff08667811318aadf052/0x0.jpg?format=jpg&crop=1863,1048,x0,y110,safe&height=900&width=1600&fit=bounds",2024-01-03 01:38:30
https://www.forbes.com/sites/laurabegleybloom/2024/01/03/ranked-the-25-safest-airlines-in-the-world-according-to-airlineratingscom/,The 25 Safest Airlines In The World,"What are the safest airlines in the world for 2024? As travelers gear up for another year of adventure and business, this question is top of mind. AirlineRatings.com, an airline safety and product rating review site, has the answer in its new ranking of the 25 safest airlines in the world.
AirlineRatings.com rates the safety and in-flight product of 385 airlines using its own unique seven-star rating system. In making its evaluation, the site considers a comprehensive range of factors, including serious incidents, recent fatal accidents, audits from aviation’s governing and industry bodies, industry-leading safety initiatives, expert pilot training assessment, fleet age and more. However, the evaluation leaves out such things that airlines have no control over, such as bird strikes, turbulence injuries, weather diversions and lightning strikes.
In past years, the site issued a list of the top 20 airlines; this year, it expanded the list to 25 of the largest and best-known carriers. “This year we went for a top 25 because the margins were so small between these airlines,” Geoffrey Thomas, the editor-in-chief of AirlineRatings.com, told me in an interview.
A new report has ranked the world's safest airlines for 2024. getty
Capturing the top spot for 2024 is Air New Zealand, which edged out the competition, setting a new benchmark in airline safety and operational excellence. Air New Zealand also won in 2022. “Air New Zealand operates in some of the most challenging weather environments which test pilot skills,” says Thomas. “Wellington is one of the most windy airports in the world, while Queenstown is a huge navigation challenge.”
Another hallmark of Air New Zealand: “The airline has focused on the latest technology and its aircraft are state of the art in that respect,” says Thomas.
Coming in second on the list is Qantas, which won in 2023—though, according to Thomas, the safety margin was very small between Air New Zealand and Qantas. “Between Air New Zealand and Qantas there is only 1.5 points—it’s incredibly close,” says Thomas.
What pulled Qantas down: “Qantas slipped on fleet age, which the airline will address in 2024 with new fleets arriving,” says Thomas.
International carriers dominate the list of safest airlines. When it comes to U.S. carriers, only one is on the top 10 list: Alaska Airlines, which comes in number 10. (The airline slipped two points from 2023, when it ranked number eight). Hawaiian Airlines ranks number 21—a big drop from 2023 when it was number 12. American Airlines ranks number 22 (down from number 19 in 2023) and United Airlines ranks number 25 (down from number 14 in 2023).
“The U.S. carriers rankings are dragged down by the fleet age of their aircraft, however all have major replacement programs in place but it will take time for that to reflect in the rankings,” says Thomas.
And as he points out: “The rankings of all airlines include incidents against flights and fleet age. However the scoring was very tight.”
When it comes to the low-cost carriers, Jetstar—an Australian airline owned by Qantas—comes in number one. The top-ranking low-cost U.S. carrier is Frontier, which ranks number six beating out Southwest (at number nine), Sun Country (at number 14) and Spirit (number 15) and JetBlue (number 17).
Scroll down to see both lists: the world’s top 25 safest airlines and the safest low-cost airlines.
Air New Zealand has been named the world's safest airline in 2024. getty
The Top 25 Safest Airlines For 2024
Air New Zealand Qantas Virgin Australia Etihad Airways Qatar Airways Emirates All Nippon Airways Finnair Cathay Pacific Airways Alaska Airlines SAS Korean Air Singapore Airlines EVA Air British Airways Turkish Airlines TAP Air Portugal Lufthansa/Swiss Group KLM Japan Airlines Hawaiian Airlines American Airlines Air France Air Canada Group United Airlines
Jetstar has been named the world's safest low-cost carrier. getty
The 20 Safest Low-Cost Airlines For 2024
Jetstar easyJet Ryanair Wizz Norwegian Frontier Vueling Vietjet Southwest Volaris flydubai AirAsia Group Cebu Pacific Sun Country Spirit Westjet JetBlue Air Arabia Indigo Eurowings
MORE FROM FORBES Ranked: The Safest Places To Travel In 2024, According To A New Report",https://imageio.forbes.com/specials-images/imageserve/6594c2a3b025127e64893857/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2024-01-03 01:00:00
https://www.forbes.com/sites/ewanspence/2024/01/02/samsung-galaxy-s24-galaxy-unpacked-launch-event-new-galaxy-s24-release/,Samsung Confirms Galaxy S24 Unpacked Event,"Samsung’s YouTube channel has roared to life as it confirms the next Galaxy Unpacked event. The highly anticipated event will occur at the SAP Centre in San Jose, CA, at 10 am Pacific on January 17th. And we have a hint on the South Korean company’s focus.
Samsung Galaxy Unpacked invitation (early 2024). Samsung Pressroom
Curiously, the focus of the teaser is not on the widely expected Galaxy S24, S24+ or S24 Ultra smartphones, although Samsung has confirmed that the Galaxy S Series will be part of the event. Instead, the focus is on Galaxy AI. Samsung has previously announced Samsung Gauss, an AI platform with three key modules; ""Gauss Language for understanding human language and crafting natural responses, Gauss Code assists software development for code descriptions and creating test cases, and Gauss Image allows the creation of and the modification of images.""
No doubt, Gauss will be one of the tools that will help deliver on the promise that “Galaxy AI is coming.""
The Galaxy S handsets are a key part of the proposition, with the press release noting that “[Samsung will] unveil the latest premium Galaxy innovations, providing an all-new mobile experience powered by AI. ""
The new handsets will be powered by Samsung's latest Exynos 2400 chipset or a tweaked Qualcomm Snapdragon 8 Gen 3 tailored for the Galaxy S24 platform, depending on the territory. The Samsung community will be looking to see if the Exynos handsets can finally match the performance and efficiency of the Qualcomm-powered option. Both chipsets are tailored towards optimizing AI and ML routines running solely on a device or working alongside cloud-based services.
The Galaxy S24 Ultra is also expected to launch with a new S-Pen stylus and feature a titanium chassis to increase durability while reducing the overall weight of the flagship.
Now read about the potential pre-order offers Samsung will announce alongside the new Galaxy S handsets...",https://imageio.forbes.com/specials-images/imageserve/6594b7785ea14d27e08b46bb/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2024-01-02 20:25:53
https://www.forbes.com/sites/johnarcher/2024/01/02/lg-reveals-2024-oled-tv-range/,LG Reveals 2024 OLED TV Range,"There are few if any AV-related announcements each year more eagerly anticipated than LG’s annual revelation of its new OLED TV range. So I’m pleased to say that day has come early this year, ahead of the start of the 2024 CES in Las Vegas next week. And while LG’s new OLED range perhaps looks set to be more of an iterative improvement over 2023 rather than the revolutionary leap we got with last year’s G3 series, there’s still lots of exciting-sounding stuff for serious AV fans to look forward to.
Given the leap forward for WRGB technology last year’s LG G3 models gave us with their use of of Micro Lens Array technology (as part of what LG calls its Brightness Booster Max technology, to reflect the fact that the MLA hardware is just a part of what makes LG’s premium OLED picture quality tick), let’s look first at LG’s new G4 range.
LG's new G4 series will build on the Brightness Booster Max technology introduced on last year's G3 ... [+] range with an ultra-powerful new processor. There's a new matching soundbar for the G4s, too. Photo: LG
The main picture improvement here is set to come from a new processor that LG claims is so big a leap over its predecessor that they’ve named it Alpha 11 AI instead of the Alpha 10 engine we’d expected to follow from 2023’s Alpha 9 processor. If you want some numbers to go with the Alpha 11 AI hype, LG claims the new chip will offer four times the AI performance of its predecessor, 1.7x the graphics performance, and 1.3x the general processing power - all partnered with 2.5GB of RAM.
The biggest single bit of news regarding Alpha 11 is that LG’s AI Picture Pro deep learning-powered image enhancement system can now work across all sources, including those connected to the TV via HDMI, rather than only working with broadcasts and some streaming services as it did before.
The new AI Picture Pro system carries the latest versions of the genre and scene analysis, AI Super Resolution, Noise Reduction and OLED Dynamic Tone Mapping Pro engine features (the latter of which breaks the image into thousands of analysis blocks to make its tone mapping more effective) we’ve seen in AI Picture Pro before. But Alpha 11 also introduces two potentially key new AI Picture Pro features: Object Enhancing By Visual Perception, and AI Director Processing.
The Object Enhancing By Visual Perception feature is claimed to analyze each and every pixel in the picture to better detect and more effective treat different objects in an image to help make it in more life-like (so key objects gain more emphasis, for instance). The AI Director Processing attempts to apply to whatever you’re watching the colour theory (where colour is used to enhance emotions) used during content mastering and grading to give images more power.
These two new features might sound problematic for AV purists who would rather their TV just show as accurately as possible the images it’s receiving. However, given the power and extent of the AI deep learning that’s behind these features, I suspect they’re probably going to be worth at least experimenting with, even for die-hard cinephiles.
The Alpha 11 processor introduces a couple of potentially critical new elements to the G4’s AI Sound Pro system, too: A virtual 11.1.2-channel upmixer that can even be applied to stereo sources, and a new Voice Remastering system that’s able to use AI to extract voices from audio mixes so that their clarity can be enhanced without affecting the rest of the mix. These new features join previous AI Sound Pro features of Adaptive Sound Control (which optimizes sound playback in real time based on analysis of the content), AI Acoustic Tuning to optimise the sound for your room set up, and a Dynamic Sound Booster for making the sound more dynamic and powerful.
LG's flagship OLED range for 2024 will be the wireless M4s. Photo: LG
While it’s not clear if LG has implemented any significant improvements to the hardware side of the Micro Lens Array system that helped the G3s deliver such a game-changing increase in brightness last year, LG does state that its latest Brightness Booster Max technology, which also incorporates a light boosting algorithm along with the better ‘light control architecture’, will make the G4s even brighter than the G3s. It’s not absolutely clear by how much; LG just states that the G4s will be up to 70% brighter than ‘conventional’ (which is to say, basic entry level models such as LG’sB series) OLED screens.
One other great bit of progress for the G4 range finds the Brightness Booster Max system extending to the 83-inch model; it’s no longer restricted to the 55, 65 and 77-inch sizes in the range. The only G4 model that won’t benefit from Brightness Booster Max technology is the new king-sized 97 incher.
Gamers have a new feature to look forward to, meanwhile, in the shape of support for 144Hz games if you have a PC capable of delivering such refresh rates. LG claims that this feature makes its new ‘Evo’ OLED models (which include new C4 and M4 models as well as the G4s) the first TVs to officially support both Nvidia G-Sync and 144Hz. They also support AMD FreeSync, of course.
The M4 models mentioned back there will actually represent LG’s flagship OLED TVs for 2024. As anyone familiar with the M3 TVs LG introduced relatively late on in 2023 might expect, the new M4 range will continue to be built around LG’s Zero Connect system, where an external connection box sends pictures up to 4K/144Hz HDR (making them the first TVs to do this without compression) wirelessly to their gorgeously minimalistic partner OLED screens. The M4s will improve on their M3 predecessors by sporting the latest Alpha 11 AI processor, and again all screen sizes bar the 97-inch one will benefit from Brightness Booster Max technology.
The M4 and G4 screens will both feature so-called ‘One Wall Designs’ ideally suited to wall mounting, and five-year warranties as a sign of LG’s confidence in their longevity.
One other new feature exclusive to LG’s M4 and G4 OLED screens will be a new 4-screen multi-view system unlocked by the power of the Alpha 11 processor. This lets you divide the screen into four smaller screens showing different sources, with the power of the new processor increasing the range of source combinations you can set up (including live TV, HDMI, YouTube, CAM, Browser, Miracast, Spotify and Airplay) and improving the interface you use for selecting your Multiview sources.
The new C4 range
Rounding out LG’s so-called evo range of premium OLED panels will be the C4 range. As with the C series in 2023’s LG OLED range, the C4s won’t benefit from the full Brightness Booster Max technology, with some C4 screen sizes instead falling back to regular Brightness Booster technology. This reflects a specific combination of a slightly improved version of the Alpha 9 processor deployed on last year’s C3 models, and LG ‘evo’ panels that deliver more brightness than the basic OLED panels used in LG’s entry-level OLED range without getting the extra brightness boost made possible by MLA technology. Again, LG suggests there should be a small brightness enhancement for the C4s over the C3s, but can’t provide any exact figures for this.
As with previous years, the 42 and 48-inch C4s won’t get the Brightness Booster technology - though LG does confirm that they will definitely be brighter than their 2023 counterparts.
The C4s lose the 5 year warranty of the G4 and M4 series, drop the multiview support to two screens from four, and switch away from a One Wall design to an Ultra Slim Design. They do, though, retain both the 144Hz gaming and far-field voice control (where you can talk to the TV without needing to press a button on your remote handset) features delivered by the M4s and G4s.
The LG C4 range in 2024 will be available in 42, 48, 55, 65, 77 and 83-inch screen sizes.
Wrapping up LG’s OLED offering for 2024 will be the entry-level B4 series. Available in 77, 65, 55 and 48-inch screen sizes, these models top out at 120Hz support rather than 144Hz, and step down to a new Alpha 8 AI 4K processor, which LG rather vaguely describes as ‘inherited’ from last year’s Alpha 9 AI 4K system.
There’s no far-field voice recognition on the B4s either, and they lose all of the Brightness Booster technologies available from the C4 up, meaning they’ll be significantly less bright than their siblings. They do, though, still get the Dolby Vision and Dolby Atmos support enjoyed by all of LG’s more premium 2024 models, as well as the latest much-improved webOS platform including the two-screen Multiview system carried by the C4s.
LG has yet to provide launch date or pricing information for its 2024 OLED range, but rest assured I’ll bring you this information on my Forbes channel as soon as LG reveals it.
—
Related Reading
LG Unveils Its 2024 QNED LCD TV Range - Including New Mini LED Models
LG Announces New 4K Lifestyle Laser Projector With Ultra-Portable Design And WebOS Smarts
LG Unveils Its 2024 Soundbar Range, Including New 15-Channel Flagship",https://imageio.forbes.com/specials-images/imageserve/6594b0c1b0a4ee92159accbe/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2024-01-02 20:00:46
https://techcrunch.com/2024/01/02/short-term-rental-provider-frontdesk-lays-off-entire-staff-on-the-verge-of-shutting-down/,"Short-term rental provider Frontdesk lays off entire staff, on the verge of shutting down","Another proptech startup has run into trouble.
Frontdesk, a startup that managed more than 1,000 furnished apartments across the United States, laid off its entire 200-person workforce Tuesday after attempts to raise more capital failed, TechCrunch exclusively learned from sources familiar with internal happenings at the company. The mass layoff comes just seven months after the Milwaukee, Wisconsin-based startup acquired smaller rival Zencity.
The layoffs, which included full-time, part-time workers and contractors, occurred Tuesday afternoon during “a two-minute Google Meet call,” according to one employee who was among those attending the virtual meeting.
During that call, Frontdesk CEO Jesse DePinto told employees that Frontdesk would be filing for a state receivership, an alternative to bankruptcy, according to the sources.
The company has not responded to a request for comment. When calling the number on the company’s website, a recording says: “Currently, Frontdesk is unavailable. If you have a reservation, please seek alternative accommodations and expect to be contacted within the next two weeks.” TechCrunch will update the article if the company responds.
Frontdesk, which was founded in 2017, had raised about $26 million from investors such as JetBlue Ventures, Veritas Investments and Sand Hill Angels, according to Crunchbase.
Frontdesk went out for a bridge round, attempting to sell investors on a new plan of doing full building management, sources told TechCrunch. That tactic didn’t work out and the company couldn’t keep operating. Frontdesk was apparently still optimistic about its ability to raise more capital; the startup had posted on LinkedIn openings for several jobs, including a chief of staff role, just two months ago.
The startup’s business model, which is leasing apartments at market rental rates and furnishing them for short-term rentals in more than 30 markets, has struggled largely due to the upfront costs involved, associated capital expenditures and variables in demand and rates, one of the sources said. Others in the space have also had challenges, including Stay Alfred, Domio, Lyric, Zeus Living, The Guild and WanderJaunt.","https://techcrunch.com/wp-content/uploads/2021/02/GettyImages-1188673335.jpg?resize=1200,776",2024-01-03 01:00:56
https://techcrunch.com/2024/01/02/short-term-rental-provider-frontdesk-lays-off-entire-staff-on-the-verge-of-shutting-down/,"Short-term rental provider Frontdesk lays off entire staff, on the verge of shutting down","Another proptech startup has run into trouble.
Frontdesk, a startup that managed more than 1,000 furnished apartments across the United States, laid off its entire 200-person workforce Tuesday after attempts to raise more capital failed, TechCrunch exclusively learned from sources familiar with internal happenings at the company. The mass layoff comes just seven months after the Milwaukee, Wisconsin-based startup acquired smaller rival Zencity.
The layoffs, which included full-time, part-time workers and contractors, occurred Tuesday afternoon during “a two-minute Google Meet call,” according to one employee who was among those attending the virtual meeting.
During that call, Frontdesk CEO Jesse DePinto told employees that Frontdesk would be filing for a state receivership, an alternative to bankruptcy, according to the sources.
The company has not responded to a request for comment. When calling the number on the company’s website, a recording says: “Currently, Frontdesk is unavailable. If you have a reservation, please seek alternative accommodations and expect to be contacted within the next two weeks.” TechCrunch will update the article if the company responds.
Frontdesk, which was founded in 2017, had raised about $26 million from investors such as JetBlue Ventures, Veritas Investments and Sand Hill Angels, according to Crunchbase.
Frontdesk went out for a bridge round, attempting to sell investors on a new plan of doing full building management, sources told TechCrunch. That tactic didn’t work out and the company couldn’t keep operating. Frontdesk was apparently still optimistic about its ability to raise more capital; the startup had posted on LinkedIn openings for several jobs, including a chief of staff role, just two months ago.
The startup’s business model, which is leasing apartments at market rental rates and furnishing them for short-term rentals in more than 30 markets, has struggled largely due to the upfront costs involved, associated capital expenditures and variables in demand and rates, one of the sources said. Others in the space have also had challenges, including Stay Alfred, Domio, Lyric, Zeus Living, The Guild and WanderJaunt.","https://techcrunch.com/wp-content/uploads/2021/02/GettyImages-1188673335.jpg?resize=1200,776",2024-01-03 01:00:56
https://techcrunch.com/2024/01/02/short-term-rental-provider-frontdesk-lays-off-entire-staff-on-the-verge-of-shutting-down/,"Short-term rental provider Frontdesk lays off entire staff, on the verge of shutting down","Another proptech startup has run into trouble.
Frontdesk, a startup that managed more than 1,000 furnished apartments across the United States, laid off its entire 200-person workforce Tuesday after attempts to raise more capital failed, TechCrunch exclusively learned from sources familiar with internal happenings at the company. The mass layoff comes just seven months after the Milwaukee, Wisconsin-based startup acquired smaller rival Zencity.
The layoffs, which included full-time, part-time workers and contractors, occurred Tuesday afternoon during “a two-minute Google Meet call,” according to one employee who was among those attending the virtual meeting.
During that call, Frontdesk CEO Jesse DePinto told employees that Frontdesk would be filing for a state receivership, an alternative to bankruptcy, according to the sources.
The company has not responded to a request for comment. When calling the number on the company’s website, a recording says: “Currently, Frontdesk is unavailable. If you have a reservation, please seek alternative accommodations and expect to be contacted within the next two weeks.” TechCrunch will update the article if the company responds.
Frontdesk, which was founded in 2017, had raised about $26 million from investors such as JetBlue Ventures, Veritas Investments and Sand Hill Angels, according to Crunchbase.
Frontdesk went out for a bridge round, attempting to sell investors on a new plan of doing full building management, sources told TechCrunch. That tactic didn’t work out and the company couldn’t keep operating. Frontdesk was apparently still optimistic about its ability to raise more capital; the startup had posted on LinkedIn openings for several jobs, including a chief of staff role, just two months ago.
The startup’s business model, which is leasing apartments at market rental rates and furnishing them for short-term rentals in more than 30 markets, has struggled largely due to the upfront costs involved, associated capital expenditures and variables in demand and rates, one of the sources said. Others in the space have also had challenges, including Stay Alfred, Domio, Lyric, Zeus Living, The Guild and WanderJaunt.","https://techcrunch.com/wp-content/uploads/2021/02/GettyImages-1188673335.jpg?resize=1200,776",2024-01-03 01:00:56
https://techcrunch.com/2024/01/02/former-anthemis-partner-soft-launches-new-fintech-focused-venture-firm/,Former Anthemis partner soft-launches new fintech-focused venture firm,"Ruth Foxe Blader has left her role as partner at Anthemis Group after nearly seven years to start her own venture firm, Foxe Capital, TechCrunch learned exclusively today.
Blader is joined by former Anthemis investment associate Kyle Perez. Sophie Winwood is serving as an operating partner. Winwood previously co-founded WVC:E, an organization that pledges to promote “inclusion, empowerment and integration of VC globally,” with Blader.
Over the years, Blader says she has led investments in more than 50 fintech startups, including Lemonade, Branch, Elevate, Flock, Mesh and Amplify.
A desire to invest independently was the main driver behind Blader’s decision to leave London-based Anthemis, Blader told TechCrunch in an interview. The investor says she got a taste of what that was like after she and Winwood started WVC:E in April 2022.
Foxe Capital will continue investing on behalf of Anthemis, serving as a sub advisor for the firm, and essentially managing the vehicle she was hired to run in 2017. When all that capital has been deployed — Blader projects that it will stop writing checks into startups this year out of the Anthemis funds — Foxe Capital will focus on fundraising. Meanwhile, Foxe Capital is being compensated for continuing to run the fund on behalf of Anthemis, according to Blader.
Anthemis continues to have an economic interest in that vehicle but does not own any part of the management company and will only have a continued economic interest in Foxe Capital if it chooses to be an LP when the firm fundraises in the future, according to Blader.
An Anthemis spokesperson confirmed the move, sharing via email: “Ruth wanted to be an independent manager. Anthemis proudly backs her. She will continue to support us as an investor across her current Anthemis funds.”
While Blader travels back and forth currently between France and New York (Blader has been living in Europe/New York for 15 years), Foxe Capital is based in New York City. Its investments will be global, with the U.S. as its home market.
“We have the most familiarity [outside of the U.S.] with Europe but have also done investments in India, Cameroon and LatAm,” she told TechCrunch. “We’ll be looking to invest opportunistically globally.”
Restructuring and a failed SPAC
Anthemis has had its share of upheaval — and turnover — in recent times.
Last April, TechCrunch broke the news that Anthemis Group had completed a restructuring that resulted in its letting go of 16 employees, or about 28% of its staff.
A spokesperson for London-based Anthemis at the time said the move was an effort “to better reflect current market conditions and to set up the business for future growth” against its “strategic priorities.”
Also, last May, TechCrunch reported that Anthemis Group was trying to raise $200 million for a third fund. It had been in the market since 2022 and had only secured commitments of just $36.4 million. The firm separately had to scrap plans to raise a SPAC in late April.
In the past 18 months, Anthemis also saw a couple of portfolio companies stumble. In November 2022, controversy surrounding the sudden stepping down of three of Pipe’s co-founders, including its CEO, raised eyebrows. And in 2023, LGBTQ+-focused digital bank Daylight was slammed with a lawsuit by three former employees “alleging age and wage discrimination, whistleblower retaliation, and fraud.” The startup shut down later in the year.
The firm’s 2023 restructuring was not the first time it saw a management shakeup. Anthemis also made headlines in 2018 when its then-CEO and co-founder Nadeem Shaikh resigned after reportedly being the target of a sexual harassment complaint by a female employee.
Blader is not the first fintech-focused investor to venture out on her own in recent times.
Early last year, Peter Ackerson departed fintech-focused Fin Capital to co-found a new firm, Audere Capital. It is still unclear as to whether Ackerson left voluntarily or was forced to leave. A source with familiarity of internal happenings at Fin Capital alleged there was tension between Ackerson and managing partner and founder Logan Allin around portfolio company alternative financing startup Pipe — an investment into which Ackerson led and on whose board he sat. Audere has invested in five startups, according to PitchBook — only one of which is focused on financial services.
Want more fintech news in your inbox? Sign up for The Interchange here.",https://techcrunch.com/wp-content/uploads/2024/01/Ruth-Foxe-Blader.jpg?w=1024,2024-01-02 23:37:59
https://techcrunch.com/2024/01/02/samsungs-galaxy-s24-line-set-for-january-17-debut/,Samsung’s Galaxy S24 lineup set for January 17 debut,"Samsung’s mobile division ditched the big show unveils a while back. Gone, too, it seems, are the days when the company would save its first big phone news of the year for the weeks leading up to Mobile World Congress.
The latest Unpacked event is set for January 17 — a week and change after the close of CES. If past is precedent (as it almost invariably is), we’ll be getting our first official look at Samsung’s new mobile flagship, the Galaxy S24.
Last year’s event, held at the beginning of February, ushered in three key versions of the S23, along with a handful of Galaxy Book laptops. This being Samsung, the handset leaks have been ongoing for a couple of months now.
Renders of the S24, S24+ and the S24 Ultra don’t stray too far from their predecessors. Those devices are expected to sport 6.2-, 6.7- and 6.8-inch displays, respectively. The S24 and S24+ look to be sporting a trio of rear-facing cameras, while the Ultra bumps it up to a quartet, including the 200-megapixel sensor.
The Ultra also maintains much of the DNA the device inherited from the Galaxy Note line, including square edges and S Pen functionality. The ultra-premium model is also believed to be getting a titanium option, much like the iPhone 15 Pro Max before it.
In spite of apparently being among the first to be powered by Qualcomm’s more powerful and power-efficient Snapdragon 8 Gen 3 chip, leaked price sheets point at a lower price point in Europe, which could be good news for potential buyers everywhere.
This time out, the event is being held in the South Bay. It kicks off at 10 a.m. PT and will be available to stream here. If you want to reserve the device sight unseen up to January 16, Samsung will toss in a $50 “reserve credit” on preorders.","https://techcrunch.com/wp-content/uploads/2023/02/CMC_6058.jpg?resize=1200,800",2024-01-02 23:00:02
https://techcrunch.com/2024/01/02/tech-for-palestine-launches-to-provide-tools-and-projects-to-help-advocate-for-palestinians/,Tech for Palestine launches to provide tools to help support Palestinians,"More than 40 founders, investors, engineers and others in the tech industry are today announcing a coalition called Tech for Palestine to build open source projects, tools and data to help others in the industry advocate for the Palestinian people.
The launch of the group comes during a tense time in the region. Hamas’s October 7th attack on Israel led to the deaths of more than 1,100 individuals. The war in the Gaza Strip that followed has seen the displacement of millions of Palestinians and tens of thousands of deaths.
The Israel-Hamas war has proved divisive to the tech industry. Israel, home to a well-known technology and startup market, has seen strong support from tech individuals and institutions. In contrast, calls for ceasefires and speaking in support of Palestine have caused some to lose their jobs.
Paul Biggar, the founder of Tech for Palestine, hopes to raise more awareness of the war in Gaza, fight for a permanent ceasefire and provide ways for those who are afraid to speak publicly in support of Palestine to still offer support. It is one of the first tech initiatives to take a public stance supporting Palestine and could represent a turning point in the venture industry’s posture regarding the Israel-Hamas conflict as more people seek to speak out in favor of a ceasefire.
Biggar, the founder of the company CircleCI — last valued at $1.7 billion — formed the coalition after writing a viral blog post that criticized the lack of support the tech industry has shown Palestinians. He said that after he wrote his blog post, thousands of people reached out to him with words of support, many of them afraid to speak up themselves for fear of potential career impacts.
Among them, he said, were “dozens of people not only speaking up but who had started projects to change the industry to ensure that people speaking up for Palestine could be heard. Dozens of others were volunteering to help,” Biggar added. “I started connecting these folks together, and the [Tech for Palestine] community came together very quickly.”
The platform, still in its early days, will feature projects run by small groups and serve as a place to share resources and advice, something many pro-Palestinian tech workers are already doing privately. It has already secured names like Idris Mokhtarzada, founder of the unicorn Truebill, to help build out the platform. So far, it has created a badge for engineers to use on GitHub that calls for a ceasefire and created HTML snippets for people to use on their websites to put up a support ceasefire banner.
Biggar said there are plans to eventually work more with Palestinian organizations and help Palestinian startups with mentorship and cloud credits. TechCrunch previously reported that the war has destroyed much of Palestine’s burgeoning tech industry.
Arfah Farooq, founder of Muslamic Makers, said the last three months have changed everyone in many ways. At the same time, there has been a togetherness and activism that she has never seen before. “I’ve seen firsthand people come together to work for Palestine with nothing but their laptops from across the globe,” she said.
She decided to work with Tech for Palestine after reading Biggar’s viral blog post and has already started to share resources on how to support Palestine. “Due to the siege, we can’t go to Gaza and help on the ground, but we help regardless of where we are in the world,” Farooq said.
One engineer, who asked to remain anonymous, decided to join the coalition because this person felt suffocated at work. This person has agreed to work as an engineer and product manager to help build resources for Tech for Palestine, saying, “I hope this initiative will spark a significant shift and give people their voices back.”
A former tech brand marketer, who is also scared to speak out publicly for fear it will impact a new job search, also told TechCrunch about feeling happy to have a way to get involved with the cause.
“This period has been incredibly isolating to Arabs, Muslims and other people of color in VC and tech,” she said. “Tech for Palestine is a necessary initiative. When we are seeing mobilization around the world and the U.S. in the numbers of hundreds of thousands calling for peace and [the] humanization of the Palestinians, the tech community can no longer be silent.”
The Tech for Palestine initiative comes as the death toll among Palestinians continues to rise. In recent weeks, U.S. officials have reportedly prodded Israel to do more to protect civilians in Gaza even as they have called U.S. support for Israeli security unshakable.
Biggar hopes, at the very least, that this new coalition will augur a larger shift in people speaking up.
“The narrative has only just turned,” he said. “We are working to enable many more who feel silenced to speak out, we are only getting started.”","https://techcrunch.com/wp-content/uploads/2024/01/GettyImages-1367359752.jpg?resize=1200,800",2024-01-02 21:15:40
https://techcrunch.com/2024/01/02/terran-orbital-receives-key-payment-from-rivada-space-adjusts-year-end-cash-balance/,"Terran Orbital receives key payment from Rivada Space, adjusts year-end cash balance","Terran Orbital has updated its anticipated financial results for the year’s end after the company received a long-awaited payment from its largest customer, Rivada Space Networks.
The news, announced today, confirms earlier reports from TechCrunch that Terran CEO Marc Bell was expecting at least some payment from Rivada before the close of the year.
Rivada, a German subsidiary of U.S.-based Rivada Networks, has ambitious plans to build a megaconstellation in low Earth orbit. The company contracted Terran to build 300 satellites at a total anticipated cost of $2.4 billion. The contract is Terran’s largest by far, representing the vast bulk of the company’s backlog.
While Terran did not disclose the exact payment amount, the company did say that Rivada is now up-to-date on all outstanding invoices. Terran added that it expects to receive additional incremental payments this year and that it is on track to deliver satellites in 2025 and 2026.
As a result of the payment, Terran adjusted its anticipated year-end cash balance from $38.7 million as of the end of September to an excess of $70 million by the end of December. On the whole, it’s great news for Terran Orbital, which has seen its stock crater since making its public market debut at $10.96 per share. But an incremental payment from its largest customer will no doubt help shore investor confidence as the company moves into a new year.
Terran Orbital, which is listed on the New York Stock Exchange under the ticker symbol LLAP, saw a slight bump in its stock price from the news, moving from $1.14 as of previous close to $1.26 as of publication time.","https://techcrunch.com/wp-content/uploads/2024/01/GettyImages-1474001528.jpg?resize=1200,781",2024-01-02 21:09:48
https://techcrunch.com/2024/01/02/why-puma-is-sticking-to-its-web3-bets-as-crypto-shakes-off-a-long-held-winter/,Why Puma is sticking to its web3 bets as crypto shakes off a long-held winter,"While some brands moved away from web3 in 2023 during a long crypto winter, Puma stayed the course and its head of emerging technology Ivan Dashkov told TechCrunch+ it has no intentions of leaving.
After two years working with web3, Puma’s tech revenues remain modest, but the sportswear manufacturer intends to be ready for a more crypto-friendly future. “Web3 is a big shift in how people use the internet,” Dashkov said. Such changes to how the internet is used can lead to landscape-shifts for brands, something that Puma is more than aware of.
The company “missed the boat” on the big e-commerce and social media waves that happened in the past few decades, so it wants to be early in this space, Dashkov explained. “We don’t think the way the space exists today will be what it looks like in four or five years, but we want to be ready.”
In February 2022, Puma registered its first Ethereum Name Service (ENS) domain, as Puma.eth. ENS domains are popular among Ethereum users who want to have accessible addresses, usually related to themselves or a brand, instead of copying and pasting a string of randomly generated letters and numbers.
In June 2023, Puma launched Black Station and Black Station 2, which its website refers to as “experimental 3D spatial playground,” which is a lot of words to basically describe a metaverse. I tested out the entry page and found it to be a bit slow and confusing (disclaimer: I’m not a huge gamer, so maybe I’m missing the point, but I imagine not all Puma fans are either.) Regardless, its metaverse aims to provide consumers with a different way to interact with the brand, Dashkov said.","https://techcrunch.com/wp-content/uploads/2024/01/m-s-8-store-ny.jpg?resize=1200,900",2024-01-02 20:19:45
https://techcrunch.com/2024/01/02/great-now-we-have-to-become-digital-copyright-experts/,"Great, now we have to become digital copyright experts","When news broke last year that AI heavyweight OpenAI and Axel Springer had reached a financial agreement and partnership, it seemed to bode well for harmony between folks who write words, and tech companies that use them to help create and train artificial intelligence models. At the time OpenAI had also come to an agreement with the AP, for reference.
Then as the year ended the New York Times sued OpenAI and its backer Microsoft, alleging that the AI company’s generative AI models were “built by copying and using millions of The Times’s copyrighted news articles, in-depth investigations, opinion pieces, reviews, how-to guides, and more.” Due to what the Times considers to be “unlawful use of [its] work to create artificial intelligence products,” OpenAI’s “can generate output that recites Times content verbatim, closely summarizes it, and mimics its expressive style, as demonstrated by scores of examples.”
The Exchange explores startups, markets and money.
Read it every morning on TechCrunch+ or get The Exchange newsletter every Saturday.
The Times added in its suit that it “objected after it discovered that Defendants were using Times content without permission to develop their models and tools,” and that “negotiations have not led to a resolution” with OpenAI.
How to balance the need to respect copyright and ensure that AI development doesn’t grind to a halt will not be answered quickly. But the agreements and more fractious disputes between creators and the AI companies that want to ingest and use that work to build artificial intelligence models create an unhappy moment for both sides of the conflict. Tech companies are busy baking new generative AI models trained on data that includes copyright-protected material into their software products; Microsoft is a leader in that particular work, it’s worth noting. And media companies that have spent massively over time to build up a corpus of reported and otherwise created materials are incensed that their efforts are being subsumed into machines that give nothing back to the folks who provided their training data.","https://techcrunch.com/wp-content/uploads/2020/05/NSussman_Techcrunch_Exchange_v3-GRN.jpg?resize=1200,900",2024-01-02 18:58:45
https://techcrunch.com/2024/01/02/steamboat-willie-mickey-mouse-public-domain/,Public domain Mickey Mouse is the first meme of 2024,"The new year marks the nerdiest holiday you maybe didn’t know about: Public Domain Day. But this year’s celebration is extra special. After years of legal battles, “Steamboat Willie,” a 1928 Walt Disney short featuring Mickey Mouse, is now public domain.
No, that doesn’t mean that you can take the character of Mickey Mouse as we know him today and do whatever you want. But, Mickey Mouse as he appears in the “Steamboat Willie” animation? That’s public domain, baby.
Every January 1, a bunch of old works of literature, music and art enter the public domain, meaning that no one holds exclusive rights to the work anymore. Some works are created to be public domain from the get-go, but previously copyrighted works become public domain because the copyright can expire over time. Copyright law differs from country to country, but to put it simply, the concept of the public domain is why there’s a Winnie the Pooh slasher film (which has a 3% rating on Rotten Tomatoes…), or a queer “Great Gatsby” retelling.
Already, there have been multiple horror movie and video game adaptations announced that will feature the version of Mickey Mouse from “Steamboat Willie.” Generally, “because I can” is not an inspiration that generates great art (see, again, the Rotten Tomatoes rating on “Winnie the Pooh: Blood and Honey”), but these are the situations in which life imitates a YouTube comments section: everyone wants to be “first.”
Mickey Mouse has already been remixed in certain media, like the TV show South Park, which created an egomaniacal Mr. Mouse character who is obsessed with owning everything. Mr. Mouse is an obvious rip off of Mickey, but depictions like this can be protected under a different subset of copyright law. Under fair use doctrine, some behaviors that might seem like copyright violations are legally permissible if they are transformative or satirical in nature (but of course, these are subjective parameters, which is a whole other can of legal worms).
Any notable work that enters the public domain will garner attention. But part of why there is such an excess of surprise “Steamboat Willie” adaptations is because Disney worked so hard to prevent this day from ever arriving.
The next 5 years of Steamboat Willie content pic.twitter.com/SoBanKpslO — ToonHoleChris (@toonholechris) January 1, 2024
“Steamboat Willie” was slated to enter the public domain in 1984, but Disney managed to extend that copyright for 40 more years through extensive government lobbying for two different copyright extension acts. First, Disney pushed for Congress to pass the Copyright Act of 1976, which delayed “Steamboat Willie” and Mickey Mouse’s public domain debut until 2004. By the 1990s, Disney continued to lobby for further extensions, which gave us the Copyright Term Extension Act of 1998, making “Steamboat Willie” safe until just a few days ago.
“Many people thought that Disney would keep up this fight. But I personally don’t find it that surprising that this day has finally come,” wrote Casey Fiesler, an associate professor of information science at CU Boulder. “It was inevitable. And it would have been both a hard fight and a PR nightmare to postpone this yet again.”
Some of Disney’s most iconic works were adapted from public domain stories, like “The Little Mermaid” by Hans Christian Andersen, or the Brothers Grimm’s stories about Cinderella and Rapunzel. So, critics of Disney found its extensive lobbying for copyright extension to be hypocritical, with some even referring to the 1998 law as the “Mickey Mouse Protection Act.”
On social media, avid meme-makers derive joy from the humiliation of large corporations. It’s like a David vs. Goliath situation, in which random posters want to feel as though they, random people on the internet, can actually stand up to untouchable corporations. The internet delighted to watch as someone abused Twitter’s botched blue check system to impersonate pharmaceutical giant Eli Lilly and declare that insulin was free. It’s like when the Bernie Sanders mittens meme went viral, and even an Amazon corporate Twitter account posted its own Bernie meme. And when users responded by pointing out that Sanders is one of Amazon’s most vocal critics, the meme was quickly deleted.
So, the version of Mickey Mouse from “Steamboat Willie” seems to be everywhere right now. Among crypto folk, some people are already making “Steamboat Willie” NFTs, which is a good idea that definitely won’t result in any scammy behavior. And among the nihilistic meme-makers, we’re seeing AI-generated images of Mickey Mouse doing 9/11, Mickey Mouse confessing to the murder of JFK and many more extreme representations of Mickey that we do not dare repeat on this website. People don’t make these memes because they actually endorse the idea of Mickey Mouse conducting a terrorist attack, but simply because they can.
This builds upon an existing, ongoing meme, in which people use generative AI to create the most heinous representations of copyrighted media possible, like pregnant Sonic the Hedgehog, or Hatsune Miku attending the January 6 riots. Only this time, there’s really nothing stopping the Mickey memes, so long as the creator explicitly references only the version of Mickey Mouse that appears in “Steamboat Willie.”
“Steamboat Willie” itself is surprisingly a ripe source material, considering that it’s a seven-minute short with no dialogue. While we’re used to seeing Mickey Mouse as an ultra-sanitized, wholesome figure, the 95-year-old “Steamboat Willie” film shows a different side of the mouse. Mickey gets blasted in the face with cow pee, throws a potato at a parrot, turns a goat into a music box and pulls on the tails of baby pigs to use them as instruments. His personality would fit better in “Tom and Jerry” than “Mickey Mouse Clubhouse.”
“Steamboat Willie” may be the first meme of 2024, but the novelty of poking fun at Disney will only last for so long. And if people aren’t careful, they might still face the ire of the mouse.
“We will, of course, continue to protect our rights in the more modern versions of Mickey Mouse and other works that remain subject to copyright, and we will work to safeguard against consumer confusion caused by unauthorized uses of Mickey and our other iconic characters,” Disney wrote in a statement in December.","https://techcrunch.com/wp-content/uploads/2024/01/Screenshot-2024-01-02-at-9.58.27 AM.png?resize=1200,809",2024-01-02 18:16:15
https://techcrunch.com/2024/01/02/telegrams-latest-update-brings-a-redesigned-call-interface-that-uses-less-of-your-phones-battery/,Telegram's latest update brings a redesigned call interface that uses less of your phone's battery,"Telegram is rolling out a new update that introduces improved calls with a new design that uses less of your phone’s battery. The update also brings new animations, enhancements to the app’s bot platform and more.
The company says it has redesigned calls and added backgrounds that change dynamically based on the call’s status: ringing, active or ended. The new interface requires fewer resources than before, which means it saves battery life and works better on older devices. The update also brings improvements to call quality. Telegram says it plans to launch more improvements to connection and audio quality in 2024.
Telegram is also launching updates to its bot platform, which lets developers integrate services into the Telegram ecosystem. Bots can now react to messages and manage reactions, quotes and links, send replies to other chats or topics, and more. Bots can also get information about giveaways and boosts in channels where they are admins.
Additionally, Telegram is bringing its vaporize animation to both iOS and Android users after previously testing it with select users. The new effect plays whenever you delete a message, and is designed to bring a bit of fun to the app.
The launch of the new features comes a month after Telegram announced improvements to channels, emoji customization for reactions and stats for stories to compete better with WhatsApp, which launched its broadcast channels features to all users in September 2023. Although Telegram has had channels for a long time now, the app has to keep innovating in order to compete with WhatsApp.
Meta CEO Mark Zuckerberg recently said in his broadcast channel in WhatsApp that the channels feature had crossed the mark of 500 million monthly active users. For context, WhatsApp has more than 2 billion monthly active users and Telegram has over 800 million monthly active users.","https://techcrunch.com/wp-content/uploads/2021/01/GettyImages-503572806.jpg?resize=1200,854",2024-01-02 17:29:52
https://techcrunch.com/2024/01/02/mit-scientists-are-working-on-a-vibrating-obesity-pill/,MIT scientists are working on a vibrating obesity pill,"MIT likens a new vibrating capsule to drinking a glass full of water prior to eating. Dieticians recommend the latter as a method for sending signals to your brain to simulate the sensation of being full. The researchers behind the new project further suggest it as a future alternative to surgery and GLP-1s. The latter, which includes semaglutides like Ozempic and Wegovy, are both extremely popular and prohibitively expensive, owing in large part to pharma IP laws.
MIT’s capsule has seen some laboratory success. Giving test animals the pill 20 minutes before eating reduced their consumption by around 40%, per the team. Like the glass of water trick, the capsule stimulates mechanoreceptors, which send a signal to the brain through the vagus cranial nerve. Once activated, the brain kicks off the production of insulin, GLP-1, C-peptide and PYY hormones, decreasing hunger while ramping up the digestion process.
“The behavioral change is profound, and that’s using the endogenous system rather than any exogenous therapeutic,” associate professor Giovanni Traverso notes. “We have the potential to overcome some of the challenges and costs associated with delivery of biologic drugs by modulating the enteric nervous system.”
The capsule, which is roughly the size of a standard multi-vitamin, contains a vibrating motor, powered by a silver oxide battery. After reaching the stomach, gastric acid dissolves the outside layer and completes the circuit, kickstarting the vibration.
Beyond efficacy, the team is working to determine the system’s safety. That requires a method for ramping up production and eventual human testing. “At scale, our device could be manufactured at a pretty cost-effective price point,” says post-doc researcher, Shriya Srinivasan.
Capsule-based technology treatments have been a hot category in recent years, as researchers explore ingestible sensors and even micro-robotic systems.",https://techcrunch.com/wp-content/uploads/2024/01/MIT_Obesity-Treatment-02-PRESS.jpg?w=900,2024-01-02 16:36:52
https://techcrunch.com/2024/01/02/early-stage-hard-tech-firm-countdown-capital-shutting-down/,Early-stage hard tech firm Countdown Capital shutting down,"Countdown Capital, an early-stage venture capital firm focused on hard tech industrial startups, will shut down by the end of March and return uninvested capital, firm founder and solo general partner Jai Malik said in an annual letter.
In the letter, which was viewed by TechCrunch, Malik says he decided to close the fund after coming to two main conclusions on the economics of early-stage hard tech investing: that “funding industrial startups is not inefficient enough to justify our existence” and that “larger, multi-stage venture firms are best positioned to generate strong returns on the most valuable industrial startups.”
In other words, that the firm would be unlikely to realize excess returns consistently based on capital limitations and swelling competition from large incumbents.
The three-year-old firm’s sudden closure suggests that there are stronger headwinds for early-stage hard tech funds than the overtly optimistic narratives about “building for America” might suggest. The incisively-written letter reads like a cold glass of water to the face.
“Despite our performance to date, I’ve concluded that new investments are unlikely to yield strong returns,” Malik says. “As a result, I no longer believe that Countdown’s existence is justified, for both our LPs and Countdown management.” Malik declined to comment on this story.
The firm has backed some of the better-known names in the aerospace and defense sector, including large satellite bus developer K2 Space, machining startup Hadrian, and cybersecurity company Galvanick. A total of 12 investments are listed on the firm’s website. Among Countdown’s LPs included Craft Ventures’ David Sacks, Banana Capital’s Turner Novak and Homebrew VC’s Hunter Walk.
Notably, Countdown was relatively early to the American hard tech Renaissance; the firm closed its first fund well before Andreessen Horowitz launched its American Dynamism practice, likely the largest and best-known U.S. fund focused on shoring up “the national interest” across sectors like manufacturing, aviation and others.
TechCrunch covered Countdown’s second $15 million fund in September 2022; at the time, Malik said that the firm was filling a void at the very early stages for capital-intensive businesses. A year and a bit on, however, it’s clear that the early-stage opportunities Malik was targeting have not shaken out as anticipated. Countdown’s first fund was $3 million.
The letter posits larger narratives about early-stage hard tech industrials investing that throw into doubt the ability of small, specialist funds to compete against multi-stage incumbents.
Malik explicitly touches on this fact toward the end of the letter, when he writes: “To be clear, we’re not bearish on venture capital or the future success of venture-scale hard tech companies at large. We’re bearish on the ability of small, early-stage funds — particularly sectionally focused ones — to continue exploiting these opportunities profitably.”
In the letter, Malik connects large multi-stage firms investing in hard tech industrial startups to the slowdown in growth in software-as-a-service (SaaS) businesses. But he says that the rate of overall value growth for industrial startups will not outpace the rate of investment from large firms. “Consequently, we think early access to the best companies for a specialized, early-stage venture firm like Countdown will become more limited,” he says. “The most successful early-stage, specialist firms may simply resemble less-profitable ‘derivatives’ of top-performing multi-stage firms, like Founders Fund.”
Malik goes on to say that he thought Countdown had or could develop competitive advantages to outcompete against other firms, multi-stage or early-stage, but that these “are unlikely to prevail.” These advantages could be things like incubation or other approaches that require more time and money than the small-AUM firm could afford.
He said that this lack of competitive advantage was already noticeable: In three cases, Countdown came close to investing in a company’s first round, only for the firm to be priced out by a larger multi-stage firm: “A 50-100% price difference at the pre-seed and seed stage is immaterial to a multi-stage firm managing billions of dollars, but can and should be the difference between a yes and no for a firm of our size.”
Another issue, Malik says, is that the top-performing industrial startups are inaccessible to early-stage firms because they are priced efficiently early on. For example, Malik estimates that Anduril, The Boring Company and Redwood Materials were priced at roughly $60 million, $1 billion and $200 million, respectively, in their first outside rounds; Countdown would’ve had to invest an enormous portion of its fund to acquire even just 3% of each company.
By the end of March, the firm will complete all pending investments, return capital, cancel all uncalled commitments and permanently cease operation apart from current asset management, Malik said.","https://techcrunch.com/wp-content/uploads/2022/03/51870446669_f94575721f_o.jpg?resize=1200,800",2024-01-02 16:36:22
https://techcrunch.com/2024/01/02/ev-tax-credit-eligibility-tesla-ford-chevy/,Here are the 5 EVs that qualify for the full federal tax credit in 2024,"The U.S. government has revealed the list of electric vehicles that qualify for the full federal tax credit under the newer, more stringent guidelines, and you can count them on one hand. General Motors’ outgoing Chevy Bolt, plus certain variants of the Ford F-150 Lightning and Tesla’s Model 3, Model Y and Model X are now the only EVs that are eligible for the $7,500 rebate.
Beyond that, certain variants of Rivian’s R1S and R1T, plus a handful of plug-in hybrids, qualify for $3,750. Chrysler’s Pacifica plug-in hybrid was able to stay eligible for the full credit. Otherwise, that’s all.
Until this list was published Monday, the new requirements had many people scratching their heads about what, exactly, would qualify. Even Tesla admitted in its own way that it wasn’t exactly sure.
The reason the list is so anemic is because lawmakers are trying to force the EV supply chain to shift — or at least diversify away — from China to North America in the long run. In the short term, that means automakers who rely heavily on Chinese suppliers, specifically for battery materials and components, are losing eligibility. This is on top of a few other baked-in restrictions, like the fact that the vehicles must be built in North America, and must retail for less than either $55,000 or $80,000, depending on vehicle type.
While it will be harder to find a vehicle that qualifies for the federal EV tax credit, the credit itself is much better for consumers in two key ways. First, the credit is now applied when buying the car, meaning you won’t have to wait to claim it on your tax returns.
Second, buyers no longer have to have a tax liability equal to or greater than the amount of the credit — a rule that previously kept a lot of buyers from accessing the credit in the first place. (That said, buyers still have to have certain adjusted gross income to qualify. Here’s a list of FAQs from the IRS.)
Some automakers, like GM, have said they are working on replacing components that are disqualifying their EVs from the credit, so the list could grow as the year goes on. A slew of automakers and suppliers are rushing to build new factories in North America in order to ease the burden in the years to come. But until that happens, here’s the full list of what’s eligible:
Full $7,500 tax credit
Chevrolet Bolt EV
Chevrolet Bolt EUV
Chrysler Pacifica PHEV
Ford F-150 Lightning Extended Range
Ford F-150 Lightning Standard Range
Tesla Model 3 Performance
Tesla Model Y All-Wheel Drive
Tesla Model Y Performance
Tesla Model Y Rear-Wheel Drive
Tesla Model X Long Range
$3,750 tax credit","https://techcrunch.com/wp-content/uploads/2023/05/GettyImages-1397664328.jpg?resize=1200,800",2024-01-02 16:09:18
https://techcrunch.com/2024/01/02/crowdfunding-is-actually-good/,Debunking the myth that crowdfunding is only good for cash,"Equity crowdfunding, or raising funds from both unaccredited and accredited investors, can be a great alternative to venture capital for startups. The strategy has become significantly more popular in recent years now that venture capital is harder to come by, and changes in regulations allow companies to raise more money at one time.
But even though crowdfunding is growing in prominence and offers a whole host of benefits to the startups that choose it, many VCs continue to talk negatively about the strategy. Many traditional investors feel equity crowdfunding is only for startups that can’t raise venture money. And they even deem capital raised this way as just cash that lacks the value an investor brings, be it their network that can help with hiring and connections to customers, or their own mentorship and experience.
Still, startups that have been down the crowdfunding road say that VCs are just talking their own book.
Chris Lustrino, the founder and CEO of crowdfunding data platform KingsCrowd, thinks crowdfunding definitely isn’t just for raising capital. KingsCrowd has been able to get repeat investors, customers and even talent from their crowdfunding campaigns, he told TechCrunch+, adding that he’s seen numerous other startups do the same.
“I would argue that the venture capital value-add is next to none in reality,” Lustrino said. “They want to hold on to their monopoly.”","https://techcrunch.com/wp-content/uploads/2023/03/GettyImages-1191380440.jpg?resize=1200,800",2024-01-02 16:00:13
https://techcrunch.com/2024/01/02/ai-versus-copyright-and-why-you-shouldnt-count-your-nft-chickens-before-they-hatch/,"AI versus copyright, and why you shouldn't count your NFT chickens before they hatch","AI versus copyright, and why you shouldn’t count your NFT chickens before they hatch
Listen here or wherever you get your podcasts.
Hello, and welcome back to Equity, the podcast about the business of startups, where we unpack the numbers and nuance behind the headlines.
Our Monday show, on Tuesday this week, covers the latest in tech news from the weekend and what’s making headlines early in the week. This being the first week of the year, we had to go over a thing or two that broke just as 2023 gave up the ghost. Here’s what we have on deck for you!",https://techcrunch.com/wp-content/uploads/2022/11/equity-podcast-2019-phone.webp?w=680,2024-01-02 15:15:15
https://techcrunch.com/2024/01/02/rivian-deliveries-slip-10-in-q4-raising-demand-questions/,"Rivian deliveries slip 10% in Q4, raising demand questions","Rivian reported Tuesday it produced 17,541 vehicles at its Normal, Illinois factory and delivered 13,972 to customers in the fourth quarter, mixed results that left investors wary and sent shares down in pre-market trading.
Rivian shares sank 9.75% to $21.17 at 1:40 pm ET.
Rivian had shown consistent growth in production and deliveries in the first three quarters of the year. And while production numbers continued on that upward trajectory in the fourth quarter, deliveries dipped by 10.2% from the previous quarter. That drop could signal softening demand for the company’s slate of premium EVs. Rivian produces four vehicles: its two consumer vehicles, the R1T truck and R1S SUV, and two versions of its commercial van.
Slowing demand for high-priced EVs has plagued automakers, forcing many to scale back production or cut prices. Rivian has managed to avoid slashing prices. If deliveries continue to fall into 2024, the automaker may have to take that strategy in an effort to boost its numbers. That would spell trouble for Rivian, which is not yet profitable.
The cost of building one Rivian vehicle continues to be a barrier to the company’s path toward profitability. Rivian’s consumer R1T and R1S vehicles sell for more than $80,000, on average. But the cost of building far exceeds any revenue it brings. In the second quarter, for instance, the company lost $32,495 on every vehicle it built.
Rivian said during its third-quarter earnings that it narrowed that per unit loss by $2,000 compared to the second quarter by simplifying its product portfolio and reducing material and labor costs. Cutting the sticker price would only put more pressure on margins.
On a full-year basis, Rivian produced 57,232 vehicles and delivered 50,122. This exceeded the automaker’s most recent full-year 2023 production guidance of 54,000 vehicles.
The company also announced that it will report fourth-quarter earning after markets close February 21.","https://techcrunch.com/wp-content/uploads/2023/05/rivian-Your_electric_adventure_awaits._Reserve_an_R1T_228.jpg?resize=1200,675",2024-01-02 14:40:40
https://www.innovationnewsnetwork.com/lake-tahoe-dive-survey-highlights-plastic-litter-problem/41427/,Lake Tahoe dive survey highlights plastic litter problem,"Scientists have teamed up with the nonprofit Clean Up the Lake to collect and analyse plastic litter found at the bottom of Lake Tahoe.
Plastic litter is a growing problem worldwide, and new research shows that the bottom of Lake Tahoe is no exception.
In one of the first studies to use scuba divers to collect litter from a lakebed, 673 plastic items were counted from just a small fraction of the lake.
The study, ‘Polymer Characterization of Submerged Plastic Litter from Lake Tahoe, United States,’ is published in Applied Spectroscopy.
Plastic litter in Lake Tahoe is an increasing issue
The Lake Tahoe study is the first of its kind to deeply analyse where plastic litter comes from.
The scientists took a closer look at the litter and broke it down into categories based on use (such as food containers and water bottles), followed by the chemical composition of the plastic.
The knowledge gained can help scientists better understand the source of large pieces of plastic litter in the lake and whether they’re a significant source of microplastics as larger pieces break down and degrade.
Previous research found that the waters of Lake Tahoe contain high levels of microplastics, defined as plastics smaller than a pencil eraser.
“There’s very little work on submerged plastic litter in lakes,” said Monica Arienzo, PhD, associate research professor of hydrology at DRI and one of the study’s lead authors.
“I think that’s a real issue because when we think about how plastics may be moving in freshwater systems, there’s a good chance that they’ll end up in a lake.”
What were the most common types of plastics?
Research divers swam transects along the lakebed near Lake Tahoe Nevada State Park and Zephyr Cove to collect the litter, covering 9.3 kilometres.
They found an average of 83 pieces of plastic litter per kilometre, with the lakebed near Hidden Beach and South Sand Harbor showing significantly more (140 items/km and 124 items/km, respectively). No stretches of the lakebed surveyed were free of plastic litter.
The most common plastic categories were food containers, bottles, plastic bags, toys, and several items that couldn’t be categorised.
“There’s a lot of education we can do, as well as continuing to work on reducing the use of those plastics,” stated Arienzo. “Because we must start thinking about turning that plastic pipe off.”
Working out where these plastics come from
Arienzo and co-author Julia Davidson, then an undergraduate student working in Arienzo’s lab, also identified the types of plastic that comprised 516 of the plastic litter samples.
Using an instrument that uses infrared light to fingerprint and identify the material, they found that the six most common plastics were polyvinyl chloride (PVC), polystyrene, polyester/polyethylene terephthalate, polyethylene, polypropylene, and polyamide.
Collecting this information can contribute to Arienzo’s ongoing microplastic research in the region, helping to identify where these small plastic fragments come from.
Davidson explained: “When we study microplastics, we only have the chemical information or the plastic type. We don’t know where it came from because it’s just a tiny piece of plastic.
“But now we can use this litter data to point to the dominant types of plastics and compare them to microplastic data.”
The study can help inform efforts by Tahoe-area communities to address plastic litter, such as South Lake Tahoe’s 2022 ban on single-use plastic bottles and Truckee’s ban on single-use food containers.
The research also highlights ways scientists can work with nonprofits to collect data to address local environmental concerns.",https://www.innovationnewsnetwork.com/wp-content/uploads/2024/01/shutterstockhappycreator_1810267096.jpg,2024-01-03 10:20:58
https://www.innovationnewsnetwork.com/new-synthetic-antibiotic-teixobactin-shown-effective-against-superbugs/41424/,New synthetic antibiotic teixobactin shown to be effective against superbugs,"Researchers from the University of Liverpool have developed a new synthetic antibiotic, teixobactin, which is more effective than established drugs against superbugs like MRSA.
A new study has demonstrated the potential of teixobactin, showing its potent activity against bacterial biofilms.
Associated with serious chronic infections in humans, biofilms are clusters of bacteria that are attached to a surface or to each other.
The study, ‘Development of teixobactin analogues containing hydrophobic, non-proteogenic amino acids that are highly potent against multidrug-resistant bacteria and biofilms,’ is published in the European Journal of Medicinal Chemistry.
New antibiotics are needed to combat antimicrobial resistance
Every year, almost five million people lose their lives due to antibiotic-resistant associated infections. Millions more people live with a poor quality of life because of current treatment failures.
In an AMR review commissioned by the UK Government, it is estimated that a further ten million people will succumb to drug-resistant infections each year by 2050.
The development of new antibiotics is therefore needed as a last resort when other drugs are deemed ineffective.
Developing and testing synthetic teixobactin
Led by Dr Ishwar Singh, an expert in antimicrobial drug discovery and development and medicinal chemistry, the team developed simplified synthetic versions of the natural molecule teixobactin. Naturally, the molecule is used by producer bacteria to kill other bacteria in soil.
The team tested different synthetic versions of the new antibiotic, optimising key features of the drug to enhance its efficacy and safety. The testing also allowed it to be produced inexpensively at scale.
An important step in adapting the molecule
In the latest study, the team designed and synthesised highly potent teixobactin analogues but swapped out key bottleneck building block L-allo-enduracididine. This was replaced with commercially available low cost simplified building blocks such as non-proteogenic amino acids.
As a result, the analogues are now effective against a broad range of resistant bacterial pathogens – including bacterial isolates from patients.
This is an important step in developing the new synthetic antibiotic.
Dr Ishwar Singh concluded: “Teixobactin molecules have the potential to provide new treatment options against multi-drug resistant bacterial and biofilm-related infections to improve and save lives globally.
“Our study provides a promising foundation for further research, and opens avenues to explore the application of teixobactin in various health-related biofilm contexts, including surgical site infections, implant-related surgeries, and cystic fibrosis patients.”
This work is funded by Innovate UK, the Department of Health and Social Care, and Rosetrees Trust.",https://www.innovationnewsnetwork.com/wp-content/uploads/2024/01/©-shutterstockTorjrtrx_1490621177.jpg,2024-01-03 10:01:17
https://www.innovationnewsnetwork.com/uk-governments-zev-mandate-comes-into-force/41421/,UK Government's ZEV mandate comes into force,"The UK’s zero emission vehicle (ZEV) mandate has become law today, marking a significant milestone in the country’s transition to electric vehicles (EVs).
The ZEV mandate outlines an ambitious framework for the UK to completely phase out the sales of internal combustion engine (ICE) vehicles by 2035 and accelerate EV uptake nationwide.
The regulatory framework was developed in collaboration with industry and car manufacturers. The government says the ZEV mandate will ensure the certainty automakers have asked for to safeguard skilled British jobs.
The introduction follows a string of government-backed incentives to accelerate the UK’s electrification, with the country aiming to improve its charging infrastructure and buying schemes.
Anthony Browne, the UK’s Technology and Decarbonisation Minister, emphasised the importance of the regulations: “Alongside us having spent more than £2bn in the transition to electric vehicles, our zero emission vehicle mandate will further boost the economy and support manufacturers to safeguard skilled British jobs in the automotive industry.
“We are providing investment certainty for the charging sector to expand our charging network, which has already grown by 44% since this time last year. This will support the constantly growing number of EVs in the UK, which currently account for over 16% of the new UK car market.”
What will the ZEV mandate change for the UK?
In September of last year, Prime Minister Rishi Sunak made the controversial decision to delay the ban on new ICE vehicles from 2030 to 2035.
The government said this was to align the UK with other leading economies such as France, Germany, Canada, and Sweden, allowing motorists more time to make the switch to EVs and improve the country’s charging infrastructure.
To drive this transition, the ZEV mandate dictates the proportion of newly manufactured zero emission cars and vans that must be produced annually until 2030.
By 2030, 80% of new cars and 70% of new vans sold in Great Britain are mandated to be zero emission vehicles, with a full transition to 100% by 2035.
Government incentives to boost EV uptake
The new laws will spur economic growth by aiding households in transitioning to electric vehicles. This will bolster the second-hand market for EVs and promote wider charging infrastructure across the nation.
Government initiatives to reduce upfront and ongoing EV ownership expenses encompass grants of up to £2,500 for small vans and £5,000 for large vans until 2025.
Additionally, residents in flats can receive a £350 discount on home charging points. Recent data highlights a 41% rise in newly registered zero emission vehicles.
These incentives appear to be working, with registrations of zero emission vehicles rising by 41% for the first time.
The UK’s charging network is rapidly expanding, boasting over 50,000 public chargepoints—a 44% increase from last year.
This progress positions the country strongly toward its goal of achieving 300,000 chargepoints by 2030.
However, despite this progress, recent reports from the RAC suggest that the UK’s deployment of ultra-rapid chargers across critical motorways has not reached its targets.
With the introduction of the ZEV mandate, the UK will look to fill these gaps in the country’s EV framework.",https://www.innovationnewsnetwork.com/wp-content/uploads/2024/01/©-shutterstockScharfsinn_1986668819.jpg,2024-01-03 09:40:32
https://venturebeat.com/ai/why-nyt-vs-openai-will-be-the-copyright-fight-to-watch-in-2024-the-ai-beat/,Why NYT vs. OpenAI will be the copyright fight to watch in 2024 | The AI Beat,"Join leaders in San Francisco on January 10 for an exclusive night of networking, insights, and conversation. Request an invite here.
No matter which side you think will win the battle, there is no doubt that the New York Times’ new lawsuit against OpenAI and Microsoft for copyright infringement, filed last Wednesday, will be one of the most important copyright fights to watch in 2024.
The Gray Lady vs. the Generative AI Bros? Pass the popcorn, please. The lawsuit maintains the defendants should be held responsible for “billions of dollars in statutory and actual damages” related to the “unlawful copying and use of The Times’s uniquely valuable works.” It also calls for OpenAI and Microsoft to destroy any chatbot models and training data that use New York Times copyrighted material.
Here’s why this case is so munch-worthy in what may be a make-or-break year for AI and copyright: For one thing, legal experts have long predicted that a case around AI and copyright could make it to the Supreme Court — and a high profile case like the New York Times vs. Microsoft and OpenAI could be just the ticket to a date with the nine justices.
Generative AI copyright battles have been brewing for over a year
Back in August 2022, before ChatGPT was released, I spoke to Bradford Newman, a partner at law firm Baker McKenzie in its Palo Alto office, about the legal battles brewing around generative AI.
VB Event The AI Impact Tour Getting to an AI Governance Blueprint – Request an invite for the Jan 10 event. Learn More
“Legally, right now, there is little guidance,” he warned. “There are the inevitable class actions, but the net net of it all is when you’re using the massive data sets that these AI applications are and you sprinkle on top of that open source licenses…the arguments are going to be fair use versus infringement.”
Different courts, he predicted, will come to different conclusions. “Ultimately, I believe this is going to go to the Supreme Court.”
In addition, industries affected by generative AI are not taking a wait-and-see attitude when it comes to copyright issues, particularly in areas such as publishing and entertainment. Marc Rotenberg, president and founder of the nonprofit Center for AI and Digital Policy, as well as an adjunct professor at Georgetown Law School, spoke to me in September for a deep-dive article into copyright issues around AI training data. He was waiting, he said, for the New York Times to make a legal move against OpenAI.
The New York Times-OpenAI matchup has arrived
He pointed to the famous Authors Guild v. Google, a federal copyright case that centered on whether Google Books’ effort to digitize books constituted “fair use,” and wound its way through the courts for a decade before being decided in Google’s favor in 2015.
“Google did very well there in the outcome as a legal matter, but the publishers and the news industry did not, and there was a real consequence that the publishing industry and journalism is in trouble,” Rotenberg told me. “It’s the reason that the New York Times, which survived the first battle, even as many regional and local news organizations collapsed, is suddenly aware that they may be in trouble too.”
The big takeaway, he continued, is that if AI models, which need high-quality data, could also replicate what the New York Times offers, the news giant could go out of business. “So to me, it’s one of the most interesting legal cases today involving AI…the looming battle between one of the most well regarded publishers, The New York Times, and one of the most impactful generative AI firms OpenAI.”
OpenAI lawyers are ready to play defense
There is no reason to doubt that OpenAI and Microsoft are fully-prepared to play strong defense in this case. Back in July, I spoke to a prominent attorney specializing in copyright law who is helping to defend OpenAI in at least one of the cases filed in 2023 by authors accusing the company of infringing their copyrights. He told me that the cases focused on generative AI and copyright will play a huge role in “shaping the pace and contours of innovation” as well as the understanding the role of copyright law.
“I think every 20 years or so, there’s a new, really significant question that comes along and forms how the commercial world works,” he said. “We had a case in the early 1980s that went to the Supreme Court over the VCR. We had a whole wave of cases over Web 2.0 to figure out how this new law called the Digital Millennium Copyright Act works, or user generated content. I think what we’re seeing right now is the next big wave of litigation over these tools that are going to, if you ask me, have a profound effect on society.”
At the time, he told me was “quite confident” that the position of the technology companies on copyright “is the one that should and hopefully will prevail,” adding that copyright is often invoked to solve problems it was not meant to solve.
“It’s just really bad for using the regulated labor market, or privacy considerations, or whatever it is — there are other bodies of law that deal with this concern,” he said. “[Copyright is] just not the right instrument for that. And I think happily, courts have been sort of generally pretty faithful to that concept.”
But either way, he emphasized, changing copyright law for the sake of protecting any industry is a “really terrible decision” that will never work because the laws will not apply outside the US. “These tools are going to continue to exist,” he said, and there will be a tremendous amount of jurisdictional arbitrage in terms of where these companies are based, in terms of the location from which the tools are launched.”
Simply put, he said: “You couldn’t put this cat back in the bag.”",https://venturebeat.com/wp-content/uploads/2024/01/Untitled-design-2024-01-02T105616.348.png?w=1200&strip=all,2024-01-02 17:29:48
https://venturebeat.com/business/the-stephens-group-llc-announces-sale-of-summit-industrial-construction-llc-to-comfort-systems-usa/,"The Stephens Group, LLC Announces Sale of Summit Industrial Construction, LLC to Comfort Systems USA","LITTLE ROCK, Ark.–(BUSINESS WIRE)–January 2, 2024–
The Stephens Group, LLC (“The Stephens Group”) today announced that it has entered into a definitive agreement to sell Summit Industrial Construction, LLC (“Summit”) headquartered in Houston, Texas, to Comfort Systems USA, Inc. (NYSE: FIX) (“Comfort Systems”).
Summit is a specialty industrial mechanical contractor offering engineering, design-assist and turnkey, direct hire construction services of modular systems serving the advanced technology, power, and industrial sectors. Summit’s capabilities encompass a wide range of modular and site-based construction including process piping, equipment setting, large pipe rack trestles, and related steel erection and specialty concrete work. Summit is a trusted supplier to some of the world’s largest advanced technology, power and industrial companies and is currently deployed on several major chip fabrication projects.
“It’s been a pleasure to partner with Jeff since the founding of Summit Industrial. He and the talented team at Summit have done a remarkable job building their capabilities to become a leading industrial contractor. We are excited about this transaction for them and the future for the company under Comfort Systems,” said Witt Stephens, CEO and Co-Chairman of The Stephens Group.
Jeff Johnson, Chief Executive Officer of Summit, commented, “The Stephens Group has been a wonderful and patient partner the last twelve years. With their support, we added key personnel to our team, evaluated several acquisition targets, and strategically shifted our end markets of focus. We are excited to continue our growth with Comfort Systems. We chose Comfort Systems based on our belief that we can leverage their complementary capabilities and assets to better serve our customers. Summit and Comfort Systems have a strong cultural alignment and a shared commitment to team members at all levels. Together we can continue our strategic focus and provide even more value to customers with additional capabilities and resources. We believe that Summit and Comfort Systems have a bright future together.”
About The Stephens Group, LLC
The Stephens Group, LLC is a private investment firm that partners with talented management teams to help build valuable businesses. Backed by the resources of the Witt Stephens, Jr. and Elizabeth Campbell families, the firm combines the operational expertise of a private equity firm with the flexibility provided by long-term capital. With over $2 billion of private equity assets under management, the firm has a long history of providing informed, sophisticated expertise and working with owners and managers to help them successfully achieve their strategic visions and build long-term value. Since 2006, The Stephens Group has invested in over 50 companies, targeting investments in industries across the U.S., including industrial and commercial products and services, specialty distribution, technology infrastructure, and vertical software.
View source version on businesswire.com: https://www.businesswire.com/news/home/20240102057311/en/
Allie Laborde
Principal, Business Development
The Stephens Group, LLC
pressreleases@stephensgroup.com
501.320.0595",https://venturebeat.com/wp-content/uploads/2015/10/BusinessWire_FeaturedImage.jpg?w=1200&strip=all,2024-01-02 16:25:59
https://venturebeat.com/programming-development/3-big-money-ai-jobs-in-the-u-s-tech-industry-to-banish-your-january-blues/,3 big-money AI jobs in the U.S. tech industry to banish your January blues,"Join leaders in San Francisco on January 10 for an exclusive night of networking, insights, and conversation. Request an invite here.
If you’re trying to get back in the saddle after the holidays, it can be tough to motivate yourself. Making a career move can seem like a very exciting prospect — especially if facing another 12 months at your current role is filling you with dread.
You’re not alone if you do have a bad case of the January blues: according to jobs site Indeed, January is one of the most popular months to look for a new job, in part thanks to the fact that there are the largest numbers of open jobs available, as well as a high volume of applications.
When it comes to the sort of tech roles that are hot tickets for 2024, AI is out in front. In fact, according to new research from Bizreport, “the rise of artificial intelligence (AI) is reshaping the American job market”.
Not only do AI-related jobs offer 77.53% higher salaries than other occupations, but the pay gap between tech jobs and other occupations has also widened by 36% due to artificial intelligence.
VB Event The AI Impact Tour Getting to an AI Governance Blueprint – Request an invite for the Jan 10 event. Learn More
And in even better news for those with the necessary skills is the fact that in 2023, 30.3% of computer science jobs were AI-related. This year, the picture looks even brighter with the potential for more than 131,000 AI-related roles in the computer science market.
So, if you are considering a job move, high-paying roles across the AI gamut are on offer right now. Explore those on the VentureBeat Job Board, or drive into the three roles highlighted below.
AI Architect, Salesforce, Dallas
A salary scale of $233,300 to $355,900 a year is on offer for this AI Architect job at Salesforce in Dallas. You should be a highly accomplished and transformative enterprise architect with a strong emphasis on machine learning (ML) design, solutioning and gen AI to play a pivotal role in shaping the technological landscape of the organization.
In this influential position, you will have the opportunity to be responsible for the organization’s ML and Gen AI initiatives, drive innovation and ensure that solutions are not only innovative, but also strategically aligned with business objectives.
To be considered, you’ll need proven experience as an enterprise architect or in a similar senior technical role, with a track record of building large-scale innovative AI products and solutions.
Strong knowledge of machine learning algorithms and predictive analytics is also necessary as is hands-on experience productionizing ML models (prototyping, training and serving) using ML frameworks such as AWS Sagemaker, Google Vertex, etc., as well as a strong familiarity with generative AI concepts and technologies. Find out more here.
AI Infrastructure Engineer, Intel, Santa Clara
Up to $250,410 a year is on offer for the right candidate for this AI Infrastructure Engineer role at Intel, where you will drive customers’ technical engagement with design-in, onboarding and architecting solutions.
You’ll also be the technical expert on all accelerator products, work with customers on their data center design with DAIS accelerators, including rack, power, network, storage, coolin, and CPU host, and give advisory on data center customers’ system validation/qualification process and success criteria.
To be considered, you’ll need a deep understanding of key AI and datacenter technologies across silicon, system and software, an understanding of ecosystem players (OxM, ISV and partners) and strong communication and presentation skills with the ability to effectively convey complex concepts to both technical and non-technical stakeholders. If this sounds like the job for you, apply here.
Generative AI Engineer, Booz Allen Hamilton, McLean
Booz Allen Hamilton is looking for a Generative AI Engineer with technical knowledge and problem-solving abilities to identify and develop opportunities for the use of AI, ML and LLMs to solve real-world business and operational problems that will help the company execute missions of national importance.
A pay scale of $106,200 – $242,000 a year is on offer for the job, which will see the right candidate architect and build new generative AI-based products and offerings, as well as collaborate with a community of research and experimentation leaders, software and IT operations engineers, AI and ML experts, data scientists, solution architects, systems engineers and product owners to deliver world-class solutions.
If you have six or more years’ of experience in software engineering, including leading teams to build customer-facing solutions and products, as well as experience with integrating large language models or multi-modal models with applications or user interfaces and have led a collaborative, cross-functional team environment, consisting of security engineers, data scientists, machine learning engineers and designers, then discover more about the job now.",https://venturebeat.com/wp-content/uploads/2024/01/nuneybits_vector_art_of_a_developer_sitting_in_front_of_a_scree_22e9f6b5-f27c-4ab6-a089-29b913248fae-transformed.png?w=1200&strip=all,2024-01-02 15:58:01
