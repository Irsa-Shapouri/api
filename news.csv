Link,post_title,post_content,featured_image,Date-Publish,post_category,post_tag
https://techxplore.com/news/2024-03-large-language-simple-mechanism-knowledge.html,Large language models use a surprisingly simple mechanism to retrieve some stored knowledge,"Large language models (LLMs) like ChatGPT are incredibly complex and widely used in various applications. However, scientists still have limited understanding of how these models work. To gain insights into their functioning, researchers at MIT and other institutions delved into the mechanisms behind LLMs' retrieval of stored knowledge. Surprisingly, they discovered that LLMs often employ a simple linear function to recover and decode facts. These linear functions, which have two variables and no exponents, capture the straightforward relationship between variables. By identifying linear functions for different types of facts, the researchers were able to probe the model and locate where knowledge is stored. Even when a model provides incorrect answers, it often retains the correct information. This discovery opens up possibilities for using this approach to identify and correct falsehoods within the model, reducing the occurrence of incorrect or nonsensical responses.

According to Evan Hernandez, an electrical engineering and computer science graduate student at MIT and co-lead author of the research paper, LLMs may seem complicated due to their nonlinear functions and extensive training on data, but there are instances where simple mechanisms are at work. Hernandez co-authored the paper with Arnab Sharma, a computer science graduate student at Northeastern University, along with Jacob Andreas, an associate professor in EECS and a member of CSAIL, David Bau, an assistant professor of computer science at Northeastern, and other researchers from MIT, Harvard University, and the Israeli Institute of Technology. The findings will be presented at the International Conference on Learning Representations (ICLR 2024) in Vienna.

LLMs, also known as transformer models, are neural networks that consist of interconnected nodes or neurons organized into multiple layers for data encoding and processing. The knowledge stored in these models can be represented as relations connecting subjects and objects. For example, the relation ""Miles Davis plays the trumpet"" connects Miles Davis as the subject to the trumpet as the object. As a transformer gains more knowledge, it stores additional facts about a subject across its layers. When a user queries the model about a subject, it must decode the most relevant fact to generate a response.

To understand how transformers decode relational information, the researchers conducted a series of experiments. Despite the complexity of LLMs, they found that these models utilize a simple linear function for decoding. Each function is specific to the type of fact being retrieved. For instance, the transformer uses one decoding function to output the instrument a person plays and a different function to output the state where a person was born. The researchers developed a method to estimate these linear functions and computed functions for 47 different relations, such as ""capital city of a country"" and ""lead singer of a band."" Although there are countless possible relations, the chosen subset represents typical facts that can be expressed in this manner.

The researchers tested each function by altering the subject to see if it could recover the correct object information. The results showed that the linear functions were effective in retrieving the desired facts. However, there were some relations for which the linear function approach did not yield satisfactory results, indicating the presence of a non-linear decoding process.

This research sheds light on the inner workings of LLMs and their ability to retrieve and decode stored knowledge using simple linear functions. Understanding these mechanisms could contribute to the development of techniques for identifying and rectifying falsehoods within the model. By reducing the occurrence of incorrect or nonsensical answers, these models can become more reliable and trustworthy in various applications.",https://scx2.b-cdn.net/gfx/news/hires/2024/large-language-models-2.jpg,2024-03-25 09:34:06,Innovation,Innovation
