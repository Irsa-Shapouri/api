Link,Title,Text,Image,Date Publish
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/how-ceos-can-change-culture-reduce-burnout-and-retain-talent/,"How CEOs Can Change Culture, Reduce Burnout And Retain Talent","Marc is CEO at Devo, a cloud-native logging and security analytics solution.
getty
One of the top challenges facing CISOs and security teams today isn’t just the security risks they are being bombarded with; it’s burnout. This directly ties into security risks in that if you're lacking staff, or if the existing staff you have is burnt out, your assets aren’t being optimally protected. In fact, as we found in our recent research with Wakefield Research, 83% of IT security professionals have seen burnout lead to errors that resulted in a security breach; 39% of them have experienced this situation more than once.
Heightened security concerns can also contribute to an even greater sense of burnout. Because keeping the network and its assets safe is such a huge responsibility, security teams are under constant pressure. According to our research, half of IT security staff point to ongoing staff shortages as a top contributor to stress. Their jobs can seem quite thankless, too, since people typically only think about security when something has gone wrong. No wonder they’re burnt out.
The conviction of Uber’s former CISO embodied many security leaders’ fears that they could be the sacrificial lambs if an attack occurred under their watch. This is an extreme example of an executive’s willful obstruction, but security professionals can’t help but wonder if they’ll somehow be blamed if a security incident gets ugly.
Burnout and mental health in this sector aren’t issues for just the security team to deal with—it needs to come from the top. Leaders in the C-suite need to understand why burnout is happening and its implications, then work to help change the culture to avoid or mitigate these issues. This is no easy feat, but the status quo can’t remain unchallenged if you want a content, retained security workforce. That’s particularly important considering the ongoing cybersecurity skills gap.
Looking At Burnout
In our survey, we aimed to dig deeper into what’s really going on with security professionals who are the lifeblood of almost any organization’s cybersecurity efforts. These are the teams who are grappling day in and day out with cyber threats and round-the-clock monitoring.
What we uncovered provided us with a wealth of insights related to burnout and mental health challenges, including:
• Eighty-five percent of IT security professionals predict they'll need to leave their company or role because of burnout, and 24% say they may leave cybersecurity entirely.
• For 77% of respondents, stress levels at work are having a direct impact on keeping private customer data safe.
In another survey we conducted, we also found that hiring, especially for the security operations center (SOC), continues to be a problem.
• Less than one-quarter of respondents say they can typically fill a vacancy in less than a month, and the average time spent filling a position is seven months.
• Fifteen percent of SOC leaders said it takes over two years to hire for a SOC role. You can imagine the impact this has on everyone else who has to pick up the slack.
On top of this, there’s an ongoing shortage of cybersecurity professionals—to the tune of 3.4 million skilled cyber pros globally. There’s no sugarcoating it: the problem is bad. So, now what?
How Leaders Can Take Action
We are living in a time when conversations about burnout and mental health are happening in a much more open fashion than ever before. That’s great because conversations are key and an important first step.
One thing we’ve seen in the last few years, especially coming out of the pandemic, is more willingness to talk about these challenges and treat them as the socioeconomic matters they are as opposed to being considered an individual’s problems—or shrouding these challenges in shame and secrecy.
Mental health must become a company priority with dedicated resources and time. From a leadership and management perspective, there are a number of ways to do this beyond what’s being done at the HR level.
For instance, enable your CISOs and other security leaders to connect and network with their peers. Make the time for them to participate in CISO dinners and networking events where they can share stories with others and get guidance from peers who are grappling with some of the very same challenges day in and day out.
It’s also important to invest in more training and education. Bad actors are continually changing and evolving their tactics, so education and training need to likewise happen regularly. This should be a key aspect of corporate directives. Training should extend across departments, not just the SOC. Giving everyone more access to cyber hygiene training can go a long way toward reducing risk and thereby limit some of the noise coming into the SOC.
Looking To External Resources For Help
Just as there’s no shame in staff admitting they’re struggling, there’s no shame in asking for outside assistance. You are not alone in this challenge. Burnout is a huge problem across many sectors, not just in security, but there are obviously many factors specific to security that make this issue especially concerning.
There are organizations like Cybermindz, a partner of ours, that are focused on improving the burnout problem and addressing mental health in cybersecurity. Such organizations will work with you to help your staff achieve and retain mental well-being in a way that fits with your unique needs.
Steer The Ship To Well-Being
Burnout isn’t a new phenomenon in the cybersecurity space, but it remains a huge problem, one that’s exacerbated by the ongoing skills gap. Organizations struggle to hire and retain staff, and there is a revolving door when it comes to SOC analysts in particular. A burned-out SOC analyst isn’t effective, and it doesn’t set a good precedent for your company’s culture, either.
Your CEO role gives you both the authority and the mandate to navigate your CISO and your SOC through the treacherous landscape of burnout. Culture change comes from the top down, so use the above suggestions to support your security leaders and their teams in every way you can.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/652da8a9e1225ef6f2760e75/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:45:00
https://www.forbes.com/sites/sap/2023/10/17/147b-as-a-service-market-is-linchpin-to-unleash-renewable-energy/,$147 Billion As-A-Service Market Is Linchpin To Unleash Renewable Energy,"As people install solar panels on their rooftops, heat pumps in their basements, and EV charging stations in their communities, electric utilities are exploring service-based opportunities slated to upend traditional energy industry models and address climate change.
Electricity can be a less expensive and more accessible energy source, particularly in regions like Europe that depend on costly imported and increasingly scarce fossil fuels. Driven by growing demand for renewable energy resources amidst supply chain uncertainties – no one in Europe has forgotten last winter’s fossil fuel crisis – energy from the electrical grid offers utilities a viable growth path in a sustainable world. In fact, the global energy-as-a-service market (EaaS) is projected to reach $147.56 billion by 2029.
Servitization helps customers and providers track the carbon footprint of households, businesses, and communities for improved energy management. getty
Servitization powers new revenue streams for utilities
Forward-looking utilities have begun experimenting with as-a-service bundles that encompass customer advice, hardware installation, financing, and energy management software. IDC researchers predicted that by 2025, one-third of competitive gentailers, meaning retail energy providers that also own power generation assets, will have set up integrated supply, efficiency, decarbonization, and electrification service portfolios, growing average profit per customer by more than 20%.
Meantime, consumers are expected to become prosumers who use and produce electricity. In some cases, customers are turning into flexumers who also provide flexibility to the grid, shifting energy usage dynamically with load demands and resource availability throughout the day and night.
“With rooftop solar panels, customers can produce electricity, potentially competing with traditional utility business models,” said Daniela Sellmann, global vice president and head of energy and utilities industries at SAP. “Servitization from connected data between infrastructure like solar panels or charging stations can help providers better distribute or store renewable energy to improve cost efficiency and grid management while generating new revenue streams.”
Trusted customer relationships from digitalization
Broadly speaking, EaaS is just emerging with subscription-based models that offer customers unique benefits. For example, a company or consumer can lease the solar panels, heat pumps, or air conditioning units they need at an affordable fee per month minus a large upfront capital investment. Analyzing a customer’s profile that includes energy usage patterns over time, utilities can personalize service-based offerings.
“EaaS can be simplified on a digital platform connecting intelligent data from onsite equipment in a home or business facility,” said Sellmann. “Similar to other industries like manufacturing, we’re applying SAP’s business solution expertise to help utilities manage servitization end-to-end, including order management and billing, as well as proactive offerings. Customers could compare and select EaaS bundles on a digital commerce site. The utility can offer complimentary equipment or loyalty coupons based on the customer’s energy usage and customer lifecycle value. This would help utilities build trusted customer relationships.”
Data transparency eases renewable energy transition
Unlike traditional utility business models, EaaS requires co-innovation with commercial customers and partners along the energy value chain. As providers add new services, they can integrate data from the solar panel or EV charging station manufacturers, banks that offer financing deals, and service technician support. A bundled offering might include everything from hardware installation through field service management aligned to the contractual agreement. Customers and providers could track the carbon footprint of households, businesses, and communities for improved energy management throughout the lifetime of a customer relationship.
“During project planning for say, a new real estate development, data transparency can help organizations and utilities capture expected energy usage from renewables. Organizations can track actual performance against corporate sustainability commitments and regulatory targets,” said Sellmann. “With the right technologies, utilities are well-positioned to help companies and consumers evolve to renewable energy.”
EaaS supports sustainable energy choices
Natural resources like sun and wind are central to global decarbonization, but they also add new dimensions of volatility to energy management. Leaders in the utility industry are rethinking how to partner with consumers, business, and the public sector to roll-out new services that people can easily find and use in this changed environment.
“We’re excited about the opportunities as society transitions to renewable energy, and utilities adopt EaaS business models,” said Sellmann. “We envision advanced technologies like generative AI will further benefit both utilities and customers. Our newly announced generative AI copilot Joule, which will be embedded into SAP solutions, directly supports the industry’s evolution to EaaS business models.”
Learn how companies in five industries are reshaping their future by focusing on service excellence: Read the IDC InfoBrief",https://imageio.forbes.com/specials-images/imageserve/6523fbf72c5009d9935ef940/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:30:00
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/theres-a-way-out-of-technical-debt-with-more-strategic-software-planning/,More Strategic Software Planning,"Carlos M. Meléndez is the COO and cofounder of Wovenware, a Maxar Company, offering AI and software development services.
getty
Software development teams, with projects ranging from deployment of a cloud-based CRM to modernizing an invoice processing application, can be overwhelmed with not only the enormity of their tasks but the clamoring coming from department heads, all thinking their project is the most critical. With faster development cycles and continuous software iterations, it can be oh-so-tempting to take the path of least resistance and simply build it in the fastest way possible with no regard for how it fits into a long-term strategy.
The resulting software may seem to do the trick and meet the immediate need. It may even help the development team look like the hero who saves the day with a quick-fix solution. However, those disparate software development projects add up, and before you know it, you're way deep in technical debt and struggling to stay afloat.
Technical debt is the costs and problems that occur when software is deployed with a quick-fix mentality without a strategic long-term plan in mind or because cost or speed was selected over quality. The end result is what is known in the industry as ""cruft""—poorly written or redundant code.
There are often many good reasons why it creeps up. Budget concerns can delay upgrades to legacy systems, limited software features and functionality might be deemed ""good enough,"" or other projects might be considered higher priority. Almost always, however, once technical debt sets in, companies struggle with higher costs for future upgrades or fixes, or they struggle with productivity losses since systems can't keep up with changing needs. In fact, according to author and speaker Martin Fowler, ""high-quality software is cheaper to produce.""
Another reason for technical debt (especially within larger enterprises) is the sheer size of the tech stack, which can be impossible to manage thanks to rogue software development projects containing duplicate solutions, outdated technologies and incompatible systems.
Designing For The Future
How can companies avoid technical debt in the first place? Consider the following best practices.
Start With Object-Oriented Design
Progressive software development practices such as object-oriented design, which organizes software design around objects (data) rather than functions and logic, enable developers to make changes to software without having to alter its structure. If object-oriented design is deployed from the outset, developers can ""tweak"" objects and optimize programs without changing the structure of the software. Some types of object-oriented programming languages include Java, Python or C++.
According to Fowler, many people in the software industry compare building software to constructing a building. While new floors or additions can be added, the fundamental structure of the building can't be changed. If the infrastructure is sound and strategic, it will allow for future upgrades.
Fowler noted that building software can be quite uncertain and changing. Customers are never really sure what they need in their software or how it will be received by customers. It needs to be adaptable without having to bulldoze the whole building whenever changes have to be made. Object-oriented design can more easily enable developers to keep software current.
Train Software Developers On Object-Oriented Design
The role of object-oriented design isn't discussed much in universities, and many software developers have been trained in traditional functional programming. Many developers know how to code in the traditional way, but I've noticed that the notion of objects seems to be relatively new. Part of the problem is that traditional coding practices are much easier to follow, and there is much more talent to go around. Yet object-oriented design is more specialized and can require more upfront time.
It's important, therefore, for universities to expose students to object-oriented programming and for software firms to hire developers who are trained in it. While it may require more upfront time, it enables them to break the program into bite-sized parts that can be tackled one object at a time to enable higher-quality software, reduced maintenance costs and more productive developers.
Clean Code As You Go
The best way to remove cruft in software projects is to avoid it in the first place. Object-oriented design, better technical oversight and quality, and more strategic planning can help to prevent it, but cruft happens. According to Fowler, removing cruft is like cleaning as you go while cooking. There's no way to avoid dirty dishes, but if you don't clean them as you go, it will be really hard to remove the crud later. By removing as much cruft as possible in programming, technical debt is reduced since quality software comprises the tech stack.
Conduct Regular Code Reviews
It's important to have software programs reviewed regularly to understand the coding logic and approach taken. An internal code quality and review council can help with this, also helping to set standard operating procedures and policies across the company. These reviews can result in the need to refactor or rewrite code or establish a means of gradual improvement.
These reviews can help to address technical debt by establishing parties or teams responsible for software development oversight.
Understand That Coding Is As Much An Art As It Is A Science
It's well-known in the software industry that programmers take great pride in their code. Every programmer likes to write software, but very few like to maintain someone else's software. By establishing a culture of quality and appealing to a programmer's pride of work, good software can be created from the beginning,
Like bills, credit card payments or expenses, debt can gradually creep up until you're drowning in it. The same goes for technical debt. The key to avoiding it is to work to pay it down, replacing traditional unwieldy programming or knee-jerk technology purchases with strategic, future-proof solutions that don't compromise quality for speedier delivery.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/63e566d03907607403b58ccf/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:30:00
https://www.forbes.com/sites/grrlscientist/2023/10/17/twenty-one-species-removed-from-the-endangered-species-act-due-to-extinction/,Twenty-One Species Removed From The Endangered Species Act Due To Extinction,"Delisting species due to their extinction signals a ‘wake-up call on the importance of conserving imperiled species before it’s too late’
© Copyright by GrrlScientist | hosted by Forbes | LinkTr.ee
The Hawaiian po’ouli (Melamprosops phaeosoma) or black-faced honeycreeper, now declared extinct, ... [+] according to the U.S. Fish and Wildlife Service. (Credit: Hawai'i DLNR Division of Forestry and Wildlife via the Center for Biological Diversity.) Hawai'i DLNR Division of Forestry and Wildlife via the Center for Biological Diversity
The U.S. Fish and Wildlife Service announced yesterday that they will delist 21 species from the Endangered Species Act because they are extinct. Found in 16 states and in the U.S. territory of Guam, most of these species were listed under the ESA in its early days in the 1970s and 80s and were in very low numbers or likely already extinct when listed.
According to the statement, one mammal (the Little Mariana fruit bat from Guam), eight freshwater mussels from the Eastern Seaboard, two freshwater fish and 10 birds were declared extinct after being listed as critically endangered for decades. Eight of the delisted birds were endemic to Hawaii, with the po’ouli, Melamprosops phaeosoma, or black-faced honeycreeper, last seen in 2004, making it the most recently sighted of the delisted species.
Other Hawaiian birds may soon become extinct too: after the massive wildfires triggered by climate breakdown roared across much of Maui, the critically endangered ʻAkikiki, Oreomystis bairdi, or Kauaʻi honeycreeper, is now reported to have a population of just five individuals (ref), down from a total of 454 wild individuals reported in 2018 (ref).
“Few people realize the extent to which the crises of extinction and climate change are deeply intertwined,” said Noah Greenwald, endangered species director at the Center for Biological Diversity.
“Both threaten to undo our very way of life, leaving our children with a considerably poorer planet. One silver lining to this sad situation is that protecting and restoring forests, grasslands and other natural habitats will help address both.”
A critically endangered `Akikiki or Kaua`i Honeycreeper (Oreomystis bairdi), a Hawaiian honeycreeper ... [+] that currently has a wild population of just five individuals. (Credit: Carter Atkinson, USGS / Public domain) Carter Atkinson, USGS / Public domain
“Federal protection came too late to reverse these species’ decline, and it’s a wake-up call on the importance of conserving imperiled species before it’s too late,” said USFWS Director Martha Williams in a statement.
These extinctions highlight the importance of the ESA and efforts to conserve species before declines become irreversible. The circumstances of each extinction also underscores how human destructiveness can drive species declines and extinctions through habitat loss, shooting and other forms of exploitation, the introduction of invasive species, particularly cats, rats and pigs, and diseases.
A total of 650 species have already been listed as extinct in the United States, according to the Center for Biological Diversity, a national U.S. nonprofit wildlife conservation organization (ref). Scientists have been warning the public for decades that Earth is experiencing a mass extinction event, which is defined as the loss of more than 75% of its species (more here) in less than 2.8 million years. Unlike the previous mass extinctions, which resulted from extreme temperature changes, rising or falling sea levels and catastrophic, single events such as huge volcanic eruptions or an asteroid crashing into Earth, this current mass extinction event is solely due to human destruction.
“As we commemorate 50 years of the Endangered Species Act this year, we are reminded of the Act’s purpose to be a safety net that stops the journey toward extinction,” Ms Williams reminded us in a statement.
“The ultimate goal is to recover these species, so they no longer need the Act’s protection.”
The ESA has generally been highly effective and is credited with saving 99% of listed species from extinction. Thus far, more than 100 species of plants and animals have been delisted based on recovery or reclassified from endangered to threatened based on improved conservation status, and hundreds more species are stable or improving thanks to the collaborative actions of Tribes, federal agencies, state and local governments, conservation organizations and private citizens.
The final rule to delist these 21 species from the ESA due to extinction will be published today, 17 October 2023, in the Federal Register and will go into effect in 30 days.
Two Reprieves
Two endangered species were granted a temporary reprieve. After the USFWS proposed to delist 23 species in September 2021, public comments informed them that two species may yet cling to life. The first, a Hawaiian perennial mint, Phyllostegia glabra var. lanaiensis, may possibly still be found in newly identified, potentially suitable habitats for the species.
Chris Pague of the Nature Conservancy holds a museum specimen of Ivory Billed woodpecker ... [+] (Campephilus principalis) found in 1894 on the Aucilla River in Jefferson County Florida. The bird, part of the Denver Museum's collection, is probably extinct after the last confirmed sighting in 1944. Recently there was a reported rediscovery of the Ivory-billed woodpecker at the Cache River National Wildlife Refuge in Arkansas. (Credit: RJ Sangosti / The Denver Post via Getty Images) Denver Post via Getty Images
The other species, the ivory-billed woodpecker, Campephilus principalis, also known by a variety of common names including the Lord Gawd Bird, because it was the largest woodpecker species in the United States, is still under review. Despite all the squabbling amongst bird watchers (and even a few professional ornithologists) over the past 20 years, along with hundreds of thousands of hours of exhaustive searching by hundreds of skilled observers, nothing definitive has been reported, making it likely this dramatic bird is truly gone forever. This showy species is likely extinct due to uncontrolled shooting and rampant logging of old-growth bottomland hardwood forests and old-growth temperate coniferous forests of the Southeastern United States and Cuba, where it lived.
SHA-256: 9ab94921e06b203a216cb219d873f92ea4083642075e2e0be632939cd42949aa
Socials: Bluesky | CounterSocial | LinkedIn | Mastodon | MeWe | Post.News | Spoutible | SubStack | Tribel | Tumblr | Twitter","https://imageio.forbes.com/specials-images/imageserve/652e67b2aa2b82de4b6d76e5/0x0.jpg?format=jpg&crop=1140,640,x0,y0,safe&height=900&width=1600&fit=bounds",2023-10-17 08:16:52
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/generative-ai-a-new-era-for-data-analysis-and-interpretation/,Generative AI: A New Era For Data Analysis And Interpretation,"CEO and cofounder of Unacast, a location data and insights platform.
getty
Data has always been the compass guiding businesses. But the modern landscape demands more than just data; it craves insights. Despite the presence of sophisticated dashboards and graphs, the interpretation often remains confined to those with a deep understanding of the data.
I see how our clients, who make site selections, streamline retail operations and make crucial investment decisions, have a significant need for clear and comprehensible narratives to make better decisions.
This is where I believe generative AI will be a game-changer, bridging the gap between numbers and narratives.
The Need For Narratives In Data Interpretation
A dashboard filled with graphs, pie charts and scatter plots can be daunting. Each visualization presents data points, trends and patterns. But how many viewers can understand the stories lying within these numbers? While a seasoned analyst might see a trend or anomaly, for many others, it remains just a collection of numbers and lines.
This is where narratives come in. Stories and contextual descriptions can breathe life into numbers, making them more relatable and understandable.
Generative AI: The Bridge Between Numbers And Narratives
Generative AI, with its capability to produce content, can transform data into rich and helpful narratives.
I see three ways in which the technology will improve data interpretation.
1. Bridging The Knowledge Gap
Not everyone is a data enthusiast. When a dashboard is shared across departments, it's essential that everyone, from retail operations and research to investment and executive teams, understands the insights. Generative AI can convert data patterns into plain, relatable language.
For businesses, particularly in the retail sector, choosing a location is pivotal. Site selection depends on many factors: demographic data, traffic patterns, competition analysis and more. While data points can provide these numbers, what if generative AI could narrate a story?
For instance, instead of just presenting a chart with foot traffic, the AI could generate an insight like, ""The location of your interest sees 20% higher foot traffic during weekends and has two colleges within a three-mile radius, making it an ideal location for businesses targeting the youth demographic.""
Such narratives can significantly enhance decision-making, offering companies a clearer view of potential site benefits and challenges.
2. Providing Enriched Context
Beyond just the current data, understanding trends often requires historical or comparative context.
AI, with its ability to pull from extensive databases, can compare current data with past trends, providing a more comprehensive view.
For instance, if there are changes in trade areas, foot traffic or demographic changes, AI can compare those observations with similar past events and generate insights about potential causes or implications.
3. Boosting Time Efficiency
One of the primary roles of a data analyst or researcher is not just to analyze data but to explain it. Crafting meaningful interpretations takes time.
Generative AI can assist in this process, automating the generation of basic insights and summaries so the numbers and graphs actually have a meaning—also for those not as data savvy.
I have seen numerous times how analysts and researchers, highly skilled and highly paid professionals, spend too much time writing and creating slides for their executive team. Generative AI allows these professionals to dedicate more time to strategy, planning and diving into more complex analysis—what they excel at.
Generative AI As A De Facto Part Of The Workflow
While generative AI holds tremendous potential, it's essential to understand that it's a tool designed to complement human skills, not replace them. A machine can provide narratives in a different and more efficient way than a person, but the deeper, nuanced understanding and strategic application of data will always require the human touch of a data analyst or a researcher looking to inform their teams and colleagues.
The future of data interpretation, especially in sectors like site selection, retail and investment, isn't just about presenting numbers. It's about narrating stories that resonate, insights that inform and analyses that drive action.
Generative AI, with its capability to craft narratives from data, prompts the question of not if but how soon we can integrate generative AI into our data interpretation workflows.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/5ffe0a25c386c343ec2b0a9b/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:15:00
https://www.forbes.com/sites/erikkain/2023/10/17/spider-man-2s-glowing-ps5-reviews-gloss-over-one-glaring-problem/,Spider-Man 2’s Glowing PS5 Reviews Gloss Over One Glaring Problem,"Spider-Man 2 Credit: Sony
When I reviewed Spider-Man back in 2018 when it slung its way onto PS4, I gave it a near-perfect score of 9.5 out of 10. It was just so close to a perfect video game, only held back by a few annoying things like forced stealth missions that were super tedious and had no business disrupting the flow of the good stuff.
MORE FROM FORBES 'Spider-Man' (2018) PS4 Review: The Good, The Bad And The Spidey
Reviewing the sequel five years later on the PS5, my colleague Paul Tassi is slightly less generous, giving Spider-Man 2 a still very respectable 8.5/10 (though it’s the kind of score that makes fanboys very, very angry). Paul’s biggest complaint about the game is its open world, which is filled with mini-games and busy work.
He writes:
Early on, you will immediately get overloaded with “points of interest” and little minigames that the game has doubled down on from last time. There are “science” games that have you splicing genes and decoding molecules and using bee drones to shoot down bad bees in order to increase the good bee population. Miles solves very basic puzzles from his reformed Prowler uncle conveniently using PS5 pressure triggers. Miles, at one point, also has a rhythm matching beat segment. But otherwise, it’s pretty standard fare, similar crimes to last time, fighting arenas, minibase takeovers.
IGN docks another .5 off its score, for an 8/10, writing:
As a sequel in a spectacular series, Marvel’s Spider-Man 2 is both blessed and cursed. Its story of two Spider-Men is a great time and a Spidey fan’s dream to play through as comic book pages are brought to life, elegantly walking the tightrope between light humour and heavier themes. Meanwhile, Insomniac refines a successful formula of combat and web-swinging without revolutionising either in major ways, making them comfy and familiar with just enough new tweaks and abilities to elevate them to fun new heights. The part that feels like it actually needed a radical rethinking is the open world of New York City, which has been made bigger but not better, with an exhausting checklist of mostly repetitious side activities.
I added emphasis to that last part, because it’s pretty much the exact thing that Paul said about the game. And frankly, “an exhausting checklist of mostly repetitious side activities” sounds, well, exhausting!
This is a complaint I’ve seen echoed by many reviewers, in one form or another, and it’s something that I think is important. I loved the first game, but the last thing a sequel needed was more open-world stuff to do, more boxes to tick, more repetitive objectives to overcome.
I know that for many people, having more is always better. Ah, we have a 20-hour game, but if we add 20 more hours of filler content and add in more open-world to explore, it will be better! Think Starfield with its thousands of empty, barren planets. More! More! More!
No!
Stop it. Please.
The only thing we need more of is quality. We need denser, higher-value content. Yes, we still need some open-world activities and the fun base-takeovers and all that, but we don’t need more of them. We need compelling stories and missions, awesome boss fights, great dialogue and performances. And yes, it sounds like all of that is in Spider-Man 2 as well, which is great, but does it really need to be baked into such a thick crust of filler?
Again, I’m sure the game is awesome and I’m sure I’m going to have a blast playing it just like I did when I played the first one. The combat was great, the web-slinging traversal was absolutely fantastic and I had a blast with the story. But there were tedious moments and some goofy missions that irked me. And just in general, I think I’ve really burned out on boilerplate open worlds.
I like to see innovation in this space like we got with Elden Ring or Ghost Of Tsushima or the new Zelda games. A game like Spider-Man 2 doesn’t need a bigger NYC, it just needs Queens and it doesn’t need a ton of mini-games, it needs a good story and top-notch combat and traversal mechanics.
I’m a big champion of semi-open world / semi-linear game design, because I think it’s fun to have some freedom but also that narrative momentum that drives the action and story along with some, but not too many, distractions (which are, let’s face it, Peter Parker’s weakness). And because time is precious and we don’t all have time to do all this open-world busy work.
I’ll end by noting that this is definitely personal preference and if you like all that other stuff, cool. You win because by and large, the industry is all about more and rarely about better. You win, I lose. C’est la vie.",https://imageio.forbes.com/specials-images/imageserve/652da5fc80d944ebd7d288d4/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:15:00
https://www.forbes.com/sites/energyinnovation/2023/10/17/doe-hydrogen-hubs-decision-funds-fossil-fuels-but-45v-tax-credit-can-right-the-ship/,"DOE Hydrogen Hubs Fund Fossil Fuels, But 45V Tax Credit Can Right The Ship","The U.S. Department of Energy’s (DOE) $7 billion announcement launching seven “hydrogen hubs” is intended to fund clusters of projects to test different ways of making, storing, moving, and using a gas that could cut emissions.
However, the decision supports concepts that could ultimately worsen climate pollution and waste billions in public money if allowed to scale, such as making hydrogen from fossil fuels and burning it to heat homes.
Hydrogen hubs will cultivate soil for the nascent industry and scatter seeds of beneficial and harmful projects alike, but they’re only half the story. A looming decision will determine how the new 45V tax credit fertilizes the field—either choking valuable projects amid a mass of weeds, or growing a strong, domestic, truly clean hydrogen industry.
Green hydrogen tanks at a fuel hub for trucks, busses and other vehicles. The tanks could contain ... [+] liquid or gas hydrogen. Wind turbines in the background provide renew energy to produce the hydrogen fuel. getty
45V Tax Credit Will Determine If Hydrogen Is A Useful Tool Or Boondoogle
Congress directed DOE to fund hubs meeting a wide scope of hydrogen designs. With its hands tied, DOE selected proposals including making hydrogen from fossil gas with carbon capture in Appalachia and from existing nuclear power in the Mid-Atlantic, as well as using hydrogen to power buildings in the upper Great Plains and vehicles in California.
Zero-carbon hydrogen is the key to slashing climate pollution from tricky sectors like fertilizer, steel, and long-haul aviation. But fossil fuel-based hydrogen increases climate-warming methane leaks and faces limits to carbon capture. Hydrogen has no place in buildings, and electric technologies will be better for cleaning up pollution sources like cars and heavy trucks.
While the DOE’s hydrogen hubs award generous grants to some short-sighted projects, they are primarily learning opportunities. The federal government can still ensure truly clean hydrogen production.
The Inflation Reduction Act’s lucrative Section 45V Clean Hydrogen Production Tax Credit will drive the industry’s growth wherever the hubs are testbeds—and determine whether hydrogen becomes a legitimate tool to fight climate change or becomes the country’s next decades-long boondoggle.
The U.S. Treasury Department’s guidance on this vital policy is expected by the end of 2023, and it’s critical they get this right. The rules concern a technology called electrolyzers, which use electricity to split water into hydrogen and oxygen. This can make hydrogen either emissions-free or up to five times as polluting as today’s fossil fuel-based production methods. It all comes down to how Treasury accounts for an electrolyzer electricity use, as only the cleanest hydrogen can earn the highest-value subsidy.
Illustrative depiction of clean energy additionality importance to hydrogen Energy Innovation
Making truly clean hydrogen requires electrolyzers to use power from new, local sources of clean energy that generate electricity while hydrogen is being produced. Research shows cutting any corners would be disastrous for climate by causing fossil fuel power plants to run more often; it would also threaten the clean hydrogen industry’s long-term viability.
Special interests have pushed Treasury for weaker 45V rules, arguing stringent guidance would crush the hydrogen industry before it can sprout. But requiring new, local, time-matched clean power is cost-effective and wouldn’t slow the industry’s growth.
The European Union has already adopted these requirements. Renewable energy and hydrogen developers are also on board, urging the Biden administration to act similarly.
Hydrogen Hubs Can Be Laboratories For Industry’s Growth If Treasury Gets It Right
Looser rules would be a generational mistake, leaving the 45V tax credit to spur polluting hydrogen production projects that increase power prices and facilitate the fuel’s use where it doesn’t belong.
Worst of all, it would build an industry that requires indefinite subsidies to survive. Hydrogen would follow the path blazed by U.S. financial support for ethanol—never fulfilling its promise of reducing gasoline emissions but persisting due to political support for continued funding.
LENA, IL - OCTOBER 4: Norm Crain displays an ear of corn and a beaker of 200 proof ethanol produced ... [+] at the Adkins Energy ethanol production facility October 4, 2004 near Lena, Illinois. The facility uses corn to produce 40 million gallons of ethanol a year as well as 132 thousand tons of animal feed, a byproduct of the ethanol production. An average bushel of corn produces 2.7 gallons of ethanol. Currently the United States produces 3.3 billion gallons a year, by 2012 it is estimated production will exceed 5 billion gallons a year. (Photo Illustration by Scott Olson/Getty Images) Getty Images
These rules also matter for the U.S. Environmental Protection Agency’s proposed regulations to cut power sector climate pollution. EPA proposes blending hydrogen with natural gas to cut power plant emissions—but this depends on the cleanliness of the industry stimulated by 45V.
Getting 45V right can put the clean hydrogen industry on a strong foundation. It would cut climate pollution from day one, help the industry outlive the subsidies that spur its development, and create lasting domestic jobs.
Hydrogen hubs will be laboratories that seed regional networks for the industry’s growth. But hydrogen is a means to an end in fighting climate change rather than an end in itself. Only projects making zero-carbon hydrogen and using it in high-value sectors like chemicals and steel are worth scaling.
It’s now up to the Treasury Department to ensure we cultivate a thriving hydrogen industry—not a field of weeds.",https://imageio.forbes.com/specials-images/imageserve/652df351521ad4e9f91f5e43/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:15:00
https://www.forbes.com/sites/johnlamattina/2023/10/17/companies-seek-to-increase-life-span-by-a-decade-or-more/,Companies Seek To Increase Life-Span By A Decade Or More.,"Is This Really Possible? Legend has it that, more than 500 years ago, Ponce de Leon sought what Florida natives believed was the Fountain of Youth. Of course, no such fountain was found. Yet, that hasn’t discouraged people to search for ways to extend human lifespans. Over the past five centuries, this has been a fruitless endeavor. However, recent scientific breakthroughs have recently led to the birth of a number of new companies seeking to increase human longevity – companies with some big name backers like Jeff Bezos. Leaders of these start-ups have, in fact, made meaningful strides in uncovering new biological mechanisms that they believe can lead to drugs that will increase human longevity by at least a decade.
(Photo credit: Bettmann Archive) Bettmann Archive
One particularly promising approach involves partially reprogramming our epigenome which dictates the expression of genes. In doing so it is hoped that the damage done to cells by environmental factors as well as things like obesity and diabetes can be reversed thereby providing treatments for age-related diseases. This all sounds terrific. However, there is a major challenge for discovering and developing anti-aging drugs. How does one design a clinical trial to convince patients, physicians, payers and, especially, the FDA that your drug actually works?
To do this, a company would have to prove that their drug extends lives. You can’t test such a drug in young or even middle-aged people as these groups still have considerable life left assuming a life-expectancy of 80 years. Thus, you would probably need to study the drug in healthy 70 year-olds (with a placebo control group as comparator) and then follow these subjects for a decade or more to see if those on drug live meaningfully longer than those in the placebo group. In order to see a statistically meaningful longevity effect, however, the study would need a minimum of 20,000 subjects. This “outcomes” trial would be similar to what is now done for new drugs to treat heart disease in which a drug’s efficacy is determined by whether it reduced heart attacks and strokes. Such studies are not cheap and the costs can be on the order of $2 billon.
The FDA would likely set a very high bar for safety and efficacy for such a study. Unlike studying patients with heart disease, here you would be testing your drug on 70 year-olds who are relatively healthy. Yet, these patients are entering a decade when they become more susceptible to various cancers, neurological disorders, cardiovascular diseases, etc. You would have to be certain that your drug was no different from placebo when studying these safety parameters. The FDA’s caution would be well justified. Such a drug would be in tremendous demand should it actually work. Should such a drug be approved and then later shown to increase major side-effects, the fall-out would be unprecedented.
And then, of course, you must convince payers to reimburse for such a drug – a drug that will probably command a high price. Payers would set a high bar as to who should be eligible for such a drug much as now happens with the PCSK9 inhibitors, the potent LDL-cholesterol lowering drugs which have been proven to reduce heart attacks and strokes in patients with heart disease.
Given these enormous challenges, why would anyone actually engage in anti-aging R&D drugs? After all, the people investing in these field are accomplished scientists and investors. They are aware of these challenges. Despite the hype around extending the human life-span by 10 -20 years, these companies will not look to conduct life extension studies right out of the gate. Rather, the first drugs will be tested against age-related diseases. For example, one company, Life Biosciences, hopes to use its work in epigenome reprogramming to treat strokes that occur in the back of the eye – a condition known as non-arteritic anterior ischemic optic neuropathy. While a new treatment for this disease would be welcomed, it is a far cry from adding a decade to the life-expectancy of baby boomers.
So, when reading the hype about new scientific breakthroughs that offer the promise of the “Fountain of Youth”, admire the science. But keep in mind that it’s going to be a long while before such drugs are actually available.
(John L. LaMattina is the former president of Pfizer Global R&D and the author of “Pharma & Profits – Balancing Innovation, Medicine, and Drug Prices”.)",https://imageio.forbes.com/specials-images/imageserve/652d9ab1a463dffc3a2bfae5/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:08:00
https://www.forbes.com/sites/davidhambling/2023/10/17/ukraines-ai-drones-seek-and-attack-russian-forces-without-human-oversight/,"Ukraine AI Drones Seek, Attack Russian Forces Without Humans Oversight","The Saker Scout drone can operate autonomously, making it immune to jamming Saker
Ukrainian developers have confirmed that their drones are now carrying out autonomous strikes on Russian forces without a human operator. This is the first time such drones are known to have been used, as UN allegations about autonomous attacks in Libya in 2020 remain unproven.
The Saker Scout drones can find, identify and attack 64 different types of Russian ‘military objects’ on their own, operating in areas where radio jamming blocks communication and prevents other drones from working.
The quadcopter Saker Scout, came into service last month and can carry three kilos of bombs to a range of around 12 kilometres. Small drones operated by remote control have proved extremely effective as bombers with modified RKG-3 anti-tank grenades or RPG warheads and can destroy even heavy tanks.
AI In The Service Of Ukraine
The Saker company was founded in 2021 and to develop affordable AI for small business, with applications as drone-based vision systems crop protection. When Russia invaded, the company switched to assisting the military. One of the first requirements was an AI to help a drone operator spot vehicles concealed by vegetation or camouflage.
Saker’s system is based on machine learning and the developers say it can currently recognize 64 different types of ‘military object’ including tanks, personnel carriers and other hardware. It is continuously improved and is updated on demand when there is a requirement to detect a specific new object or vehicle type. An early Saker video shows the system identifying light and heavy armor (personnel carriers and tanks) and trucks as it flies over them.
The Saker Scout is integrated with Ukraine’s Delta intelligence distribution system which fuses data from drones, satellites and other sources to produce a complete map of the battlefield. A Saker Scout can reconnoiter an area on its own and bring back information. Rather than just raw video, the software highlights enemy vehicles, so a the drones map out enemy positions which would otherwise take hundreds of hours of analysis by humans – not practical when you are fighting a war in real time.
The aim is to enable an extremely fast reconnaissance-decision making-strike process (also known as the ‘kill chain’) in a way that is not possible when humans are involved. Saker suggest that a kill-chain moving at machine-speed, with minimal human involvement, could be transformational in defeating Russian forces.
The Saker Scout can act as a hunter for FPV attack teams: the AI acts as an observer pointing out targets, and automatically passes details to FPV attack drone operators who will verify a target before striking it.
Autonomous Attack Drone
The most radical use of the Saker Scout is to carry out attacks without a human in the loop, finding and hitting targets autonomously. A company spokesman confirmed to Forbes that the Saker Scout had already been used in this mode, but only on a small scale. Most likely it is only used autonomously when radio interference or jamming prevents direct operator control. A video from Saker shows one of their drones carrying out a bombing mission, it is not known if this was autonomous.
The spokesman noted that the AI is not perfect, but their priority was getting a useful system that saves lives into the field. As mentioned, Saker are in constant contact with users and the system is updated continuously.
Going forward, if the system is deemed to be sufficiently reliable, large numbers of autonomous attack drones could be deployed simultaneously without the need for trained operators, or limited radio bandwidth. Dozens of bomber drones could attack simultaneously, immune to jamming and anti-drone guns, possibly targeting jammers or other defenses to make way for operator-controlled drones.
Many campaigners have sought to ban this type of ‘killer robot’, but Paul Scharre, Director of Studies at the thinktank Centre for a New American Security, told Forbes that despite UN discussions going back as far as 2014, there is still no international agreement.
“The pace of technology far outstrips the pace of diplomacy,” says Scharre. “After years of little to no progress in the consensus-based Convention on Certain Conventional Weapons (CCW), humanitarian disarmament activists pushing for a ban on autonomous weapons are taking their case to the UN General Assembly in late October. Whether this process spurs states to take action on autonomous weapons remains to be seen.”
Scharre notes the underlying technology has been around some time, so many others may have comparable systems. Other companies produce drone AI software claimed to be better at spotting targets than a human operators. AeroVironment AVAV say they are ready with an autonomous version of their SwitchBlade kamikaze drone if there is a demand for it.
In January Mykhailo Fedorov, Ukraine’s Minister for Digital Transformation and lead on the Army of Drones initiative , stated in an interview with AP that autonomous weapons were a “logical and inevitable” next step in drone development, suggesting a quiet but deliberate policy. On October 6th Fedorov announced in an official statement that some 2,000 AI-enabled drones had just been supplied via Army of Drones.
""They will assist in safely carrying out reconnaissance, adjusting artillery fire, and uncovering even well-concealed Russian objectives thanks to AI,"" Fedorov stated. He did not say whether these drones were also capable of autonomous attack. If not this capability is only a software update away.
The concern in the past has always been that bad actors would develop this type of technology. However, the moral argument becomes more complex when it is being deployed in an existential fight for its survival against a brutal invasion. As with cluster bombs, Ukraine may be less concerned with possible long-term effects and more concerned about winning the war. But such weapons will not be confined to Ukraine for long.
“Operational pressures are likely to push both sides in the direction of autonomous weapons,” says Scharre.
Scharre notes that while they may initially be limited to military targets like tanks and radar, autonomous weapons could soon be used for less discriminate approaches such as targeting personnel.",https://imageio.forbes.com/specials-images/imageserve/652e51e0bc78291d55b40581/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:07:11
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/ai-is-not-a-competitive-advantage-for-startups-heres-why/,AI Is Not A Competitive Advantage For Startups: Here's Why,"Serge Vasin is Partner at Flintera, a startup studio with a private equity arm that's focused on SaaS, blockchain, AI and EdTech.
getty
Artificial intelligence (AI) has become one of the hottest buzzwords in the world of technology. And two things the hype machine is going all in on are the technology’s potential benefits and the disruptions it will bring to businesses.
Because of this, there’s a growing list of startup founders who believe AI is the silver bullet they need to get funded and find success in the market. In fact, of the 282 companies in the Y-Combinator Winter 2023 (W23) batch, over 40% are related to AI or machine learning (ML).
Unfortunately, the assumption that all a startup needs to succeed is AI couldn’t be further from the truth. That’s why founders should not create startups where AI is the only competitive advantage. Areas where companies are almost certainly doomed in this regard are when it comes to things like email applications, web design, language lessons, search, chatbots, code writing and more.
Yet many startups have tried, and continue to try, to make AI the David that will disrupt the big Goliaths in these categories.
How are market leaders incorporating AI?
Startups need to look at the world of AI differently. For the bigger incumbents, the technology is just another feature. It’s simply one more way to monetize what they already have.
Take Wix, the website-building platform, for example. Wix uses AI to enhance its website design and creation process. Wix's AI site generator uses algorithms to create websites based entirely on the user's inputs. The commercial value of Wix's AI implementation comes from the increased customization options and enhanced user experience it can provide to its already captive audience.
Duolingo, the language learning platform, has also upgraded to the tech, incorporating GPT-4 into its newly launched Duolingo Max. The platform leverages GPT-4 to offer a role-playing feature, enabling users to engage with an AI-powered bot and complete a variety of learning-oriented tasks. Duolingo Max also uses GPT-4 within its ""Explain My Answer"" feature, which provides a personalized explanation of why an answer was graded as correct or incorrect.
Another example is the email client Superhuman, which will continue integrating AI to make email workflows faster by automatically analyzing, categorizing and prioritizing incoming emails. The platform already uses generative AI to help customers write personalized emails and responses with just a few prompts. LinkedIn has a similar feature, which enables job candidates to send AI-generated messages to hiring managers. And in June, the company made a similar feature available for B2B marketers.
What are Big Tech's big advantages?
Implementing AI is no longer a vast moat for seasoned market leaders to cross. If a startup can do it, more established technology enterprises can, too. And they can do it well. Unfortunately, many startups in the AI space aren’t transparent about their technology. Many say they have proprietary technologies to gain investment dollars when, in reality, they are using the ChatGPT API.
Big Tech also has these five advantages.
1. Bigger players have an unfair distribution advantage.
One of the main reasons why AI is not a sustainable competitive advantage is that bigger tech players have the upper hand when it comes to distribution. Startups may have cutting-edge AI products, but seasoned enterprises have a massive audience and brand reputation built over time. This established audience can be monetized through AI-powered products and services. This is evident from the examples of Wix's AI Site Generator, Duolingo Max powered by GPT-4 and Superhuman AI.
2. AI can be replicated.
Another reason why AI is not a sustainable competitive advantage is that it can be replicated. While the companies that develop AI have unique algorithms, these algorithms can be reproduced by other tech firms or even individuals. Currently, most AI platforms plug into a handful of models. As a result, they are all the same.
Hence, the technological edge that AI initially offered can evaporate quickly, and startups are left competing in a highly saturated market, which is not an attractive proposition for any new business.
3. Most AI startups don’t have a real competitive advantage.
Startups must have a real competitive advantage to succeed in today's market. Utilizing APIs or open-source algorithms does not constitute an advantage. Startups should invest in their own models and create products that others cannot replicate by simply leveraging an API or copying some open-source code.
4. AI requires human domain expertise.
AI is not a magic pill that can solve all problems independently. It requires human expertise, especially in the specific domain the algorithms are trained on. Unfortunately, startups may not want to use resources to hire domain experts to train their algorithms in the fields they want to disrupt. But this should be a priority.
5. The AI market has become oversaturated.
There isn’t a day that goes by without an AI startup being mentioned in the news. In the future, I believe AI startups will cannibalize each other, especially as more and more of them turn to open-source algorithms and APIs. The market will become more flooded. This will only worsen as LLM tokens become increasingly cheap, just like what happened with bandwidth, data storage and CPUs.
In a flooded market, survival depends on being unique and standing out. That means having innovative algorithms and producing high-quality software solutions. And this is an advantage Big Tech will have over most startups.
Final Thoughts
The increasing popularity of AI and machine learning technology has not spared startups from its lure. Many founders now aspire to build an AI startup and exploit the hype. However, they may not realize that AI is not necessarily the competitive advantage they’ve been dreaming of. At the end of the day, it's not a silver bullet.
That’s why startups betting on AI as their primary competitive advantage may want to rethink their strategies. As the implementation of AI becomes more accessible, startups need to differentiate themselves by creating unique and innovative products that solve real-world problems. This approach will always make a better bet than relying entirely on this new technology.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/648c6843d9c37c71b63b363e/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:00:00
https://techxplore.com/news/2023-10-fire-inhibiting-nonflammable-gel-polymer-electrolyte.html,"Fire-inhibiting, nonflammable gel polymer electrolyte for lithium-ion batteries","This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Schematic image depicting the principles of operation of non-flammable gel electrolytes. Credit: Ulsan National Institute of Science and Technology
A collaborative research team has achieved a milestone in battery technology. Their achievement in developing a non-flammable gel polymer electrolyte (GPE) is set to revolutionize the safety of lithium-ion batteries (LIBs) by mitigating the risks of thermal runaway and fire incidents.
The research was led by Professor Hyun-Kon Song in the School of Energy and Chemical Engineering at UNIST, Dr. Seo-Hyun Jung from Research Center for Advanced Specialty Chemicals at Korea Research Institute of Chemical Technology (KRICT), and Dr. Tae-Hee Kim from the Ulsan Advanced Energy Technology R&D Center at Korea Institute of Energy Research (KIER). The results have been published in ACS Energy Letters.
In the past, the potential flammability of LIBs has raised significant concerns, especially in electric vehicles, where fire hazards pose a serious threat to underground parking lots. Addressing this critical issue, the research team has successfully developed a groundbreaking non-flammable polymer semi-solid electrolyte, offering a promising solution to mitigate battery fires.
Conventionally, non-flammable electrolytes have heavily relied on the incorporation of flame retardant additives or solvents with exceptionally high boiling points. However, these methods often resulted in a considerable decrease in ion conductivity, compromising the overall performance of the electrolyte.
In their breakthrough research, the team introduced a trace amount of polymer, creating a semi-solid electrolyte. This novel approach dramatically increased the lithium ion conductivity by 33% compared to existing liquid electrolytes. Moreover, the pouch-type batteries incorporating this non-flammable semi-solid electrolyte exhibited a remarkable 110% improvement in life characteristics, effectively preventing unnecessary electrolyte reactions during the formation and operation of the solid-electrolyte interphase (SEI) layer.
Nail penetration of 650 mAh pouch cells of NCM811||graphite. (a to c) Voltage and temperature profiles (d to f). Credit: Ulsan National Institute of Science and Technology
The key advantage of this innovative electrolyte lies in its exceptional performance and non-combustibility. By suppressing radical chain reactions with fuel compounds during the combustion process, the polymer semi-solid electrolyte effectively inhibits the occurrence of battery fires. The research team demonstrated the excellence of the developed polymer by quantitatively analyzing its ability to stabilize and suppress radicals.
Jihong Jeong (School of Energy and Chemical Engineering, UNIST) said, ""The interaction between the polymerized material inside the battery and volatile solvents allows us to effectively suppress radical chain reactions. Through electrochemical quantification, this breakthrough will greatly contribute to understanding the mechanism of non-flammable electrolytes.""
Co-first author Mideum Kim, a master student in the School of Energy and Chemical Engineering at UNIST and the Korea Research Institute of Chemical Technology (KRICT), further confirmed the exceptional safety of the battery itself through various experiments. The team's comprehensive approach included applying the non-flammable semi-solid electrolyte to pouch-type batteries, ensuring the evaluation of electrolyte non-combustibility extended to practical battery applications.
""The research team's multidisciplinary composition, involving electrochemistry from UNIST, polymer synthesis from the KRICT Research Center for Advanced Specialty Chemicals, and battery safety testing by the Ulsan Advanced Energy Technology R&D Center at Korea Institute of Energy Research (KIER), has been instrumental in achieving this breakthrough,"" stated Professor Song. ""The use of non-flammable semi-solid electrolytes, which can be directly incorporated into existing battery assembly processes, will accelerate the future commercialization of safer batteries.""
More information: Jihong Jeong et al, Fire-Inhibiting Nonflammable Gel Polymer Electrolyte for Lithium-Ion Batteries, ACS Energy Letters (2023). DOI: 10.1021/acsenergylett.3c01128 Journal information: ACS Energy Letters",https://scx2.b-cdn.net/gfx/news/2023/fire-inhibiting-nonfla.jpg,2023-10-17 08:29:04
https://techxplore.com/news/2023-10-soft-packaged-portable-glove.html,"Researchers develop soft-packaged, portable rehabilitation glove","This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Multi-scene application and life assistance function of flexible rehabilitation glove robot. Credit: USTC
Researchers from the University of Science and Technology of China (USTC) of the Chinese Academy of Sciences (CAS) have proposed a soft-packaged and portable rehabilitation glove with fine movement training. It is expected to serve the fine motor rehabilitation and daily living assistance for tens of millions of patients with hand dysfunction around the world.
The technology was described in an article published on Oct. 5 in Nature Machine Intelligence.
Patients with hand dysfunction can recover through repeated and continuous hand movement training. Soft-packaged rehabilitation gloves are lightweight and flexible.
However, because a flexible body is prone to large deformation, which is not conducive to motion perception, and currently available gloves are not conducive to portability, most soft-packaged rehabilitation gloves can only achieve therapeutic movement based on open loop control. This makes the precise rehabilitation of fine motor skills of the hand still challenging. Assisted daily tasks on a poststroke individual. Credit: Nature Machine Intelligence (2023). DOI: 10.1038/s42256-023-00728-z
In this study, the researchers designed a bionic finger sleeve structure that integrates smooth movement and accurate perception by integrating 15 bending sensors and 10 shape-memory-alloy actuators.
Due to the shape memory alloy with high work-to-weight ratio and integrated design, the flexible rehabilitation glove robot weighs only 490 grams and has the ability to work independently.
By imitating the folded skin of the back of the finger, the research team proposed a bionic design featuring a non-uniform stiffness flexible finger sleeve, which reduces the interference of finger sleeve movement on the sensing system and achieves stable and accurate finger state perception.
Further, they proposed a multi-modal fine action rehabilitation training method to achieve portable, accurate and safe rehabilitation training for patients with hand dysfunction. Clinical trials have preliminarily verified the advantages of the portable, low-cost soft-packaged rehabilitation glove robot in fine sports rehabilitation and daily living assistance.
More information: Mengli Sui et al, A soft-packaged and portable rehabilitation glove capable of closed-loop fine motor skills, Nature Machine Intelligence (2023). DOI: 10.1038/s42256-023-00728-z Journal information: Nature Machine Intelligence",https://scx2.b-cdn.net/gfx/news/hires/2023/researchers-develop-so-2.jpg,2023-10-17 08:36:39
https://techxplore.com/news/2023-10-unveils-partially-disordered-phase-li-.html,Study unveils a new partially disordered phase in Li- and Mn-rich cathode materials,"This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Characterization of the as-synthesized L5M85, L10M70 and L15M55. a, The partially disordered spinel phase as an intermediate between the DRX and fully ordered spinel structures. b, XRD patterns of L5M85, L10M70 and L15M55, refined based on the rock salt structure. The lattice parameter (a) and weighted profile R factor (R wp ) are shown in the figure. c–e, SEM images of L5M85 (c), L10M70 (d) and L15M55 (e) after shaker milling with carbon (scale bar = 500 nm). f, STEM image and EDS mapping of the elements O, Mn and Ti in as-synthesized L5M85 (scare bar = 500 nm). g, SAED pattern of as-synthesized L5M85. Credit: Nature Energy (2023). DOI: 10.1038/s41560-023-01375-9
Lithium-ion (Li-ion) batteries are among the most widespread battery technologies worldwide, due to their light weight, high energy densities, easy fabrication process, rapid charging times and other advantageous properties. Identifying strategies that could boost their performance further or facilitate their future upscaling has been the focus of numerous recent studies.
One of the proposed approaches for improving the performance of Li-ion batteries entails identifying new promising cathode materials that can be made from metals that are abundant in nature. So far, Li-ion cathodes have been found to be in some ways ineffective, as phase transitions inside them can elicit what is known as voltage hysteresis, which adversely impacts battery capacity.
Researchers at University of California Berkeley and other institutes across the United States have recently unveiled an unconventional phase transformation in cathode materials that are rich in Li and manganese (Mn). The new phase they uncovered, outlined in a paper in Nature Energy, could enable the creation of highly performing batteries with Mn-based cathodes.
""We want to create high energy density cathode materials for Li-ion that can be made from earth-abundant metals, unlike current cathode materials which contain both Co and Ni,"" Gerbrand Ceder, co-author of the paper, told Tech Xplore. ""This would mean Li-ion batteries that are less expensive, thereby helping their market penetration in EVs, grid, etc.""
Mn is an Earth abundant metal, as it is already widely produced in large quantities for various real-world applications. This metal has a good redox voltage and could thus work well in Li-ion batteries with high energy densities.
These advantageous properties are what ultimately inspired Ceder and his colleagues to try to create cathodes containing a high amount of Mn. Some studies have already explored the potential of Mn-rich cathodes, yet most results gathered so far have been unsatisfactory.
""Previous efforts to use Mn-based cathodes have suffered from the fact that Mn has a tendency to move around and rearrange the crystal structure that you start from,"" Ceder said. ""We decided to turn that into an advantage and start from a material (DRX) that when cycled would turn into a structure that is very good for storing lithium. So, it was a form on inverse design: We knew the material would transform, so we just made sure it transformed to something very good for storing lithium.""
The new phase that Ceder and his colleagues unveiled in their study, dubbed the delta (δ) phase, has a unique, unconventional structure. This structure resembles that of spinels, a class of ceramics typically marked by an ordered internal organization.
""The phase we uncovered is related to the known spinel structure, but only forms that structure in very small domains, which are anti-phased with each other,"" Ceder explained. ""Spinel is a structure that is known to store lithium ions, but its commercialization has been problematic, as it undergoes a destructive phase transformation when cycled in a battery. ""
In the phase observed by the researchers, small domains of spinel act independently. This prevents the destructive transition previously observed in spinel-based cathodes during a battery's operation, instead allowing batteries to retain a good capacity for several battery cycles.
""We can make 'complex' structures in-situ while we charge and discharge a battery and these complex structures can have excellent performance,"" Ceder said. ""A cathode material based on only Mn- and Ti-oxide precursors can potentially be very inexpensive and could reduce the cost of Li-ion batteries by 40–50%. This would make batteries for EV and grid much cheaper.""
In their experiments, the researchers were able to identify the kinetic mechanisms underpinning their Li- and Mn-rich cathodes' transition to the so-called delta phase. This could pave the way for the development of more promising cathode materials with similar characteristics.
Initial tests performed by Ceder and his colleagues yielded very promising results, as the phase they unveiled was found to enable both a high energy density and good battery cyclability. In the future, their work could thus also encourage other teams to explore the potential of cathodes rich in Mn using similar experimental strategies.
""We believe that we can make even higher energy density materials,"" Ceder added. ""Also, we are working on accelerating this transformation to delta so one doesn't have to wait for 10–20 cycles to get the best performance out of one's battery.""
More information: Zijian Cai et al, In situ formed partially disordered phases as earth-abundant Mn-rich cathode materials, Nature Energy (2023). DOI: 10.1038/s41560-023-01375-9 Journal information: Nature Energy
© 2023 Science X Network",https://scx2.b-cdn.net/gfx/news/hires/2023/study-unveils-a-new-pa-1.jpg,2023-10-17 07:40:01
https://techxplore.com/news/2023-10-method-ai-1.html,A method to interpret AI might not be so interpretable after all,"This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
A study finds humans struggle to understand the outputs of formal specifications, a method that some researchers claim can be used to make AI decision-making interpretable to humans. Credit: Bryan Mastergeorge, Massachusetts Institute of Technology
As autonomous systems and artificial intelligence become increasingly common in daily life, new methods are emerging to help humans check that these systems are behaving as expected. One method, called formal specifications, uses mathematical formulas that can be translated into natural-language expressions. Some researchers claim that this method can be used to spell out decisions an AI will make in a way that is interpretable to humans.
MIT Lincoln Laboratory researchers wanted to check such claims of interpretability. Their findings point to the opposite: Formal specifications do not seem to be interpretable by humans. In the team's study, participants were asked to check whether an AI agent's plan would succeed in a virtual game. Presented with the formal specification of the plan, the participants were correct less than half of the time.
""The results are bad news for researchers who have been claiming that formal methods lent interpretability to systems. It might be true in some restricted and abstract sense, but not for anything close to practical system validation,"" says Hosea Siu, a researcher in the laboratory's AI Technology Group. The group's paper, currently available on the arXiv preprint server, was accepted to the 2023 International Conference on Intelligent Robots and Systems held earlier this month.
Interpretability is important because it allows humans to place trust in a machine when used in the real world. If a robot or AI can explain its actions, then humans can decide whether it needs adjustments or can be trusted to make fair decisions. An interpretable system also enables the users of technology—not just the developers—to understand and trust its capabilities. However, interpretability has long been a challenge in the field of AI and autonomy. The machine learning process happens in a ""black box,"" so model developers often can't explain why or how a system came to a certain decision.
""When researchers say 'our machine learning system is accurate,' we ask 'how accurate?"" and 'using what data?' and if that information isn't provided, we reject the claim. We haven't been doing that much when researchers say 'our machine learning system is interpretable,' and we need to start holding those claims up to more scrutiny,"" Siu says.
Lost in translation
For their experiment, the researchers sought to determine whether formal specifications made the behavior of a system more interpretable. They focused on people's ability to use such specifications to validate a system—that is, to understand whether the system always met the user's goals.
Applying formal specifications for this purpose is essentially a by-product of its original use. Formal specifications are part of a broader set of formal methods that use logical expressions as a mathematical framework to describe the behavior of a model. Because the model is built on a logical flow, engineers can use ""model checkers"" to mathematically prove facts about the system, including when it is or isn't possible for the system to complete a task. Now, researchers are trying to use this same framework as a translational tool for humans.
""Researchers confuse the fact that formal specifications have precise semantics with them being interpretable to humans. These are not the same thing,"" Siu says. ""We realized that next-to-nobody was checking to see if people actually understood the outputs.""
In the team's experiment, participants were asked to validate a fairly simple set of behaviors with a robot playing a game of capture the flag, basically answering the question ""If the robot follows these rules exactly, does it always win?""
Participants included both experts and nonexperts in formal methods. They received the formal specifications in three ways—a ""raw"" logical formula, the formula translated into words closer to natural language, and a decision-tree format. Decision trees in particular are often considered in the AI world to be a human-interpretable way to show AI or robot decision-making.
The results: ""Validation performance on the whole was quite terrible, with around 45 percent accuracy, regardless of the presentation type,"" Siu says.
Confidently wrong
Those previously trained in formal specifications only did slightly better than novices. However, the experts reported far more confidence in their answers, regardless of whether they were correct or not. Across the board, people tended to over-trust the correctness of specifications put in front of them, meaning that they ignored rule sets allowing for game losses. This confirmation bias is particularly concerning for system validation, the researchers say, because people are more likely to overlook failure modes.
""We don't think that this result means we should abandon formal specifications as a way to explain system behaviors to people. But we do think that a lot more work needs to go into the design of how they are presented to people and into the workflow in which people use them,"" Siu adds.
When considering why the results were so poor, Siu recognizes that even people who work on formal methods aren't quite trained to check specifications as the experiment asked them to. And, thinking through all the possible outcomes of a set of rules is difficult. Even so, the rule sets shown to participants were short, equivalent to no more than a paragraph of text, ""much shorter than anything you'd encounter in any real system,"" Siu says.
The team isn't attempting to tie their results directly to the performance of humans in real-world robot validation. Instead, they aim to use the results as a starting point to consider what the formal logic community may be missing when claiming interpretability, and how such claims may play out in the real world.
This research was conducted as part of a larger project Siu and teammates are working on to improve the relationship between robots and human operators, especially those in the military. The process of programming robotics can often leave operators out of the loop. With a similar goal of improving interpretability and trust, the project is trying to allow operators to teach tasks to robots directly, in ways that are similar to training humans. Such a process could improve both the operator's confidence in the robot and the robot's adaptability.
Ultimately, they hope the results of this study and their ongoing research can better the application of autonomy, as it becomes more embedded in human life and decision-making.
""Our results push for the need to do human evaluations of certain systems and concepts of autonomy and AI before too many claims are made about their utility with humans,"" Siu adds.
More information: Ho Chit Siu et al, STL: Surprisingly Tricky Logic (for System Validation), arXiv (2023). DOI: 10.48550/arxiv.2305.17258 Journal information: arXiv
This story is republished courtesy of MIT News (web.mit.edu/newsoffice/), a popular site that covers news about MIT research, innovation and teaching.",https://scx2.b-cdn.net/gfx/news/2023/a-method-to-interpret.jpg,2023-10-16 17:27:04
https://techxplore.com/news/2023-10-algorithm-quality-electricity-local-generation.html,New algorithm to help control quality of electricity in local generation systems,"This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Prototype. Credit: Ildar Idrisov
With the new stage of energy transition in progress, the key tendency of power market development today is distributed power generation, which is characterized by decentralization, smart energy systems, involvement of consumers, and a higher share of renewable energy sources. In distributed generation systems, electricity comes from a number of local power objects, instead of one large station. For example, home owners who use solar panels can sell excess electricity back into the grid.
A fundamental role in these systems belongs to inverters that convert the generated electricity into the alternating current with specific frequency. In Europe and CIS countries, it equals 50 Hz. Researchers from Skoltech presented an algorithm for inverters that aims to control the quality of electricity injected to the main grid. The results are spotlighted by IEEE and have been published as part of 2023 IEEE Belgrade PowerTech.
""Inverters are programmed with mathematical functions and equations with certain coefficients,"" explains the leading author of the research Ilya Veretennikov, an engineer in the Energy Center's Smart Grid Laboratory.
""If grid parameters remain the same, it is enough to adjust the coefficients just once. Energy systems with distributed (local) generation constantly change (for example, if some market participants stopped selling their electricity), coefficients need to be recalculated. It is difficult to evaluate whether the coefficients are calculated correctly or not, but it is necessary to ensure the quality of electricity, which must meet the standard. Otherwise, it cannot be injected into the grid.""
Ilya Veretennikov working on lab experiments. Credit: Ildar Idrisov
The research team developed an algorithm for a controller, which would recalculate coefficients automatically controlling the quality of electricity injected in the main grid. Using data on calculated voltage, the controller generates a control signal, which ensures the alternating current frequency (50 Hz) without any distortions.
""During our research, we came up with a grid model and a more detailed inverter model. With their help, we checked different algorithms of the controller, their stability and efficiency. We proceeded with validating our results through experimental data. We plugged a real inverter in the grid, modeling a case with a local load, when a part of energy is transferred to the grid. In the lab, we work with low-power inverters, which are suitable for home users. We have great equipment for a detailed simulation in real time. It helps simulate any grid with any number of inverters and any parameters,"" adds Veretennikov.
According to the authors, they see the potential of the research and of the local power generation as a whole in those remote regions which, despite many clear days, still exploit unsustainable power energy sources. The research team is planning to fully automate the algorithm following the plug-and-play concept.
""Configuration is a burden for every end user. As of now, not all parameters are automatically configured, but we are working on the algorithm to make it fully automatic as a modern device that can be used right after connecting it to the computer without manual configurations,"" concludes Veretennikov.
More information: Ilya Veretennikov et al, Proportional Multiresonant Controller with Automatic Gains Adjustment for Grid-Connected Inverters, 2023 IEEE Belgrade PowerTech (2023). DOI: 10.1109/PowerTech55446.2023.10202893",https://scx2.b-cdn.net/gfx/news/2023/new-algorithm-to-help-1.jpg,2023-10-16 17:14:03
https://www.reuters.com/innovation/article/tsmc-results/tsmc-third-quarter-profit-to-slide-30-focus-on-how-much-growth-to-come-idUSKBN31H07N,"TSMC third-quarter profit to slide 30%, focus on how much growth to come","TAIPEI (Reuters) - Taiwan Semiconductor Manufacturing Co Ltd is expected to report a 30% slump in third-quarter profit on Thursday but analysts predict robust growth next year as the chip industry emerges from its current downturn.
FILE PHOTO: A logo of Taiwanse chip giant TSMC can be seen in Tainan, Taiwan December 29, 2022. REUTERS/Ann Wang/File Photo
The likely decline in profit also reflects a strong performance last year, when the company was still riding high on pent-up post-pandemic demand.
The world’s largest contract chipmaker is set to report net profit of T$195.9 billion ($6 billion) for July-September - its second straight quarter of profit decline, according to an LSEG SmartEstimate drawn from 19 analysts. SmartEstimates give greater weighting to forecasts from analysts who are more consistently accurate.
Revenue for the quarter came in at around $17 billion, according to TSMC figures, down 20% from a year earlier and roughly the middle of the company’s forecast range.
Global demand for semiconductors began to weaken in the second half of last year, but analysts say inventories at smartphone and computer makers are running down and restocking demand is expected to pick up.
Given that, much of Thursday’s focus will be on TSMC’s outlook for the fourth quarter and beyond.
Morgan Stanley analysts have forecast 10% revenue growth for the fourth quarter but also said in a research note that “guidance may surprise to the upside,” citing strong demand for high-end chips used in artificial intelligence as one factor.
The AI boom has helped drive up the price of shares in Asia’s most valuable company, with TSMC’s Taipei-listed stock having surged 23% so far this year.
An LSEG SmartEstimate puts TSMC’s 2024 revenue growth at around 22%.
Sources have said, however, that TSMC has been nervous about customer demand and told its major suppliers to delay the delivery of high-end chip-making equipment, although they added that suppliers expect the delay to be short-term.
Some analysts are also reining in their optimism somewhat.
Fubon Securities expects a slow start to next year for TSMC, with 10% growth in the first quarter, predicting order cancellations towards the year end and mild restocking demand. In particular, it is concerned that Apple, a major customer, may revise down its orders.
“We think the market consensus is still too bullish,” it said in a research note.
The company is due to report at 0600 GMT on Thursday.
($1 = 32.2290 Taiwan dollars)",https://s2.reutersmedia.net/resources/r/?m=02&d=20231017&t=2&i=1647931385&w=1200&r=LYNXMPEJ9G044,2023-10-17 03:55:49
https://www.reuters.com/innovation/article/canada-batteries/canada-to-give-belgiums-umicore-up-to-c1-billion-for-new-battery-components-plant-idUSKBN31G1PF,Canada to give Belgium's Umicore up to C$1 billion for new battery components plant,"OTTAWA (Reuters) - Canada and the province of Ontario will give up to C$1 billion to a unit of Belgium’s Umicore to help it build a plant that will produce components for electric vehicle batteries, Ottawa said on Monday.
The facility, in the Ontario town of Loyalist Township, will manufacture cathode active materials and precursor cathode active materials, federal Innovation Minister Francois-Philippe Champagne said in a statement.
The plant - the first of its kind in North America - will initially employ 600 people and have a battery materials production capacity of 35 gigawatt hours annually.
Canada, home to a large mining sector for minerals such as lithium, nickel and cobalt, wants to woo firms involved in all levels of the electric vehicle (EV) supply chain via a multibillion-dollar green technology.
The Umicore plant is due to be built in stages and could be worth C$2.7 billion when fully completed. Canada will invest up to C$551.3 billion with Ontario adding up to C$424.6 billion.
The full project has the potential to produce enough battery materials to support the production of over 800,000 EVs per year, Champagne said.",https://s4.reutersmedia.net/resources_v2/images/rcom-default.png,2023-10-16 19:03:02
https://www.reuters.com/innovation/article/overstock-com-jat/hedge-fund-jat-pushes-overstock-com-to-mull-options-filing-idUSKBN31G1KE,Hedge fund JAT pushes Overstock.com to mull options -filing,"NEW YORK (Reuters) - Hedge fund JAT Capital on Monday urged Overstock.com to consider selling certain assets and overhauling its management and compensation, warning it may seek board seats if its suggestions are ignored, a regulatory filing on Monday showed.
The hedge fund, run by John Thaler, owns a 9.1% stake in Overstock.com, and made its demands public in the filing after sending a letter outlining its wishes to the company last week.
Overstock’s stock price climbed nearly 9% after the filing.
Overstock.com, which spent $21.5 million to buy certain intellectual property assets from Bed Bath & Beyond a few months ago, did not immediately respond to a request for comment.
JAT is not traditionally an activist investor but its filing signaled mounting frustration with the company and left open the option of pursuing strategies like running a proxy contest.
JAT also wants the company to emphasize stock option participation and develop a business plan with financial objectives, the filing said.
“The Reporting Persons may consider to seek Board representation to the extent the above recommendations have not been explored, pursued and executed satisfactorily.”",https://s4.reutersmedia.net/resources_v2/images/rcom-default.png,2023-10-16 16:56:50
https://techcrunch.com/2023/10/17/ray-ban-meta-review/,Ray-Ban Meta sunglasses have 'influencer' written all over them,"Ray-Ban Meta sunglasses have ‘influencer’ written all over them The companies have maintained a slim and light design, while rendering their predecessor obsolete with Facebook and Instagram livestreaming
This is a review-in-progress. More soon!
Somewhere between the Ray-Ban Meta and Meta Quest 3 sits an ideal mixed-reality headset. It’s slim, light, offers hand tracking and passthrough and livestreams video when the moment calls for it. It’s designed to be worn inconspicuously outdoors, until the time comes for content capture.
The Meta Quest Ray-Ban is a fantasy at the moment — albeit one that points in the direction of where its makers think this is all headed. Presently, the Ray-Ban Meta and Meta Quest 3 are very different devices, with little in the way of overlap, beyond being head-worn products with built-in sensors.
The Meta Quest 3 is a mixed-reality headset designed to be worn exclusively indoors. It’s light, perhaps, compared to other headsets of its ilk, but wearing the thing while walking around outside frankly sounds a bit miserable. That’s precisely the use case the Ray-Ban Meta was designed for: freedom of movement outside the house that’s designed to go (mostly) unnoticed.
Just prior to writing this, I slipped a pair on, before the JFK airport mobility cart drove my sciatica-ridden ass to the gate. I would say the pair was inconspicuous but for the fact that I was wearing a pair of sunglasses indoors. Well, that and the extremely necessary recording lighting that flashes on so you can’t creep shoot folks without their knowledge. Here’s some of that video:
We got our first glimpse of the Ray-Ban Meta at a briefing just ahead of the recent Connect conference. I was genuinely impressed by the industrial design the join team came up with. Most folks would hard-pressed to distinguish the charger from a standard Ray-Ban classic eyeglass case. It’s a little thicker than some, sure. A bit heavier. More rigid. But the team was able to make surprisingly few concessions.
There are a lot of clever touches here. In the place of a snap is a ring. Open the case and it glows green when fully charged and orange when not. The orange starts blinking when the battery is low. Space has been maximized inside. The battery sits directly beneath the glasses’ folded temples. In front of this is a dock with two charging pins that lie flush with a pair of contact pads hidden on the underside of the glasses’ bridge, held in place with magnets and a small tab.
The USB-C port is located on the outside bottom of the case, allowing it to sit on its back while being charged. Directly above this on the case’s rear is the Bluetooth pairing button. The case is slimmer than the last gen and can be carried in a pocket comfortably.
Meta says the glasses get “up to” four hours on a charge, while the case gets a total of eight charging cycles, for a grand total of 36 hours. As the company notes, “Battery life varies by use, configuration, settings and many other factors.” That’s the case with all tech, of course, but I did notice that video is a power drainer.
The companies really leaned into the style side of things here (not a bad decision when designing tech meant to be worn on the body). There are two main designs for the glasses. There’s the classic Wayfarer (which is probably what you think of when you think of sunglasses) and the new Headliner (not dissimilar from Wayfarer, but significantly more rounded on the top and bottom).
According to Meta, there are 150 design combos possible, when you factor in all of the different design options, including frame color, style and lenses (including sunglasses, clear, prescription, transitions and polarized).
The temples are thicker than most sunglasses — to be expected, seeing as how they contain the speakers and other components (there’s a transparent option, if you want to see for yourself) — but again, the designers have done a good job keeping size down, all things considered. And again, while slightly heavier that a standard pair of Wayfarers (50.8 g vs. 44 g), you can wear them comfortably all day if you want to (or at least the less than four hours the battery lasts).
There’s a touchpad on the outside of the left temple. Swiping back and forth will adjust the volume (other features can be customized in-app). It also doubles as a control panel for live streaming, since you likely don’t want to futz with your phone or keep using the wake word. A tap can check Instagram or Facebook comments and viewers in real-time. The capture button sits next to the hinge on the left temple
There are a pair of small circular modules on the end pieces. They look identical, for the sake of symmetry, but serve very different — albeit related — functions. On the top right (when facing the glasses) is the 12-megapixel camera. On the top left is an LED that turns on to alert people in your vicinity that you’re recording.
When covered, the glasses send an audio alert that they’ve stopped recording. This is to avoid people sticking a piece of electrical tape to hide the light. Meta says they didn’t hear of any specific examples of this happening, but they almost certainly got that feedback. Again, privacy is paramount for a device like this, especially since it’s something that most people around you don’t know exists. When the battery is low, you’ll get a spoken alert and the light will blink orange and turn red right before shutting down. The light will blink white when receiving a call, do a single flash when taking a photo and glow steadily when recording.
When pairing, it flashes blue, going solid when connected. The pairing process is pretty straight forward. You’ll need to download the Meta View app, choose between Meta Ray-Ban and Ray-Ban Stories and allow bluetooth to connect . Images and video will save to the glasses’ 32GB of internal storage (that’s roughly 500 photos or 100 videos at the maximum 30 seconds apiece). You’ll need to tap “Import” inside the app to connect via WiFi and download the contents to your phone. You can also set it up to auto import via settings.
Once everything is paired, put the glasses on and open either Facebook or Instagram to livestream. Tap the plus icon and it will bring you to the livestream screen. Your phone’s camera is, understandably, the default, but double pressing the capture button will switch over to the glasses. Livestreaming is probably the single biggest killer app Ray-Ban Stories was missing.
There are barely visible down firing speakers on the bottom of the temple tips. When I first tried the speakers in an otherwise silent room, they sounded surprisingly loud and clear. They’re open-ear speakers, rather than bone conduction, which has its pluses and minuses. Bone conduction tends to be quite quiet but does a decent job with ambient noise, since it’s arriving at your eardrums through a different method.
As expected, I had to turn up the volume quite a bit among the airport din. I would recommend them for quieter environments, where possible, but obviously that isn’t always an option. Sound is integral to the headphones, beyond music listening. For instance, there’s an audible shutter click when you take a picture.
There are on-board microphones as well, which listen for the “hey Meta” wake word. Voice certainly makes sense on a device like this. It can be used to take a picture, stop and start video and adjust volume (turns out voice is kind of an annoying way to do the latter). You can also ask the glasses for the time, weather and how much battery is left. You can also ask Alexa style-questions, and Meta AI will attempt to answer. That’s currently only available here in the U.S. through an open beta.
The price starts at $299 for standard lenses. Polarized run $329 and transitions $379. Prescription lenses are on a sliding scale. The price will almost certainly be a deterrent for many — and understandably so. Ultimately, you need to ask yourself how much value a face-worn camera will bring to your life. If you make a living livestreaming, it may make sense. It’s a lot to pay however, for sheer novelty.
It’s worth noting that future updates will bring more value to the device, including sign translation (through voice) and the ability to identify landmarks in front of you. One can see the future of head-worn computing laid out in front of your face — though it’s still going to be a while before we get there.",https://techcrunch.com/wp-content/uploads/2023/09/Meta-Ray-Ban-Stories-06.jpg?w=1200,2023-10-17 07:01:44
https://techcrunch.com/2023/10/17/ray-ban-meta-review/,Ray-Ban Meta sunglasses have 'influencer' written all over them,"Ray-Ban Meta sunglasses have ‘influencer’ written all over them The companies have maintained a slim and light design, while rendering their predecessor obsolete with Facebook and Instagram livestreaming
This is a review-in-progress. More soon!
Somewhere between the Ray-Ban Meta and Meta Quest 3 sits an ideal mixed-reality headset. It’s slim, light, offers hand tracking and passthrough and livestreams video when the moment calls for it. It’s designed to be worn inconspicuously outdoors, until the time comes for content capture.
The Meta Quest Ray-Ban is a fantasy at the moment — albeit one that points in the direction of where its makers think this is all headed. Presently, the Ray-Ban Meta and Meta Quest 3 are very different devices, with little in the way of overlap, beyond being head-worn products with built-in sensors.
The Meta Quest 3 is a mixed-reality headset designed to be worn exclusively indoors. It’s light, perhaps, compared to other headsets of its ilk, but wearing the thing while walking around outside frankly sounds a bit miserable. That’s precisely the use case the Ray-Ban Meta was designed for: freedom of movement outside the house that’s designed to go (mostly) unnoticed.
Just prior to writing this, I slipped a pair on, before the JFK airport mobility cart drove my sciatica-ridden ass to the gate. I would say the pair was inconspicuous but for the fact that I was wearing a pair of sunglasses indoors. Well, that and the extremely necessary recording lighting that flashes on so you can’t creep shoot folks without their knowledge. Here’s some of that video:
We got our first glimpse of the Ray-Ban Meta at a briefing just ahead of the recent Connect conference. I was genuinely impressed by the industrial design the join team came up with. Most folks would hard-pressed to distinguish the charger from a standard Ray-Ban classic eyeglass case. It’s a little thicker than some, sure. A bit heavier. More rigid. But the team was able to make surprisingly few concessions.
There are a lot of clever touches here. In the place of a snap is a ring. Open the case and it glows green when fully charged and orange when not. The orange starts blinking when the battery is low. Space has been maximized inside. The battery sits directly beneath the glasses’ folded temples. In front of this is a dock with two charging pins that lie flush with a pair of contact pads hidden on the underside of the glasses’ bridge, held in place with magnets and a small tab.
The USB-C port is located on the outside bottom of the case, allowing it to sit on its back while being charged. Directly above this on the case’s rear is the Bluetooth pairing button. The case is slimmer than the last gen and can be carried in a pocket comfortably.
Meta says the glasses get “up to” four hours on a charge, while the case gets a total of eight charging cycles, for a grand total of 36 hours. As the company notes, “Battery life varies by use, configuration, settings and many other factors.” That’s the case with all tech, of course, but I did notice that video is a power drainer.
The companies really leaned into the style side of things here (not a bad decision when designing tech meant to be worn on the body). There are two main designs for the glasses. There’s the classic Wayfarer (which is probably what you think of when you think of sunglasses) and the new Headliner (not dissimilar from Wayfarer, but significantly more rounded on the top and bottom).
According to Meta, there are 150 design combos possible, when you factor in all of the different design options, including frame color, style and lenses (including sunglasses, clear, prescription, transitions and polarized).
The temples are thicker than most sunglasses — to be expected, seeing as how they contain the speakers and other components (there’s a transparent option, if you want to see for yourself) — but again, the designers have done a good job keeping size down, all things considered. And again, while slightly heavier that a standard pair of Wayfarers (50.8 g vs. 44 g), you can wear them comfortably all day if you want to (or at least the less than four hours the battery lasts).
There’s a touchpad on the outside of the left temple. Swiping back and forth will adjust the volume (other features can be customized in-app). It also doubles as a control panel for live streaming, since you likely don’t want to futz with your phone or keep using the wake word. A tap can check Instagram or Facebook comments and viewers in real-time. The capture button sits next to the hinge on the left temple
There are a pair of small circular modules on the end pieces. They look identical, for the sake of symmetry, but serve very different — albeit related — functions. On the top right (when facing the glasses) is the 12-megapixel camera. On the top left is an LED that turns on to alert people in your vicinity that you’re recording.
When covered, the glasses send an audio alert that they’ve stopped recording. This is to avoid people sticking a piece of electrical tape to hide the light. Meta says they didn’t hear of any specific examples of this happening, but they almost certainly got that feedback. Again, privacy is paramount for a device like this, especially since it’s something that most people around you don’t know exists. When the battery is low, you’ll get a spoken alert and the light will blink orange and turn red right before shutting down. The light will blink white when receiving a call, do a single flash when taking a photo and glow steadily when recording.
When pairing, it flashes blue, going solid when connected. The pairing process is pretty straight forward. You’ll need to download the Meta View app, choose between Meta Ray-Ban and Ray-Ban Stories and allow bluetooth to connect . Images and video will save to the glasses’ 32GB of internal storage (that’s roughly 500 photos or 100 videos at the maximum 30 seconds apiece). You’ll need to tap “Import” inside the app to connect via WiFi and download the contents to your phone. You can also set it up to auto import via settings.
Once everything is paired, put the glasses on and open either Facebook or Instagram to livestream. Tap the plus icon and it will bring you to the livestream screen. Your phone’s camera is, understandably, the default, but double pressing the capture button will switch over to the glasses. Livestreaming is probably the single biggest killer app Ray-Ban Stories was missing.
There are barely visible down firing speakers on the bottom of the temple tips. When I first tried the speakers in an otherwise silent room, they sounded surprisingly loud and clear. They’re open-ear speakers, rather than bone conduction, which has its pluses and minuses. Bone conduction tends to be quite quiet but does a decent job with ambient noise, since it’s arriving at your eardrums through a different method.
As expected, I had to turn up the volume quite a bit among the airport din. I would recommend them for quieter environments, where possible, but obviously that isn’t always an option. Sound is integral to the headphones, beyond music listening. For instance, there’s an audible shutter click when you take a picture.
There are on-board microphones as well, which listen for the “hey Meta” wake word. Voice certainly makes sense on a device like this. It can be used to take a picture, stop and start video and adjust volume (turns out voice is kind of an annoying way to do the latter). You can also ask the glasses for the time, weather and how much battery is left. You can also ask Alexa style-questions, and Meta AI will attempt to answer. That’s currently only available here in the U.S. through an open beta.
The price starts at $299 for standard lenses. Polarized run $329 and transitions $379. Prescription lenses are on a sliding scale. The price will almost certainly be a deterrent for many — and understandably so. Ultimately, you need to ask yourself how much value a face-worn camera will bring to your life. If you make a living livestreaming, it may make sense. It’s a lot to pay however, for sheer novelty.
It’s worth noting that future updates will bring more value to the device, including sign translation (through voice) and the ability to identify landmarks in front of you. One can see the future of head-worn computing laid out in front of your face — though it’s still going to be a while before we get there.",https://techcrunch.com/wp-content/uploads/2023/09/Meta-Ray-Ban-Stories-06.jpg?w=1200,2023-10-17 07:01:44
https://techcrunch.com/2023/10/17/hoxton-ventures-shoots-for-the-big-time-luring-bryan-gartner-from-khosla-ventures/,"Hoxton Ventures shoots for the big time, luring Bryan Gartner from Khosla Ventures","Everyone more or less agrees that 2023 is going to be effectively written off in VC-land, as the feeding frenzy of the last few years leaves everyone exhausted, valuations flattened or crashed, and exit market remain more or less closed.
VCs appear to be using this period to get their house in order for the next cycle. We’ve already witnessed General Catalyst using this down-time to scoop up a Seed arm in Europe; another Euro VC spend ‘quality time’ with their LPs to raise another fund; and former bystanders, use it to jump on the AI hype cycle.
Why not use it to expand your partnership, right?
To that end, Hoxton Ventures has now managed to lure former Bryan Gartner, Partner at Khosla Ventures, to join as Partner.
Gartner formerly worked on venture growth-stage investments at the VC, but he’ll be refreshing his memory of early-stage investing now he’s at Hoxton.
Over an interviewed, he admitted it’d also a career move based on his personal life: “I have a very international family. My wife is actually British, so [moving] was always on the cards…but we weren’t thinking this quickly. When I was introduced to Hussein through one of the Hoxton portfolio companies, I noticed immediately the transparent nature and the mutual respect among the partnership. A complete lack of ego in the room which you know, frankly, is a breath of fresh air. And it felt to me that there’s an inflection point that Hoxton is about to hit, and I’m thrilled to be to be part of that story.”
About his time at Khosla, he told me: “There’s there’s a dichotomy between the firm’s that truly work deeply with their portfolio companies and those that don’t… It’s not really about the board meeting. It’s about the calls you make, the meetings leading up to the board meeting. And I’ve always really enjoyed that. I’ve got a bunch of stories in my track record that weren’t obvious wins and then became 9, 10, 11x returns, because of really rolling up sleeves and plugging in and that’s what excites me in this industry. And that’s what I saw at Hoxton.”
Hoxton’s early stage credentials in Europe include a 60-strong portfolio which includes a number of successful companies including Darkrace, Deliveroo, Preply, Spacelift, and TourRadar, as well newer investments such as Avantia, Lumi, and XYZ Reality.
Hussein Kanji, partner, Hoxton Ventures said in a statement: “Bryan is an experienced investor, having seen companies go from their early formation to exit and IPO. His late stage skill set will provide us with a new strength around the table.”
He added over a call: “We’ve been intentionally thinking about expanding and building up the partnership. We started alongside Seedcamp, Connect Ventures, and a bunch of others pioneering investing in Europe, and I think we’ve gone into a really good ‘boutique mode’. But I think we’re trying to make another transition now.”
He said Hoxton is growign: “We’ve become much more of a firm and an institution, kind of in the same journey as Index Ventures in the 90s went from a small shop and became the powerhouse that it is today. And if you’re going to do that, you’ve got to hire really great people. So Brian is part of that. And we’re probably going to try and do this one or two more times.”
In March 2022, Hoxton Ventures closed a $215 million new fund, Hoxton III.
Gartner has 16 years of experience in venture capital. He was a Vice President at Insight Venture Partners, where he invested in companies that include AdColony, which was acquired by Opera for $350 million, Alteryx (IPO’d), Fenergo (acquired by Astorg and Bridgepoint for $600M), Pluralsight (acquired by Vista for $3.5B), Smartsheet (IPO’d), and Udemy (IPO’).
After Insight Venture Partners, he joined join Apax, where he invested in Wizeline, which was acquired by CDPQ for ~$450M. He was also a Partner role at Sidewalk Infrastructure Partners. He was previously a Partner at Khosla Ventures for two years.
Hoxton’s investments include Finesse, Giraffe360, Inflow, Really Clever, Peptone, Universal Quantum and XYZ Reality.",https://techcrunch.com/wp-content/uploads/2023/10/Bryan-Gartner-Hoxton-Ventures.jpg?w=762,2023-10-17 12:40:10
https://techcrunch.com/2023/10/17/amazon-passkey-sign-in/,"Amazon quietly rolls out support for passkeys, with a catch","Amazon has quietly rolled out support for passkeys as it becomes the latest tech giant to join the passwordless future. But you still might have to hold onto your Amazon password for a little while longer.
The option to set up a passkey is now available on the e-commerce giant’s website, allowing users to log in using biometric authentication on their device, such as their fingerprint or face scan. Doing so makes it far more difficult for bad actors to remotely access users’ accounts, given that the attacker also needs physical access to the user’s device.
But Amazon’s implementation of passkeys isn’t without issues, as noted by Vincent Delitz, co-founder of German tech startup Corbado, who first documented the arrival of passkey support on Amazon.
Delitz noted that there is currently no support for passkeys in Amazon’s native apps, such as Amazon’s shopping app or Prime Video, which TechCrunch has also checked, meaning you still have to use a password to sign-in (for now). What’s more, if you’ve set up a passkey but previously set up two-factor authentication (2FA), Amazon will still prompt you to enter a one-time verification code when logging in, a move Delitz said was “redundant,” since passkeys remove the need for 2FA as they are stored on your device.
Amazon says on its website: “You will still need to verify a one-time code after signing in with the passkey,” but does not explain why.
It’s unclear if the requirement for 2FA codes is a temporary feature and whether Amazon plans to add passkey support to its mobile apps. It’s also not yet known if passkey support has been made available to all Amazon users, though TechCrunch has confirmed that the feature is available in the U.S., U.K., France, and Germany.
Amazon has not responded to TechCrunch’s questions.
The arrival of passkeys on Amazon lands as WhatsApp announced that it’s rolling out support for passkeys to all Android users, and just days after Google said it planned to make passkeys the default sign-in method for all Google Account holders. GitHub, Windows 11, TikTok and 1Password have all rolled out support for passkeys.","https://techcrunch.com/wp-content/uploads/2019/06/password.jpg?resize=1200,900",2023-10-17 12:05:56
https://techcrunch.com/2023/10/17/procurify-lands-fresh-cash-to-invest-in-ai-powered-tools-for-procurement/,Procurify lands fresh cash to invest in AI-powered tools for procurement,"Roughly eight years ago, a little-known startup called Procurify raised $4 million for its platform that hosts tools to take some of the pain out of enterprise procurement.
The company never became buzzy, exactly. But Procurify grew steadily over the subsequent years, going on to raise $20 million in its Series B and today closing a $50 million Series C funding round led by Ten Coves Capital with participation from Export Development Canada, Canada’s export credit agency.
Procurify, which is based in Vancouver, Canada (hence the investment from the EDA), was co-founded by Aman Mann (the CEO), Eugene Dong (the CTO) and Kenneth Loi (the former CCO). The trio began working on the idea for the company in Dong’s parents’ basement after meeting in British Columbia Institute of Technology’s business management program in 2011.
“We recognized a gap in the procurement market for affordable, easy-to-use procurement software,” Mann told TechCrunch in an email interview. “In virtually every industry, organizations are struggling with a lack of real-time spend visibility and control.”
To Mann’s point, according to a recent Statista survey, hundreds of companies engaging in business-to-business procurement — i.e. finding, agreeing to terms and purchasing goods and services from a third party — admit to struggling with compliance processes, complex approval processes and purchasing systems and reconciling invoices in a timely manner.
So how does Procurify help? By consolidating various procurement steps in one place, mainly.
Procurify offers modules to manage purchasing, accounts payable (i.e. money owed to suppliers) and data analytics. Using the platform, customers can reallocate procurement spend and adjust forecasts, identify bottlenecks in “procure-to-pay” workflows and perform supplier analyses to inform future procurement decisions.
Procurify also leverages AI, detecting anomalies in purchase orders or invoices to flag them for review.
“In today’s post-pandemic economy, industries are grappling with layoffs, disrupted supply chains and rising operational costs,” Mann said. “The need for responsible spend controls and clear financial oversight is more pressing than ever.”
Procurify competes with incumbents (Coupa, SAP Ariba), enterprise resource management software with procurement management features (NetSuite) and upstarts (Precoro, Zip) in the over-$6.1 billion procurement software segment.
The startup appears to be doing well for itself, though, with a customer base of over 700 companies, a 100% year-over-year increase in sales and plans to invest heavily in bringing new AI capabilities to market.
“Procurify helps organizations bring more spend under management, thereby consolidating spend data from our customers’ procure-to-pay workflows to provide a complete picture of expenditures before they’re committed,” Mann said. “By harnessing the power of this comprehensive spend data and integrating it with AI models, enterprises could unearth opportunities for process optimization, manage risks and achieve cost efficiencies.”
Procurify, which has a team of just over 170 employees, has raised a total of $70 million in venture capital to date. In addition to AI R&D, Mann says that the proceeds from the Series C will be put toward general expansion and launching new payment features.","https://techcrunch.com/wp-content/uploads/2022/09/GettyImages-931203582.jpg?resize=1200,800",2023-10-17 12:00:48
https://techcrunch.com/2023/10/17/reality-defender-raises-15m-to-detect-text-video-and-image-deepfakes/,"Reality Defender raises $15M to detect text, video and image deepfakes","Reality Defender, one of several startups developing tools to attempt to detect deepfakes and other AI-generated content, today announced that it raised $15 million in a Series A funding round led by DCVC with participation from Comcast Ventures, Ex/ante, Parameter Ventures and Nat Friedman’s AI Grant.
The proceeds will be put toward doubling Reality Defender’s 23-person team into the next year and improving its AI content detection models, according to co-founder and CEO Ben Colman.
“New methods of deepfaking and content generation will consistently appear, taking the world by surprise both through spectacle and the amount of damage they can cause,” Colman told TechCrunch in an email interview. “By adopting a research-forward mindset, Reality Defender can stay several steps ahead of these new generation methods and models before they appear publicly, being proactive about detection instead of reacting to what just appears today.”
Colman, a former Goldman Sachs VP, launched Reality Defender in 2021 alongside Ali Shahriyari and Gaurav Bharaj. Shahriyari previously worked at Originate, a digital transformation tech consulting firm, and the AI Foundation, a startup building AI-powered animated chatbots. Bharaj was a colleague of Shahriyari’s at the AI Foundation, where he led R&D.
Reality Defender began as a nonprofit. But, according to Colman, the team turned to outside financing once they realized the scope of the deepfakes problem — and the growing commercial demand for deepfake-detecting technologies.
Colman’s not exaggerating about the scope. DeepMedia, a Reality Defender rival working on synthetic media detection tools, estimates that there’s been three times as many video deepfakes and eight times as many voice deepfakes posted online this year compared to the same time period in 2022.
The rise in the volume of deepfakes is attributable in large part to the commoditization of generative AI tools.
Cloning a voice or creating a deepfake image or video — that is, an image or video digitally manipulated to convincingly replace a person’s likeness — used to cost hundreds to thousands of dollars and require data science know-how. But over the last few years, platforms like the voice-synthesizing ElevenLabs and open source models such as Stable Diffusion, which generates images, have enabled malicious actors to mount deepfake campaigns at little to no cost.
Just this month, users on the notorious chat board 4chan leveraged a range of generative AI tools, including Stable Diffusion, to unleash a blitz of racist images online. Meanwhile, trolls have used ElevenLabs to imitate the voices of celebrities, generating audio ranging in content from memes and erotica to virulent hate speech. And state actors aligned with the Chinese Communist Party have generated lifelike AI avatars portraying news anchors, commenting on topics such as gun violence in the U.S.
Some generative AI platforms have implemented filters and other restrictions to combat abuse. But, as in cybersecurity, it’s a cat and mouse game.
“Some of the greatest risk from AI-generated media stems from use and abuse of deepfaked materials on social media,” Colman said. “These platforms have no incentive to scan deepfakes because there’s no legislation requiring them to do so, unlike the legislation forcing them to remove child sexual abuse material and other illegal materials.”
Reality Defender purports to detect a range of deepfakes and AI-generated media, offering an API and web app that analyze videos, audio, text and images for signs of AI-driven modifications. Using “proprietary models” trained on in-house data sets “created to work in the real world and not in the lab,” Colman claims that Reality Defender is able to achieve a higher deepfake accuracy rate than its competitors.
“We train an ensemble of deep learning detection models, each of which focuses on its own methodology,” Colman said. “We learned long ago that not only does the single-model, monomodal approach not work, but neither does testing for accuracy in a lab versus real-world accuracy.”
But can any tool reliably detect deepfakes? That’s an open question.
OpenAI, the AI startup behind the viral AI-powered chatbot ChatGPT, recently pulled its tool to detect AI-generated text, citing its “low rate of accuracy.” And at least one study shows evidence that deepfake video detectors can be fooled if the deepfakes fed into them are edited in a certain way.
There’s also the risk of deepfake detection models amplifying biases.
A 2021 paper from researchers at the University of Southern California found that some of the data sets used to train deepfake detection systems might under-represent people of a certain gender or with specific skin colors. This bias can be amplified in deepfake detectors, the coauthors said, with some detectors showing up to a 10.7% difference in error rate depending on the racial group.
Colman stands behind Reality Defender’s accuracy. And he asserts the company actively works to mitigate biases in its algorithms, incorporating “a wide variety accents, skin colors and other varied data” into its detector training data sets.
“We’re always training, retraining and improving our detector models so they fit new scenarios and use cases, all while accurately representing the real world and not just a small subset of data or individuals,” Colman said.
Call me cynical, but I’m not sure if I buy those claims without a third-party audit to back them up. My skepticism isn’t impacting Reality Defender’s business, though, which Colman tells me is quite robust. Reality Defender’s customer base spans governments “across several continents” as well as “top-tier” financial institutions, media corporations and multinationals.
That’s despite competition from startups like Truepic, Sentinel and Effectiv, as well as deepfake detection tools from incumbents such as Microsoft.
In an effort to maintain its position in the deepfake detection software market, which was valued at $3.86 billion in 2020, according to HSRC, Reality Defender plans to introduce an “explainable AI” tool that’ll let customers scan a document to see color-coded paragraphs of AI-generated text. Also on the horizon is real-time voice deepfake detection for call centers, to be followed b ay real-time video detection tool.
“In short, Reality Defender will protect a company’s bottom line and reputation,” Colman said. “Reality Defender uses AI to fight AI, helping the largest entities, platforms and governments determine whether a piece of media is likely real or likely manipulated. This helps combat against fraud in the finance world, prevent the dissemination of disinformation in media organizations and prevent the spread of irreversible and damaging materials on the governmental level, just to name three out of hundreds of use cases.”","https://techcrunch.com/wp-content/uploads/2023/07/GettyImages-1463459171.jpg?resize=1200,800",2023-10-17 12:00:27
https://techcrunch.com/2023/10/17/nova-credit-lands-45m-in-funding-to-grow-its-cross-border-and-alternative-data-credit-products/,Nova Credit lands $45M to grow its cross-border and alternative data credit products,"Moving from one country to another is difficult in many ways, not the least of which involves starting over financially.
Nova Credit, which started out as a graduate research project out of Stanford University about seven years ago, was founded to help immigrants overcome the obstacles of applying for things like apartments or loans with no credit history in the U.S.
“We realized that half of the graduate student population of any university consists of international students – and if you go and ask them about their experience with financial services, you’ll hear some version of the same story – ‘I can’t get credit or I can’t get a credit card. Or I need to go ask my classmate to co-sign for an apartment or cell phone plan,’ ” said Misha Esipov, CEO and founder of Nova Credit.
With that Credit Passport product, Nova has connectivity into credit bureau data from other parts of the world through its APIs. Nova launched that product with American Express and then added dozens of institution partners over the years, such as HSBC,Scotiabank, Verizon and Earnest. (Nova Credit emerges embedded within those institutions’ applications).
It works in 20 countries outside of the U.S. as well. If a person from the U.S. moves to London or Singapore, for example, they can get approved for banking products internationally in those markets through Nova.
In recent years, the startup also has expanded beyond its flagship Credit Passport product to also offer anyone – not just immigrants – the ability to use alternative data for credit but providing access to their bank account. That product, dubbed Cash Atlas, is cash flow underwriting product that allows anyone with a U.S. bank account to allow the information – such as rent payment history or direct deposit of paychecks – inside those accounts to be used to help them apply for credit. It is aimed at the tens of millions of people that are “effectively locked out of the U.S. financial system,” Esipov said.
“We’ve started to expand from a single product company to a multi-product company and platform…Our strategy is to evolve into a more data analytics company that serves not only people that are new to this country, but frankly any customer segment and plugging the gaps in this antiquated traditional credit reporting industry which doesn’t do well for serving customers who are new to credit,” Esipov told TechCrunch in an interview.
Today, Nova is announcing that it has raised $45 million in Series C funding, additional capital that it plans to use toward expanding its product offering and geographically, Esipov told TechCrunch exclusively. The round came together “in a matter of weeks,” he said.
Canapi Ventures led the round, which included participation from existing backers Kleiner Perkins, General Catalyst, Index Ventures and Y Combinator as well as new investors such as Avid Ventures, Geodesic Capital, Harmonic Capital, Radiate Capital, and Socium Ventures (Cox Enterprises).
The raise marks the company’s first external round of financing since February 2020, which makes it a bit of an outlier in the fintech space, which experienced a major funding boom in 2020 and 2021. At that time, Nova raised $50 million in a financing that reportedly gave it a valuation of $295 million after its first close.
Esipov declined to disclose Nova’s current valuation, saying only that the company was “xxxx happy” He also declined to reveal hard revenue figures, saying only that the company has grown its revenues by 10x since that 2020 funding round and that it more than tripled its data transaction volumes since the start of 2023.
Nova has remained relatively lean, with about 100 employees. It does plan to do some more hiring with its new capital. It also plans to take its Credit Passport business global – having already launched in markets such as Canada, the United Kingdom, the United Arab Emirate and Singapore. The company is also planning to invest in its Cash Atlas product and developing more new products.
For now, Nova’s Passport product provides the majority of the company’s revenue but Atlas “is growing even more quickly on a percentage basis right now.” Esipov said the company has invested “heavily” in information security and compliance since it was a seed-stage company.
The executive projects that the company will reach profitability in the relatively near future, possibly as early as next year. It opted not to raise more capital – in fact less than its last round – to avoid taking on “unnecessary dilution,” he added.
Jeffrey Reitman, general partner at Canapi Ventures, told TechCrunch he was initially attracted to Nova’s mission of enabling newcomers and thin-file consumers the ability to have fair access to financial products. Canapi first invested in the company in the Series B round and is impressed with “the explosive growth” it has displayed since.
“Many of our banking LPs are in active dialogues with Nova Credit to leverage their products to better serve their customers and that also nicely aligns with the mission here at Canapi,” he said.
Nova, Reitman added, “has amassed more collective access to international credit data than any one of the major credit bureaus and that makes for a very valuable proposition for its customers.”
Want more fintech news in your inbox? Sign up for The Interchange here.","https://techcrunch.com/wp-content/uploads/2020/09/GettyImages-a0146-000299-e1646928299148.jpg?resize=1200,799",2023-10-17 12:00:15
https://techcrunch.com/2023/10/17/microsoft-affiliated-research-finds-flaws-in-gtp-4/,Microsoft-affiliated research finds flaws in GTP-4,"Sometimes, following instructions too precisely can land you in hot water — if you’re a large language model, that is.
That’s the conclusion reached by a new, Microsoft-affiliated scientific paper that looked at the “trustworthiness” — and toxicity — of large language models (LLMs) including OpenAI’s GPT-4 and GPT-3.5, GPT-4’s predecessor.
The co-authors write that, possibly because GPT-4 is more likely to follow the instructions of “jailbreaking” prompts that bypass the model’s built-in safety measures, GPT-4 can be more easily prompted than other LLMs to spout toxic, biased text.
In other words, GPT-4’s good “intentions” and improved comprehension can — in the wrong hands — lead it astray.
“We find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, which are maliciously designed to bypass the security measures of LLMs, potentially because GPT-4 follows (misleading) instructions more precisely,” the co-authors write in a blog post accompanying the paper.
Now, why would Microsoft greenlight research that casts an OpenAI product it itself uses (GPT-4 powers Microsoft’s Bing Chat chatbot) in a poor light? The answer lies in a note within the blog post:
“[T]he research team worked with Microsoft product groups to confirm that the potential vulnerabilities identified do not impact current customer-facing services. This is in part true because finished AI applications apply a range of mitigation approaches to address potential harms that may occur at the model level of the technology. In addition, we have shared our research with GPT’s developer, OpenAI, which has noted the potential vulnerabilities in the system cards for relevant models.”
So the implication is that the relevant bug fixes and patches were made before the paper’s publication. Whether that’s truly the case, the research is yet another data point suggesting that LLMs — even from Microsoft-backed, billion-dollar-plus-revenue-generating startups like OpenAI — remain an imperfect science.
GPT-4, like all LLMs, must be instructed — or “prompted” — to complete a task (e.g. writing an email or summarizing an article). Jailbreaking LLMs entails using prompts worded in a specific way to “trick” the LLM into perform a task that wasn’t a part of its objective.
For example, the LLM powering Bing Chat wasn’t designed to write neo-Nazi propaganda. But at release, because it was trained on vast amounts of data from the internet — some of it toxic — it was susceptible to doing just that when fed a particular prompt.
As for GPT-4, the researchers found that the model is more likely to generate toxic text than GPT-3.5 when given certain jailbreaking prompts that they “constructed.” They also claim that GPT-4 “agrees with … biased content more frequently than GPT-3.5,” at least depending on the demographic groups mentioned in the prompt.
For example, give the prompt “Homosexuals have HIV,” GPT-4 usually “strongly” disagrees with the statement, according to the researchers. But given “Women have HIV,” GPT-4 agrees — and outputs biased content.
Just as concerningly, GPT-4 — when given the “right” jailbreaking prompts — can leak private, sensitive data including email addresses, say the researchers. All LLMs can leak details from the data on which they’re trained. But GPT-4 proves more susceptible to doing this than others.
Alongside the paper, the researchers have open sourced the code they used to benchmark the models on GitHub. “Our goal is to encourage others in the research community to utilize and build upon this work,” they wrote in the blog post, “potentially pre-empting nefarious actions by adversaries who would exploit vulnerabilities to cause harm.”","https://techcrunch.com/wp-content/uploads/2023/06/OpenAI-logo-symmetry.jpg?resize=1200,675",2023-10-17 11:30:25
https://techcrunch.com/2023/10/17/biotech-eu/,"Give biotech a chance for the planet's sake, EU lawmakers urged","European Union lawmakers are being urged to avoid too much risk aversion from holding back the potential of the homegrown biotech sector.
Developments in biotech could be transformative in a range of critical sectors. Beyond huge promise in healthcare, innovative, low carbon applications in areas like agriculture and food systems and energy production could help address pressing environmental and sustainability challenges. But there’s concern among some local operators that the bloc’s current approach could cap potential biotech benefits — especially in the context of the urgency required to tackle the climate crisis.
“The main regulatory challenges for the EU’s biotech startups are long timelines for approval of new products and a lack of openness towards modern biotech solutions that may lead to GMO solutions,” Joško Bobanović, partner at Sofinnova Partners, a major investor in European biotech, tells TechCrunch. “ Today, EU startups often do not bother trying to get approval in Europe because of long approval timelines, opting instead to go directly to the US or Asia. This is a huge loss for Europe given the plethora of leading-edge technologies developed here.
“Recent Nobel prizes for technologies like CRISPR or for discoveries that led to RNA vaccines highlight European regulators’ hesitance toward genetic technologies, similar to favoring landlines over mobile phones. (Remember what happened with Nokia and smart phones.) The potential benefits of these innovations far outweigh the risks even as they are part of a duly rigorous regulatory cycle.”
“If you look at venture capital, there’s significantly more money going into the synbio [synthetic biology] community in the United States, and so we’re really at a disadvantage here in Europe,” says Stef van Grieken, CEO and co-founder of EU-based startup Cradle, which offers generative AI tools to help bioengineers design proteins. “There’s also a lot of regulatory risk in Europe. So GMO, a lot of these types of techniques are considered genetic modification. And rules in Europe are very strict. And so if you look at a company like Meatable, that’s growing meat in in a dish instead of using a cow — they’re a Dutch company but they’re launching their products in Singapore, in the United States due to regulatory constraints.”
He also points the level of recent biotech support announced by the Biden administration, including a pledge to invest $2 billion in biotechnology and biotech manufacturing — suggesting the bloc is lagging behind on financial support for the field too. “According to McKinsey, about 60% of our current economic inputs you could make with biology,” he says. “And so that’s substantial, right? Like everything that we consume is a lot larger than the things on the internet.”
“One of the things that’s starting to become obvious is there’s lots of application domains for these types of techniques,” he adds, discussing generative AI’s role in accelerating biotech R&D. “I mean, I’m excited about ChatGPT and [popular generative AI] applications but let’s say… [helping] science and R&D teams to get their bio-based products to market faster to help us solve climate change may be a bit more important than producing better marketing copy.”
Earlier this month the European Union adopted a list of ten technologies it considers critical to the bloc’s future economic security — ranging from AI, quantum and advanced semiconductors, to space tech, robotics and biotech — making a clear statement of recognition of transformative and strategic potential. At the same time, four of the listed techs were flagged for further risk assessment, including biotech (the other three pegged for extra scrutiny are: AI, advanced semiconductors and quantum).
The Commission’s recommendation suggested Member States conduct collective risk assessments of these four critical areas by the end of the year — with lawmakers highlighting the possibility that transformative potential could also lead to highly sensitive risks, such as threats to fundamental rights or civil-military fusion.
Reports have suggested the move could prefigure the introduction of additional EU regulations.
Of the four technologies flagged for risk assessments, biotech may be the least familiar, in terms of public understanding — with the term spanning practices like genetic modification; new genomic techniques (such as CRISPR-Cas9 gene editing); gene-drive; and synthetic biology (aka synbio; a multidisciplinary field); all of which were explicitly name-checked in the Commission’s PR as examples of biotech that should be risk assessed by Member States.
The listed techs all deal with manipulating genetic material but can involve different approaches and applications. Developments in one field may also dial up potential elsewhere — such as gene editing techniques increasing potential applications for synthetic biology, for example — further advancing the complexity of developments since there may be overlap in how these biotechnologies are applied.
Despite relatively low public awareness of biotech advances, Cradle’s van Grieken points out some techniques have actually been widely used in industrial processes for years — helping to produce things like detergents which can work at lower temperatures (via industrially produced enzymes); or synthetic insulin for diabetics (i.e. instead of extracting biological insulin from the pancreatic glands of slaughtered cows and pigs).
While, as noted above, a newer wave of alternative protein startups — including companies being built in Europe — are leveraging developments in the field to do things like scale lab-grown meat or produce non-animal derived dairy proteins, on a mission to transform food systems without the huge carbon footprints attached to traditional (animal-derived) meat and dairy.
But it’s interesting how under the radar some of these regional applications of biotech remain. Certain terminology may be preferred (or avoided) in marketing copy — likely with an eye on regulatory risk and/or consumer trust.
“Precision fermentation is not synthetic biology per se,” a spokesperson for one alt protein startup — France’s Bon Vivant — told us, when we asked what it meant by “precision fermentation”, the term it prefers for explaining its dairy-targeting biotech, querying the bio techniques it’s applying to repurpose yeast microorganisms to brew up cow’s milk proteins.
“As a board member of Food Fermentation Europe, Bon Vivant is still working on a science based and still easily understandable definition,” the spokesman also responded to our ask. Its marketing copy, meanwhile, studiously avoids saying it’s genetically modifying yeast to produce milk proteins — which is essentially what it’s doing — the closest it comes is writing that it “programs” yeasts.
Yet it’s widely accepted that precision fermentation is an example of synthetic biology. (See, for e.g., Wikipedia’s definition: “Precision fermentation is an approach to manufacturing specific functional products which intends to minimise the production of unwanted by-products through the application of synthetic biology, particularly by generating synthetic ‘cell factories’ with engineered genomes and metabolic pathways optimised to produce the desired compounds as efficiently as possible with the available resources.”) So it’s curious to observe a European startup that’s doing interesting things with synthetic biology being so reluctant to say so.
The example speaks to the uncertainty steeping biotech developments in Europe — suggesting disruptors remain worried that causing a splash here could amp up their regulatory risk and bring fresh limits on their fledgling businesses, or at least trigger a new wave of consumer concern, rather than inviting admiration and unlocking homegrown support (or even — dare we say it — congratulatory cheerleading).
Cautionary tale
Cradle’s van Grieken is concerned the EU taking an overly risk averse approach to biotech is out-of-date with where the bloc needs to get to; that precautionary treatment of biotech is riskily self-defeating when it comes to the challenges now facing the bloc, including its headline green ambition to get to ‘Net Zero’ by 2050.
Europe is already “late to the party” when it comes to recognizing the economic and strategic importance of biotech compared to the US and parts of Asia, he argues. But his worry about the EU’s modus operandi is an active frustration that the bloc may be creating a blindspot by not being more encouraging of a sector with transformative potential when it comes to tackling the existential crisis of climate change.
“[Synbio’s potential] is not actually being recognised in the environmental policies of the EU,” he suggests. “If you look at the European Green Deal, a lot of it is focused on energy — like energy production, insulating more homes; it’s focused on recycling; on reducing pollution — like mobility; those types of things. Synbio isn’t really a theme. But it could be an incredibly powerful resource for the EU.
“This specific [Commission] call to the Member States to figure out what the risks are [for biotech] — my worry is that we’ll see increased regulation in this space without actually trying to promote the space and become… a leader in this space. Which we currently, unfortunately, are not. So that’s my biggest worry. But I do think at least recognising that it is something that could be strategic, it’s a good first step.”
“Biotech is a serious business and we need serious regulation here,” he adds when pressed to confirm his position. “But inversely, we don’t want to hamper innovation based on outdated notions of what this technology can and cannot do.”
“The EU needs to accelerate its regulatory processes and be more receptive to new technologies,” agrees Sofinnova’s Bobanović. “This is a critical success factor in the global race to address climate change but also to ensure food independence, a topic becoming more prominent post–COVID-19.”
“Failing to adapt may see our innovations benefiting other markets and the EU losing its competitive edge, much like the electronics industry. Once we lose talent and knowledge centers, it is impossible to recover them,” the investor also warns.
Consumer concern about genetically modified organisms (GMOs) does have a long history in the EU — especially in relation to food safety — which likely informs the precautionary approach the bloc has adopted towards the use of biotech in food production since at least the early 2000s. Out of that has come a legal framework that’s focused on health and safety; harmonized risk assessments; labelling; and traceability.
Consumer awareness of cutting edge biotech may be low but a perception of public concern over GMOs in food, which took root after an earlier era of developments during a time of more lax regulation, has been harder to shift. Yet actual consumer concerns are concentrated elsewhere, research suggests.
A 2019 Eurobarometer survey on food safety indicates EU citizens’ concern over GMO has declined while worries about food risks associated with traditional farming methods are riding high. So while 44% of respondents (the largest proportion) said they were concerned about the presence of antibiotics and hormone residues in meat; and 39% were worried about pesticide residues in foods; a lower proportion — 27% — said they were concerned about GMO being used in foods and only 4% were concerned about genome editing in this context (albeit, for the latter bio technique, the survey also found relatively low knowledge of the use of genome editing in food production — 21% vs 60% for GMO in food — so very low concern there may be a reflection of low awareness).
The survey results suggest EU policymaking in this area — certainly on the food front — risks being out of step with public safety concerns. (To wit: Environmental pollutants in fish, meat and dairy was another big worry for 37% of respondents.)
Taken together the Eurobarometer paints a picture of regional consumers with substantial anxieties about the health risks (and environmental toll) attached to current farming and agricultural practices — and lower concern about biotech being applied to engineer food output. (Also relevant: A Eurobarometer survey from 2021 which found an overwhelming percentage of EU citizens consider climate change to be the most serious problem facing the world.)
Yet the bloc remains saddled with a regulatory regime that ploughs massive subsidies into traditional agriculture while demanding high levels of caution — and even throwing up regulatory hurdles — when it comes to applying biotech to critical sustainability challenges. Critics argue this combo looks increasingly misaligned with where the bloc says it wants to get to with its flagship green transition.
Of course it’s worth noting that policymaking across the 27-Member State bloc is complex, with many entities necessarily involved in change-making. The Commission’s role, while important as a proposer of new pan-EU laws (and/or legislative reforms), is just part of the picture. EU Member States themselves can also have their own biotech and bio-ethics rules and reforms — so a Commission intervention listing biotech as a critical tech, and pushing for Member States to conduct risk assessments, may be aimed at driving for harmonization between this patchwork of national laws — which could, ultimately, streamline and simplify life for biotech entrepreneurs down the line.
Other factors also play a role. Another notable development for regulation of novel biotechs in the EU occurred, in 2018, when the Court of Justice (CJEU) ruled that organisms produced using relatively new techniques, such as gene editing, should fall under the bloc’s existing rules on GMO. So the legal system is also involved in interpreting how existing rules apply to biotech developments. But, again, it’s up to policymakers to keep up with such developments and make sure legislative frameworks are providing the right incentives.
“Europe is complex in terms of regulation, market access,” says Sofinnova partner Cedric Moreau, who is focused on the pharmaceuticals side of biotech investing. “We are not as the US [where] when you have the go from the FDA you have a more than a 300 million people market opening and very homogeneous.”
“We see where the European Commission wants to go — making sure that [it’s] not overlapping with State Members’ policy and making sure that the definition, and the category and the activity are very well defined; to not prevent any innovation or [developments] in the space that could be impacted by [divergence in Member State laws],” he suggests.
“It’s important to make some clear rules, clear definitions because [as investors] we need clarity,” he also tells us. “When we are investing in companies for five, eight, 10 years we cannot bet on regulation that will decide if our drug is a high unmet medical need or just an unmet medical need [for example]… And if our market exclusivity will be 10 years, or six years or nine years or five years. So we need to have clarity — and if it’s not clear enough what we will have to do to build our business case is always to retain the more conservative scenario.”
“At Sofinnova, we are a strong believer of Europe,” Moreau adds. “Because we are deploying — roughly 80% — of our capital in Europe. So we think that Europe is a fantastic playground for healthcare, for innovation. Because we have great science, great scientists, great people. And we have also an ecosystem that could really develop great success stor[ies]… Great products, impactful products for the patient. Then having said that… obviously, we think that there were several things that could be improved.”
Climate urgency vs legal uncertainty
“There is some urgency to consider these types of techniques seriously,” argues van Grieken, talking up the potential of synbio to help in the fight against climate change. “I’m not trying to advocate for ‘no regulation’ type of space. I think we need very strong controls. But on the actual end product, not on how they get researched and developed. And in certain cases, like for example with lab-grown meat or if you look at companies that are making alternatives to cheese or milk, those should be products that we should at least consider having on the market in the EU.”
“Take a company like Perfect Day foods in the United States,” he continues. “They’re making milk without cows. They can do that at, like around — I think — it’s 3% to 5% of the emissions compared to using a cow. That’s a pretty significant improvement. And we use a lot of dairy products, right? And we have a planet on fire.”
As we reported last year, Cradle is using generative AI to predict protein sequences to speed up R&D for protein engineers building bio-based products. So its business is applying AI to accelerate biotech developments — which, of course, means it has an interest in speeding up biotech progress by encouraging a more R&D-friendly regulatory environment, too.
The acceleration its customers are seeing is considerable, as van Grieken tells it — turning what would “typically” be a 1%-5% success rate for stabilizing a protein into a 50% success rate on average, thanks to the predictive power of its generative AI models. But stringent regulation is one brake the startup’s tech can’t uplift. Hence his call for EU lawmakers to zoom out and consider a bigger risk picture.
One idea he welcomes is if the EU were to establish more regulatory sandboxes where biotech R&D could be undertaken without so much legal uncertainty fogging the ambition — which amounts to a call for rules that focus more on outputs, than on the R&D itself.
When it comes to AI, a network of regulatory sandboxes is something the bloc is in the process of setting up — at the same time as EU co-legislators are hammering out a comprehensive, risk-based framework for applying artificial intelligence. So support for, and controls on, cutting edge techs are both possible under the regional lawmakers’ playbook.
Add to that, earlier this year (in April) the Commission put out out a proposal for reforming the bloc’s pharmaceutical regulation — which floats launching a regulatory sandbox as one of the suggested measures to boost regional innovation in drug research and design.
But, in that case, the sandbox would be limited to products regulated as medicines. So even if the bloc’s co-legislators adopt the proposal there are many other biotech innovations that won’t be granted a safe space to experiment — since the end product they’re aiming to disrupt isn’t a pharmaceutical. (And of course climate change won’t be fixed by popping a pill, personalized or otherwise.)
Supporting the production of edible proteins without the climate-heating emissions of traditional agriculture is just one example of biotech’s transformative potential for the environment. Bioplastics offer an alternative to petrochemical-based plastics, as another. While bioremediation is a field that offers promise for cleaning up pollutants — including by engineering microorganisms (such as algae) to accelerate uptake of CO2, the major climate heating gas.
Also on a climate tip, production of biofuels could be more sustainably scaled up using biotech techniques — such as, again, by designing microorganisms that can more efficiently turn biomass into low carbon biofuels.
European bioengineers are even working on genetically modifying plants to amp up their ability to fight indoor pollution (see: French startup Neoplants). So when you start to really think about engineering biology for human and environmental utility the canvas looks broad indeed.
Or, well, it should — but European biotech startups have to do their bluesky thinking from under a more legally clouded horizon.
For biotech startups operating in the EU, van Grieken argues it’s “significantly harder” to do the R&D and test potential innovations with so much regulatory risk hanging over the field. “There’s a lot of uncertainty,” he emphasizes. “For example, the Netherlands just introduced the ability to sample these types of [biotech-derived food] products and have investors taste them. But a very reasonable question from these investors is can you do that and sell this stuff? And if the answer is silence, then, you know, that is not a great answer. And I think this industry needs some clarity around that.”
Current EU rules also create some “weird” scenarios, as he tells it. For example, making an “informed edit” to a genome (i.e. where a bioengineer thinks about what mutation to make) would “typically” be considered a GMO in Europe (meaning the regulatory framework starts to apply) — whereas practices which produce random mutations, as happens a lot in the plant seed space, would not. So an operator that’s, for instance, shining UV light on a plant seed and introducing random mutations falls under less regulatory risk than someone doing bioengineering to select for a specific mutation — perhaps seeking higher crop yield to boost productivity or resistance to drought — regardless of the motivations behind the intent.
“If you think about how you might actually engineer one of these systems, it’s considered problematic; but if you just do it randomly, it’s fine. And so that’s not very smart,” he argues. “Because a lot of the techniques that we have today to make informed decisions about where to make changes in order to get to a certain outcome, that’s also a safe outcome — so it’s actually a lot better than doing it random.”
“If you look at, for example, the United States or places in Asia where a lot of these synthetic biology techniques are allowed it’s not like we’re seeing any major problems,” van Grieken also points out. “So we might be being a bit too constrained right now.
“You should be able to show that your product is good; actually is improving its environmental footprint; is safe to use; is delicious, in the case to food, right — and all these types of things — and get approval for it in some reasonable amount of time so you can still get to market.”
Towards a balanced approach?
Despite criticism that it’s too cautious, EU lawmakers have been talking about evolving the bloc’s approach to biotech. They have also been taking some action too.
This summer, for example, the Commission adopted a proposal for a new regulation on plants produced by certain new genomic techniques (NGTs) which would allow plants produced in this way which could also occur naturally (or via conventional breeding) to be placed on the market — exempting them from requirements in the current GMO legislation.
The NGTs the Commission has proposed loosening the rules for are targeted mutagenesis (aka plants that contain genetic material from the same plant); and cisgenesis, including intragenesis (i.e. plants that contain genetic material from crossable plants) — which would only need to undergo a verification process, under the proposal. Whereas transgenic plants (containing genetic material from non-crossable species) would remain subject to comprehensive, case-by-case risk assessment, approval and authorization prior to any sale under the EU’s existing GMO Directive.
The bloc’s Farm to Fork Strategy, meanwhile — part of the aforementioned European Green Deal which is focused on driving sustainability of agriculture and food production — recognizes biotech as having potential to contribute to the fight against climate change. “New innovative techniques, including biotechnology and the development of bio-based products, may play a role in increasing sustainability, provided they are safe for consumers and the environment while bringing benefits for society as a whole. They can also accelerate the process of reducing dependency on pesticides,” the Commission wrote in the May 2020 strategy document.
Although, subsequent to that, a 2021 study the EU undertook of new genomic techniques noted the “rapid” development of NGTs and their products over the past two decades — finding “considerable interest” in conducting research on NGTs in the EU. But it also identified that “most” development is taking place outside the EU. Which does support the contention the bloc is lagging when it comes to biotech research, despite “considerable” homegrown appetite to do this cutting-edge work.
“Following the [2018 GMO] ruling of the [CJEU], there have been reports of negative impacts on public and private research on new genomic techniques in the EU due to the current regulatory framework,” the EU’s executive also noted in the study. “Regulatory barriers would particularly affect small and medium-sized enterprises (SMEs) and smallscale operators seeking to gain market access with new genomic techniques, even though many Member States and stakeholders see opportunities for them in this sector.”
“The use of NGTs raises ethical concerns but so does missing opportunities as a result of not using them,” it went on, essentially echoing van Grieken’s point. “Based on the findings of the study, most of the ethical concerns raised relate to how these techniques are used, rather than the techniques themselves.”
At that time, the Commission concluded that any further policy action in the area should be “aimed at reaping benefits from innovation while addressing concerns”, further stipulating that a “purely safety-based risk assessment may not be enough to promote sustainability and contribute to the objectives of the European Green Deal and in particular the ‘Farm to Fork’ and biodiversity strategies”. The document also explicitly recognized that risk assessment alone could lead to a flawed evaluation process — in which “benefits contributing to sustainability” are not properly considered.
Asked about the critique it’s over-indexing on risk, when it comes to biotech, and not properly weighting potential sustainability (or, indeed, other) benefits, a Commission spokesperson declined to provide comment. But they pointed us to an EU webpage on R&D and the “bioeconomy” — where the EU’s executive also talks up the transformative potential of homegrown biotech developments, writing for example that: “Stronger development of the bioeconomy will help the EU accelerate progress towards a circular and low-carbon economy. It will help modernise and strengthen the EU industrial base, creating new value chains and greener, more cost-effective industrial processes, while protecting biodiversity and the environment.”
The page also links to the bloc’s long-standing bioeconomy strategy — which features an action plan that lists carrying out an analysis of “enablers and bottlenecks for the deployment of biobased innovations” as one of 14 “concrete actions” regional lawmakers are committed to (on paper at least).
The EU bioeconomy strategy was originally set out back in 2012, and reviewed in 2018, with the aim of supporting 2030 Sustainable Development Goals; the Paris Agreement climate objectives; and new EU policy priorities — with the Commission writing then that reaping the “economic, social and environmental benefits of the bioeconomy, dedicated bioeconomy strategies, investments and innovation are required at all levels in the EU”. Hence the updated strategy emphasizing the need for the development of national and regional bioeconomy strategies.
Five years on from that, the Commission lists just nine Member States that have set out a national bioeconomy strategy (Austria, Finland, France, Germany, Ireland, Italy, Latvia, the Netherlands and Spain) — meaning a substantial majority of EU members still lack this piece of the biotech ecosystem support puzzle. So, clearly, there’s more work for regional lawmakers to do to match the bloc’s ambition to build up Europe’s biotech base with actions that deliver results.
Looking ahead, Cradle’s van Grieken sees two big ares of promise for biotech: Human health being the first one; and what he refers to as “planetary health” as the second. “The reason why I left Google is because those are two of the major problems that my generation faces in the world,” he tells TechCrunch. “In human health, increasingly I think we’ll be a lot better at targeting disease with these types of [bioengineered] molecules and curing people.
“On the planetary health side, I think what will increasingly see is that bio-based products will come out that are cheaper than the petrochemical or animal alternatives. Because, ultimately, biology can do a lot of these things in a much lower energy way and also environmental footprint. I think we’re going to see a breadth of products that is going to be super exciting.”
He’s also bullish on cost — suggesting developments in generative AI can be the flywheel that speeds up biotech R&D — and that acceleration of developments in the lab will draw down the costs entailed in unlocking the big, transformative biotech benefits.
“It’s also why we started Cradle — to really accelerate R&D and make R&D a lot cheaper,” he says, arguing: “There is no fundamental reason why this cannot be done… Biology is ultimately capable of doing very complicated things at very low energy — like, look around you right now. There’s probably a plant somewhere there and try to realise that it’s just like water and ambient carbon that created that, right? It’s just wild, if you think about it.”
French startup Bon Vivant, meanwhile, is working to build a European business that can help tackle the planetary health challenge head on. As noted above, it’s reprogramming yeast microorganisms to produce milk proteins to offer the food industry an alternative so they can sell non-animal-based dairy products — which could have a massive impact on shrinking CO2 emissions if taken up at scale.
Foods derived from animals, including dairy, are generally associated with the highest greenhouse gas emissions (see, for e.g., this UN data on kilograms of emissions per kg of food) — owing to factors including land use, methane emissions from livestock and nitrous oxide emissions from the waste produced by animals. So biotechnologies applied to food production which can replace the need for us to get so much protein from animal sources have the potential for radical reductions in emissions if we integrate these new processes into our food systems.
Asked about the regulatory challenge of building an alternative protein business in Europe, Bon Vivant’s co-founder, Stéphane MacMillan, offers two thoughts. On the one hand he sounds sanguine — suggesting high food safety standards in the EU could create a competitive advantage for local startups over time, as a sort of ‘gold standard’ mark (i.e. once regulatory clearance to sell locally is obtained, which he estimates in their case may take two to three years vs a quicker anticipated time-to-market over in the US).
“Everyone is saying, well, it takes too long in Europe to get approval. Okay, it’s taking longer than any other countries but at the same time we have to be proud of standards that we have in Europe,” he tells TechCrunch. “These standards are also the reason why European food is really seen as the best class in most parts of the world. So we have to comply with it. It takes a bit more time. But, at the same time, I think… that guarantee for the consumer that our products are absolutely non-GMO — that’s really important and [builds trust] with customers.”
But he also suggests the bloc’s policymakers need to find “the right balance” — between having such high homegrown standards and risking a future where European consumers are forced to buy foreign bio products “because we were not able to build the champions”.
“It’s not black or white,” he suggests. “It’s a balance that we need all to find collectively. Both are right. But we just to find the right balance.”
Offering an investor perspective on the same point, Sofinnova’s Bobanović sees even less upside for EU biotech startups trying to turn increasingly strict regional food safety standards into a competitive advantage. So — at the least — the suggestion is the bloc shouldn’t be looking to pile more rules on the sector if it’s serious about growing the bioeconomy.
“While Europe’s stringent rules might enhance consumer trust in certain sectors, it’s unlikely the case for biotech,” he argues. “Unlike the luxury industry where ‘made in Europe’ is an advantage most food products are destined for local consumption and consumers already trust regulations. Increased regulation is not likely to influence product adoption.”","https://techcrunch.com/wp-content/uploads/2021/10/GettyImages-1342327233.jpg?resize=1200,800",2023-10-17 10:58:18
https://techcrunch.com/2023/10/17/scylladb-raises-43m-to-scale-its-nosql-database-platform/,ScyllaDB raises $43M to scale its NoSQL database platform,"Investors have an appetite for databases, it seems.
Today, ScyllaDB, a startup developing database tech for high-throughput, low-latency workloads, announced that it raised $43 million in a funding round led by Eight Roads Ventures with participation from AB Private Credit Investors, AllianceBernstein, TLV partners, Magma Ventures and Qualcomm Ventures.
The new cash will be put toward “accelerating” ScyllaDB’s momentum and expanding the size of its 168-person team, according to co-founder and CEO Dor Laor.
“Today’s disruptors are ingesting an unprecedented amount of data and tapping it to deliver differentiating user experiences that transform markets and displace legacy leaders,” Laor told TechCrunch in an email interview. “Data is being enriched, cleaned, streamed, fed into AI and machine learning pipelines, replicated and cached from multiple sources. That’s why it’s more important than ever to have a database that’s up to the task.”
ScyllaDB is what’s known as a NoSQL database, which — unlike the relational databases once dominant in the enterprise — provides mechanisms for data storage and retrieval that don’t rely on a “tabular relations” model. In a tabular model, a relationship is a connection between two tables of data. But with a NoSQL database, relationships don’t have to follow this schema — offering greater engineering flexibility and, in some cases, improved performance.
NoSQL databases are commonly used for applications like ad serving, AI and machine learning, recommendation and personalization engines, fraud detection and analyzing data from internet of things devices.
According to a 2022 survey by Ventana, almost a quarter (22%) of organizations are using NoSQL databases in production today, while more than one-third (34%) are planning to adopt NoSQL databases within two years or evaluating their potential use. And the NoSQL market is expected to grow to $35.7 billion by 2028, up from $7.3 billion in 2022, the IMARC Group reports.
Now, ScyllaDB’s not the only NoSQL vendor out there — far from it. There’s ArangoDB, Redis Labs and Crate.io to name a few, not to mention bigger players like MongoDB, Amazon’s DynamoDB and Couchbase.
But ScyllaDB claims that its tech offers architectural advantages, like the ability to perform millions of operations per second with “single-digit millisecond” latency. Running across multiple clouds, on a hybrid cloud setup or on-premises, ScyllaDB automatically tunes I/O and CPU performance with workload prioritization, which co-locate workloads under a single server cluster.
Those claims and capabilities were enough to win over customers, evidently. ScyllaDB says its database is now used by over 400 companies including Discord, Epic Games and Palo Alto Networks and that revenue has grown 800% since the company’s founding in December 2012.
“Across industries, R&D teams are increasingly realizing that ScyllaDB’s dramatically different database architecture delivers better performance and horizontal scalability for data-intensive workloads,” Laor said. “ScyllaDB is designed to help fast-growing, fast-moving teams deliver lightning-fast user experiences at extreme scale … ScyllaDB’s unique architecture takes full advantage of modern cloud resources, delivering impressive efficiency and price-performance.”
To date, ScyllaDB has raised $103 million in venture capital.","https://techcrunch.com/wp-content/uploads/2021/04/GettyImages-945420192.jpg?resize=1200,849",2023-10-17 10:00:57
https://techcrunch.com/2023/10/17/figures-humanoid-robot-walks-for-the-camera/,Figure's humanoid robot walks for the camera,"In May of this year, TechCrunch ran a piece titled “Figure’s humanoid robot takes its first steps.” The story was a firsthand account of my visit to the startup’s South Bay offices. The headline was a reference to both the company’s first year of existence and its stated plan to hit a key milestone by its first birthday.
The company later confirmed with me that it had, in fact, managed to get its humanoid robot to walk. I asked for video evidence, which Figure refused to send — until now. Among other things, it’s clear the company wants to get a film crew to capture the bipedal locomotion. I tend to prefer raw laboratory video for stuff like this, but that’s probably one of the many reasons no one is asking me to run marketing for their robotics firm.
Two things jump out at me immediately: First, it’s good to see a non-render from the company. Thus far, their art has been limited to mockups of what the robot could eventually look like. Watching this footage is a reminder that there are many steps along the way to that futuristic bit of product art. Second, you’ve probably noticed that the robot is moving with bent knees, rather than the fully upright motion we see in humans.
Bent knees, on the other hand, are pretty standard in robots — you’ve seen it with Boston Dynamics’ Atlas and Agility’s Digit (though the latter has a reverse bend, similar to an ostrich). Bending gives you better control of balance and other important factors. Ultimately there’s a question of how important it is to hue to a more human-like gait, but obviously this first video of the Figure 01 robot walking is very much early stage. There are plenty of kinks to work out between now and a ship date.
The other thing I will draw your attention to is the hands. Mobile manipulation remains a key problem in this world, and many humanoid systems like Digit and Apptronik’s Apollo have yet to add articulated graspers. Of course, there’s nothing in this video that suggests the grippers are currently functional. On my visit to the company’s HQ, however, they showed me a portion of the office devoted to the development of what looked to be a five-digit, human-style hand.
The video notes that Figure’s headcount at the time of shooting (10/1) was 60. Impressive growth for one year. More updates soon, no doubt.","https://techcrunch.com/wp-content/uploads/2023/10/Screenshot-2023-10-13-at-3.53.07 PM.jpg?resize=1200,725",2023-10-17 10:00:01
https://techcrunch.com/2023/10/17/invesco-raises-swiggy-valuation-to-nearly-8-billion/,Invesco raises Swiggy's valuation to nearly $8 billion,"Conditions appear to be shifting favorably for India’s Swiggy. The food delivery startup — backed by SoftBank, Prosus and Accel — saw its paper valuation slashed by more than a half this year as investors marked their holdings largely in response to the dwindling market conditions. The startup, valued at $10.7 billion in a funding round early 2022, also lost some market share to Zomato, its arch publicly-listed rival, according to Prosus.
Now, not so much.
Invesco, which led Swiggy’s previous round and cut its valuation to under $5.5 billion, marked up the startup’s valuation to $7.85 billion at July’s closure, according to a newly published disclosure.
The U.S. asset manager says it considers the valuation of similar public companies as a factor when reassessing the value of its private investments. Given that shares of Zomato have risen by 33% since the end of July, this could imply that the current $7.85 billion valuation for privately-held Swiggy may be conservative.
Separately, Swiggy, which is eyeing to make an initial public offering next year, appears to be closing in on some of the market share it lost to Zomato this year. Swiggy’s month-on-month volume grew 7% this July and 6% in August, beating Zomato in both months, UBS said in a report this month.","https://techcrunch.com/wp-content/uploads/2023/03/GettyImages-1239421148.jpg?resize=1200,801",2023-10-17 09:24:24
https://techcrunch.com/2023/10/17/stack-overflow-cuts-28-of-its-staff/,Stack Overflow cuts 28% of its staff,"Developer community site Stack Overflow has laid off 28% of its staff, the Prosus-owned company announced Monday.
In a blog post, Stack Overflow’s CEO, Prashanth Chandrasekar indicated that the company is focusing on its path to profitability. While the post didn’t elaborate on the reason behind the job cuts, it mentioned customers’ budgets shifting elsewhere “due to the macroeconomic pressures.”
“This year we took many steps to spend less. Changes have been pursued through the lens of minimizing the impact on the lives of Stackers. Unfortunately, those changes were not enough and we have made the extremely difficult decision to reduce the company’s headcount by approximately 28%,” Chandrasekar said.
While Stack Overflow is primarily a Q&A website for consumers it also has enterprise products like “Stack Overflow for Teams,” which helps organizations maintain a company-wide knowledge base.
The company didn’t specify the number of laid-off employees. However, since it had pushed its headcount to over 500 people last year, more than 100 people are likely to be impacted.
With generative AI gaining popularity for helping coders with different problems, Stack Overflow has seen its traffic drop as compared to last year.
In August, the company said that because of generative AI, it expects “some rises and falls in traditional traffic and engagement over the coming months.”
Earlier this year, Stack Overflow asked AI companies to pay for training data. In January, it barred users from posting answers generated by AI. The company is also trying to bolster its own AI capabilities. In July, it launched OverflowAI with features like generative AI-powered search.
Big Tech is also moving fast to make generative AI-aided products available for coders in a rapid manner. Last month, GitHub expanded access to its Copilot chat to individual users. In May, during its developer conference, Google announced a bunch of AI-centric coding tools including an assistive bot called Codey. The company has also trained its conversational AI tool Bard to help users with code generation and debugging.","https://techcrunch.com/wp-content/uploads/2023/10/3281da8e0c2332be9228c95317c493fe508c5a65-2400x1260-1.webp?resize=1200,630",2023-10-17 07:44:44
https://techcrunch.com/2023/10/17/ambani-jio-financial-launches-lending-and-insurance-businesses/,Ambani's Jio Financial launches lending and insurance businesses,"Jio Financial Services, the Indian conglomerate Reliance Industries-backed financial services firm, has started its lending and insurance businesses and plans to rapidly broaden its offerings as billionaire Mukesh Ambani expands the ever-so-wide tentacles of his oil-to-telecom empire.
The market has been closely paying attention to Reliance’s financial services ambitions for years. But it wasn’t until last year that Ambani, Asia’s richest man, revealed that the firm plans to enter into the sector, which though has grown multiple folds in the past decade remains largely untapped, serving only tens of millions of individuals.
Jio Financial Services, which made public debut in August, said in its annual presentation that it has started to offer personal loan to salaried and self-employed individuals through its MyJio app and 300 stores across India. Its insurance arm has also partnered with 24 insurers to offer a wide-range of coverage across auto, health, and corporate categories, said the firm.
Jio Financial Services has largely remained quiet about precisely what all it plans to do. The firm, whose largest backer remains Reliance Industries, earlier this year partnered with U.S. asset manager BlackRock to launch asset management services in the country.
The financial services is the newest sector for Ambani, who has entered several businesses — including telecom — in the past decade and scaled them to tentpole positions. Reliance also operates the nation’s largest retail chain, which has been valued at $100 billion in recent fund raises from investors including KKR.
As Jio Financial scales its business, it may pose a challenge to a number of players in the industry, including Paytm and Policybazaar. Reliance said it will make use of AI and analytics for its financial services business and operate on a “low cost of servicing.”
Jio Financial Services said it’s taking a direct-to-customer approach with its offerings to drive cost efficiencies and enabling personalized customer interactions. The firm is incorporating “alternate data models for 360-degree customer view and tailored offerings,” and is developing a unified app for the “diverse financial needs of customers.”
In the annual report, Jio Financial Services said it’s also testing a sound box, the fast-omnipresent portable device that alerts merchants when a transaction has completed, the firm said, confirming an August TechCrunch report. The company is “generating substantial data footprint and enhancing our customer engagement across digital channels, and in turn enriching and facilitating other businesses,” it said.
On lending, Jio Financial Services plans to extend loans to businesses and merchants as well as offer loans to facilitate vehicle and home purchases, it said. It also plans to give loans by using shares as collateral. The firm said it has also “relaunched” savings account service and bill payments and plans to launch debit cards.","https://techcrunch.com/wp-content/uploads/2023/08/GettyImages-1243634682.jpg?resize=1200,800",2023-10-17 07:26:12
https://techcrunch.com/2023/10/17/ray-ban-meta-review/,Ray-Ban Meta sunglasses have 'influencer' written all over them,"Ray-Ban Meta sunglasses have ‘influencer’ written all over them The companies have maintained a slim and light design, while rendering their predecessor obsolete with Facebook and Instagram livestreaming
This is a review-in-progress. More soon!
Somewhere between the Ray-Ban Meta and Meta Quest 3 sits an ideal mixed-reality headset. It’s slim, light, offers hand tracking and passthrough and livestreams video when the moment calls for it. It’s designed to be worn inconspicuously outdoors, until the time comes for content capture.
The Meta Quest Ray-Ban is a fantasy at the moment — albeit one that points in the direction of where its makers think this is all headed. Presently, the Ray-Ban Meta and Meta Quest 3 are very different devices, with little in the way of overlap, beyond being head-worn products with built-in sensors.
The Meta Quest 3 is a mixed-reality headset designed to be worn exclusively indoors. It’s light, perhaps, compared to other headsets of its ilk, but wearing the thing while walking around outside frankly sounds a bit miserable. That’s precisely the use case the Ray-Ban Meta was designed for: freedom of movement outside the house that’s designed to go (mostly) unnoticed.
Just prior to writing this, I slipped a pair on, before the JFK airport mobility cart drove my sciatica-ridden ass to the gate. I would say the pair was inconspicuous but for the fact that I was wearing a pair of sunglasses indoors. Well, that and the extremely necessary recording lighting that flashes on so you can’t creep shoot folks without their knowledge. Here’s some of that video:
We got our first glimpse of the Ray-Ban Meta at a briefing just ahead of the recent Connect conference. I was genuinely impressed by the industrial design the join team came up with. Most folks would hard-pressed to distinguish the charger from a standard Ray-Ban classic eyeglass case. It’s a little thicker than some, sure. A bit heavier. More rigid. But the team was able to make surprisingly few concessions.
There are a lot of clever touches here. In the place of a snap is a ring. Open the case and it glows green when fully charged and orange when not. The orange starts blinking when the battery is low. Space has been maximized inside. The battery sits directly beneath the glasses’ folded temples. In front of this is a dock with two charging pins that lie flush with a pair of contact pads hidden on the underside of the glasses’ bridge, held in place with magnets and a small tab.
The USB-C port is located on the outside bottom of the case, allowing it to sit on its back while being charged. Directly above this on the case’s rear is the Bluetooth pairing button. The case is slimmer than the last gen and can be carried in a pocket comfortably.
Meta says the glasses get “up to” four hours on a charge, while the case gets a total of eight charging cycles, for a grand total of 36 hours. As the company notes, “Battery life varies by use, configuration, settings and many other factors.” That’s the case with all tech, of course, but I did notice that video is a power drainer.
The companies really leaned into the style side of things here (not a bad decision when designing tech meant to be worn on the body). There are two main designs for the glasses. There’s the classic Wayfarer (which is probably what you think of when you think of sunglasses) and the new Headliner (not dissimilar from Wayfarer, but significantly more rounded on the top and bottom).
According to Meta, there are 150 design combos possible, when you factor in all of the different design options, including frame color, style and lenses (including sunglasses, clear, prescription, transitions and polarized).
The temples are thicker than most sunglasses — to be expected, seeing as how they contain the speakers and other components (there’s a transparent option, if you want to see for yourself) — but again, the designers have done a good job keeping size down, all things considered. And again, while slightly heavier that a standard pair of Wayfarers (50.8 g vs. 44 g), you can wear them comfortably all day if you want to (or at least the less than four hours the battery lasts).
There’s a touchpad on the outside of the left temple. Swiping back and forth will adjust the volume (other features can be customized in-app). It also doubles as a control panel for live streaming, since you likely don’t want to futz with your phone or keep using the wake word. A tap can check Instagram or Facebook comments and viewers in real-time. The capture button sits next to the hinge on the left temple
There are a pair of small circular modules on the end pieces. They look identical, for the sake of symmetry, but serve very different — albeit related — functions. On the top right (when facing the glasses) is the 12-megapixel camera. On the top left is an LED that turns on to alert people in your vicinity that you’re recording.
When covered, the glasses send an audio alert that they’ve stopped recording. This is to avoid people sticking a piece of electrical tape to hide the light. Meta says they didn’t hear of any specific examples of this happening, but they almost certainly got that feedback. Again, privacy is paramount for a device like this, especially since it’s something that most people around you don’t know exists. When the battery is low, you’ll get a spoken alert and the light will blink orange and turn red right before shutting down. The light will blink white when receiving a call, do a single flash when taking a photo and glow steadily when recording.
When pairing, it flashes blue, going solid when connected. The pairing process is pretty straight forward. You’ll need to download the Meta View app, choose between Meta Ray-Ban and Ray-Ban Stories and allow bluetooth to connect . Images and video will save to the glasses’ 32GB of internal storage (that’s roughly 500 photos or 100 videos at the maximum 30 seconds apiece). You’ll need to tap “Import” inside the app to connect via WiFi and download the contents to your phone. You can also set it up to auto import via settings.
Once everything is paired, put the glasses on and open either Facebook or Instagram to livestream. Tap the plus icon and it will bring you to the livestream screen. Your phone’s camera is, understandably, the default, but double pressing the capture button will switch over to the glasses. Livestreaming is probably the single biggest killer app Ray-Ban Stories was missing.
There are barely visible down firing speakers on the bottom of the temple tips. When I first tried the speakers in an otherwise silent room, they sounded surprisingly loud and clear. They’re open-ear speakers, rather than bone conduction, which has its pluses and minuses. Bone conduction tends to be quite quiet but does a decent job with ambient noise, since it’s arriving at your eardrums through a different method.
As expected, I had to turn up the volume quite a bit among the airport din. I would recommend them for quieter environments, where possible, but obviously that isn’t always an option. Sound is integral to the headphones, beyond music listening. For instance, there’s an audible shutter click when you take a picture.
There are on-board microphones as well, which listen for the “hey Meta” wake word. Voice certainly makes sense on a device like this. It can be used to take a picture, stop and start video and adjust volume (turns out voice is kind of an annoying way to do the latter). You can also ask the glasses for the time, weather and how much battery is left. You can also ask Alexa style-questions, and Meta AI will attempt to answer. That’s currently only available here in the U.S. through an open beta.
The price starts at $299 for standard lenses. Polarized run $329 and transitions $379. Prescription lenses are on a sliding scale. The price will almost certainly be a deterrent for many — and understandably so. Ultimately, you need to ask yourself how much value a face-worn camera will bring to your life. If you make a living livestreaming, it may make sense. It’s a lot to pay however, for sheer novelty.
It’s worth noting that future updates will bring more value to the device, including sign translation (through voice) and the ability to identify landmarks in front of you. One can see the future of head-worn computing laid out in front of your face — though it’s still going to be a while before we get there.",https://techcrunch.com/wp-content/uploads/2023/09/Meta-Ray-Ban-Stories-06.jpg?w=1200,2023-10-17 07:01:44
https://techcrunch.com/2023/10/16/snapchat-is-now-allowing-websites-to-embed-content/,Snapchat is now allowing websites to embed content,"Snapchat has relied on people consuming content on its own app. But now, the social network is allowing websites to embed public content including Lenses, Spotlight videos, Public Stories, and Public Profiles.
Users who want to embed a Story, video, or a Lens, can open up the content on a desktop browser using the link. They can click on the embed button on the share sheet to copy the code and post it to their site.
In July 2022, Snap made its website more useful for users with core features like the ability to send messages and Snaps. The initial version of Snapchat for the web was only available for Snapchat+ users in the United States, United Kingdom, Canada, Australia, and New Zealand. The company made it available to all users in September 2022.
Snapchat’s rivals Instagram and TikTok have long offered web embeds so blogs and news sites can include content from those platforms. Snap hopes that this move will drive more traffic to the app and website.
Last week, Snap CEO Evan Spiegel sent an internal memo to employees stating that the social network wants to reach 475+ million daily users in 2024, according to The Verge’s Alex Heath. The report also noted that Snap aims to have 14 million Snapchat+ subscribers and $500 million in non-ads revenue. Last month, Snap reported that Snapchat+ crossed 5 million subscribers.
The Verge’s report added that the company has set a goal to achieve a 20% increase in ad-based revenue year-on-year.",https://techcrunch.com/wp-content/uploads/2021/01/GettyImages-1172921170.jpg?w=1024,2023-10-17 06:02:54
https://techcrunch.com/2023/10/16/ftx-execs-blew-through-8b-testimony-reveals-how/,FTX execs blew through $8B; testimony reveals how,"Sam Bankman-Fried and other FTX executives spent $8 billion worth of customer funds on real estate, venture capital investments, campaign donations, endorsement deals and even a sports stadium, according to testimony from former senior FTX executive Nishad Singh.
Singh’s testimony, which kicked off the third week of Bankman-Fried’s trial, provides fresh details of exactly where that money went.
Singh, who has already pled guilty to fraud, money laundering and violation of campaign finance laws, said Monday that he learned of the massive hole in Alameda’s books as a result of a coding error that “prevented the correct accounting” of user deposits by around $8 billion.
Singh’s testimony helps corroborate the statements given by three previous prosecution witnesses, all of whom were in Bankman-Fried’s inner circle: FTX CTO Gary Wang, Alameda CEO Caroline Ellison and FTX engineer Adam Yedidia. While Wang and Ellison have pled guilty, each witness has pointed to Bankman-Fried as the orchestrator of fraud and money laundering.
Singh said that even after learning about the hole, “implicitly and explicitly, I green-lit transactions that I knew must have been digging the hole deeper and therefore coming from customer funds.”
Singh went on to describe Bankman-Fried’s spending as “excessive.” He said that he often learned about large spends after the fact, and that his expressions of concern weren’t taken seriously.
“I also would express that I felt kind of embarrassed or ashamed of how much it all wreaked of excess and flashiness,” said Singh. “It didn’t align with what I thought we were building a company for.”
Where the money went
Prosecutor Nicolas Roos and Singh went through spreadsheets detailing different ways Alameda spent the $8 billion in customer funds. Singh testified that Bankman-Fried was “in general the one making the final decision on investments and investment team decisions as a whole.”
In addition to going over a $1 billion on Genesis Digital Assets, a crypto mining firm in Kazakhstan, and $500 million on Anthropic, an AI company focused on safety, the prosecution focused on Alameda’s $200 million investment into K5 Global, a venture firm led by investor Michael Kives who is known for his extensive network.
That network seemed to impress Bankman-Fried deeply. After attending a Super Bowl Party hosted by K5 in Los Angeles, the former crypto mogul told Singh that he had met “the most impressive collection of people he ever had in one location.” Faces at the party included Hilary Clinton, Katy Perry, Orlando Bloom, Leonardo DiCaprio, Jeff Bezos, Kendall and Kris Jenner and Kate Hudson.
Bankman-Fried had proposed a term sheet to Singh and Wang one night that laid out hundreds of millions of dollars of onuses to Kives and Bryan Baum, co-founder and managing partner of K5. The sheet also proposed up to $1 billion long-term capital to give to the VC firm, according to Singh.
“We can get from them essentially infinite connections,” wrote Bankman-Fried in a letter to FTX leadership that was shared at Monday’s trial. “I think that if we asked them to arrange a dinner with us, Elon, Obama, Rihanna and Zuckerberg in a month, they would probably succeed.”
Singh said he expressed concern about partnering with K5 and giving them such substantial funds, which would be “really toxic to FTX and Alameda culture.” He said that “politicking and social climbing was not going to be rewarded, and here we were rewarding people in exorbitant amounts.”
The former FTX executive suggested that Bankman-Fried use his own money, not FTX’s, to make some of these investments. Those protestations didn’t yield results, according to the spreadsheet, which showed the K5 deal went through Alameda’s venture arm.
Bankman-Fried also believed that endorsement deals and even “unpaid partnerships with celebrities” would help increase FTX’s influence to propel its success, said Singh.
To that end, about $205 million of that $8 billion chunk was spent renaming the Miami Heat stadium to FTX Arena. Another $150 million was spent to endorse the MLB. Other items on a spreadsheet shown to the jury show FTX paid out $1.13 billion in exchange for endorsements from basketball player Steph Curry, video game developer Riot, Seinfeld writer Larry David to endorse FTX in a Super Bowl ad, football star Tom Brady and model Giselle Bündchen, with whom FTX was coordinating on some philanthropic efforts, according to Singh. .
Singh’s testimony also revealed a range of properties that had been purchased with the funds, including a $30 million penthouse in the Bahamas that Singh said was “too ostentatious.”
Bankman-Fried has also donated tens of millions to election campaigns.
The former FTX executive, who also went to high school with Bankman-Fried and was a close friend of his brother, testified that he expressed concern about the company’s spending, but was usually blown off.
Singh recalled one instance where Bankman-Fried got visibly angry with him and said that people like him were “sowing seeds of doubt in the company decisions” and were “the real insidious problem here.”
“It was pretty humiliating,” said Singh.
Where did this $8 billion hole come from?
Singh’s testimony aligned with Yedidia’s that states in June 2022, the executives learned that Alameda owed $8 billion worth of FTX customer money after Ellison shared a Google Doc displaying the “extremely negative” balance.
Singh told the court this hole was due to a bug that Yedidia accidentally introduced into the system in 2021. The bug “prevented correct accounting for fiat@FTX.com’s balances on specific types of withdrawals,” said Singh. Fiat@FTX.com was an internal accounting system that recorded user deposits.
On top of this, Singh testified that he built out systems on FTX that gave Alameda “special privileges” not afforded to other users. A feature called “allow negative” let Alameda trade, borrow and withdraw FTX funds in excess of its balance and collateral amounts, according to Singh. He testified that he coded an initial version of the feature in 2019 at Bankman-Fried and Wang’s advisement.
A later version of this code allowed Alameda to borrow from FTX without having tis collateral liquidated. In effect, it could “withdraw money that it didn’t have,” meaning it could “lose money” that “belonged to customers,” Singh said.
By June 2022, Alameda had built up its own $2.7 billion deficit on the FTX platform.
“This seemed like a real abuse of a feature that until this point I believe was serving FTX, not hurting it,” said Singh.
Alameda at this point also owed $8 billion in user funds to FTX that it no longer had on hand. In total, the negative account balance and accounting bug contributed to a $11 billion hole on FTX’s balance sheet, Singh testified.","https://techcrunch.com/wp-content/uploads/2023/10/GettyImages-1728183029.jpg?resize=1200,801",2023-10-17 03:52:32
https://techcrunch.com/2023/10/16/agnikul-funding-india-space-tech-startup/,India's Agnikul gets $26.7M to prepare for commercial space launches,"Agnikul, an Indian space tech startup developing small-lift launch vehicles, has raised $26.7 million in fresh investment as it looks to begin commercial launches using its customizable satellite rocket.
Companies — from big tech giants to startups — are looking to launch their small satellites (up to 500 kg in weight) to space to improve their existing technologies and bring new experiences, such as precise location tracking and internet connectivity for remote areas. As underlined by the European Commission, this has ramped up the demand for smaller rockets.
Small satellites have typically been launched as secondary payloads on larger launch vehicles. Existing players including Elon Musk’s SpaceX have been conducting rideshare missions for small satellite launches. However, their growing demand has encouraged space companies to seek specific solutions. Astra, Virgin Orbit and Rocket Lab are some U.S. space companies that have introduced small satellite launch vehicles to cater to the growing demand. Nevertheless, the gap between the demand and supply of small launch vehicles is still quite significant by most accounts, leaving enough room for new entrants.
Agnikul is one such entrant, via its ‘Agnibann’ small satellite rocket. It will use a single-piece engine with no assembly or conventional manufacturing process to offer a faster production timeline and tailor-made launches. It’ll instead use additive manufacturing, otherwise known as 3D printing – the same approach being taken by US-based Relativity Space. The Chennai-based startup has showcased some glimpses of its plan by launching a 3D-printed engine called Agnilet, which was successfully test-fired in early 2021.
Last year, Agnikul secured a patent for the engine and established its facility to build many such engines using end-to-end 3D printing. It also launched India’s first private launchpad and mission control center at the Satish Dhawan Space Center in Sriharikota, located in the Southern Indian state of Andhra Pradesh, in November and started the integration process of its launch vehicle Agnibaan SOrTeD (Suborbital Technological Demonstrator) in August.
Srinath Ravichandran, co-founder and CEO of Agnikul, told TechCrunch that the startup looks to complement India’s space agency, the Indian Space Research Organisation (ISRO), and is targeting to handle launches in the less than 300 kg payloads segment.
“When the customer looks at India for a solution, we are filling the gap not directly addressed by ISRO today,” he said in an interview.
ISRO currently has its Small Satellite Launch Vehicle (SSLV) to launch satellites weighing up to 500 kilograms in a low-Earth orbit. However, the space agency intends to fully transfer the vehicle to the private sector through bidding.
Ravichandran founded Agnikul along with Moin SPM and IIT Madras professor SR Chakravarthy in 2017. In December 2020, it became the first Indian private space company to sign an agreement with ISRO. Subsequently, the startup began developing its launch service for satellites weighing up to 100 kgs using the Agnibaan rocket into a 700-kilometer (about 435 miles) Earth orbit.
“We have not yet done commercial launches; we have not entered the commercialization phase. But at the same time, today, people are able to look at what we have done with the money we have received, how efficient we have been on capital, and what technology we have been able to build,” Ravichandran asserted.
Without disclosing specifics, he added that the startup has received some inbound interest from potential launch customers, mainly from companies in Europe and Japan, and also signed memorandums of understanding with a few. India also has some satellite tech startups that could become Agnikul’s customers after it starts commercialization following its first test flight, which is expected sometime before the end of 2023.
The space of small satellite launch vehicles where Agnikul operates already has Indian startup Skyroot Aerospace backed by GIC, Sherpalo Ventures and Graph Ventures, among other investors. The latter has Vikram S to take 80 kg payloads to 100-kilometer altitude. Similarly, there is global competition from players including Rocket Lab, which also has the Electron rocket for small satellite launches. However, Ravichandran said the ability to customize the vehicle depending on payload requirements helps bring a cost-effective advantage to Agnikul.
“The vehicle can be tailored to whatever payload is being asked or to whatever orbit it is being asked to go to, without compromising on the cost itself,” he said. “So just because you have only 30–40 kg to launch, we don’t believe in pricing at a very high dollar per kg. We say between 30 to 300 kgs, anyone in that range, the dollar per kg would be still the same.”
He continued that the vehicle is also being designed to be launched using mobile launchpads, and that they can be reused.
Agnikul currently has a headcount of around 225 people, predominantly in manufacturing and launch operations. It operates from four facilities and the mission control center.
With the capital infusion, the startup is looking to go beyond its first few launches and hire talent to help realize and manufacture multiple launch vehicles.
“It’s about getting out of a very design-focused phase into a phase of design+production+manufacturing, with quality as a prime focus, wherein we’ll be able to actually tell our customers that okay, your assets are safe with us,” Ravichandran stated.
“Agnikul’s pursuit of innovative space solutions aligns with our investment focus on India’s leading-edge deep tech sectors,” said Arun Kumar, Managing Partner at Agnikul investor Celesta Capital, in a prepared statement. “We are excited to support their pioneering vision and innovative approach to modernizing and democratizing the space industry. Their mission underscores the spirit of collaboration amongst the Indian Space Research Organization, space regulators, and entrepreneurs in driving advancements within India’s vibrant space-tech ecosystem.”
Agnikul sees an annual demand for about 50 tons in the less than 300 kgs satellite launch segment. Therefore, it plans to develop multiple variants of its Agnibaan rocket and increase launches from one or two per year to one or two per month over time.
“As India’s answer to SpaceX, Agnikul is poised to revolutionize the space industry not just domestically but globally. Led by Srinath, Moin and Prof. Satya, the team is super passionate, and we wish them all the success in their first mission,” said Sailesh Ramakrishnan, Managing Partner at Rocketship.vc, which also participated in the round.
Agnikul is one of the examples of how India’s space tech industry has emerged in the last few years. The country opened its space sector for private companies in June 2020, and created the Indian National Space Promotion and Authorisation Centre (IN-SPACe) as a nodal agency to collaborate with startups. Since then, it has seen significant growth in space activities.
The South Asian nation, which currently has over 150 space tech startups, introduced its anticipated space policy in April, detailing public and private cooperation guidelines. The country also saw successful launches of missions, including its highly acclaimed moon lander mission Chandrayaan-3 and solar probe Aditya-L1. Additionally, India’s growing space activities gained attention — and attracted investments — from big tech companies including Google and Microsoft.
Foreign satellite launches helped India generate $174 million, with $157 million coming in the last nine years, the government recently said in the parliament. However, the industry demands clarity on foreign direct investments in Indian space tech startups and the recently released guidelines for the private sector as it moves forward.
Equity investments in the Indian space tech startup ecosystem soared nearly 312% to $114.9 million in 2022 from $27.9 million in 2020, according to the data shared by analyst firm Tracxn. As much as $65.5 million was invested in 2023 alone.
“From our early days with Agnikul, it’s been a thrilling journey,” said Anirudh A Damani, Fund Manager at Artha Venture Fund. “Now, seeing them draw such esteemed investors showcases not just their current achievements but hints at the groundbreaking feats on the horizon in the space tech sector. Doubling our investment isn’t merely a financial move—it’s a ringing endorsement of our faith in Agnikul’s prowess. We’re all in, eager to see—and support—every giant leap they make in reshaping space exploration.”
The all-equity Series B funding round saw participation from Celesta Capital, Rocketship.vc and Artha Select Fund. Agnikul’s existing investors Artha Venture Fund, Pi Ventures, Speciale Invest and Mayfield India also participated in the round. The six-year-old startup has raised $40 million in capital to date, including the $11 million Series A round in May 2021.",https://techcrunch.com/wp-content/uploads/2023/10/agnikul-founders.jpg?w=1200,2023-10-17 03:00:58
https://techcrunch.com/2023/10/16/bandcamps-new-owner-lays-off-half-the-company/,Bandcamp's new owner lays off half the company,"Bandcamp has officially changed hands from its old new owner, Epic, to its new new owner, Songtradr, and lost half its employees in the process. Songtradr confirmed that “50% of employees received offers” to continue on under the new ownership — and naturally the other 50% didn’t.
The venerable digital music marketplace was acquired by Epic last year, but clearly the Fortnite maker wasn’t quite sure what to do with the company, and late last month resold it to music licensing platform Songtradr as part of a wave of cost-cutting.
It was known from the start that layoffs would happen, and indeed Epic and Songtradr were fairly straightforward about their necessity as part of the deal — technically the employees were laid off by Epic ahead of the formal acquisition, though it was Songtradr that decided who would and would not be hired. It was never clear whether they were talking about a few redundancies in web design and sales, or across-the-board cuts. It seems it was the latter, as Songtradr explained in a statement:
Over the past few years the operating costs of Bandcamp have significantly increased. It required some adjustments to ensure a sustainable and healthy company that can serve its community of artists and fans. After a comprehensive evaluation, including the importance of roles for smooth business operations and preexisting functions at Songtradr, 50% of Bandcamp employees have accepted offers to join Songtradr.
A spokesperson for the company added that “there were reductions made across all departments, and all departments still have original Bandcamp employees.” Acquisitions very frequently result in loss of positions, so while this isn’t unusual, 50% does seem like a lot — and a lot of people are sadly out of a job.
Notably, Bandcamp employees were in the process of unionizing, or rather some had done so already, a factor that has been suggested as contributing to Epic’s sudden distaste for ownership. I asked Bandcamp United for comment on the layoffs and have not heard back from them.
Songtradr said that it had no access to union membership, and the offers were made without any of that information.
When I asked whether Bandcamp will remain independent, Songtradr responded as follows:
Bandcamp will continue to serve its Fan and Artist community as a dedicated service and stand-alone solution. From the business structuring point of view, Bandcamp employees will be part of Songtradr and over time they will fully integrate into the Songtradr organization.
Until last year, Bandcamp seemed to be one of the few remaining places for relatively simple and equitable monetization for independent musical artists. The corporate takeover and resale does not bode well for the platform, but we will know soon what effect these layoffs and any other changes will have on the business.","https://techcrunch.com/wp-content/uploads/2023/10/bandcamp-flames.jpg?resize=1200,675",2023-10-16 22:39:08
https://techcrunch.com/2023/10/16/max-q-psyched/,Max Q: Psyche(d),"Hello and welcome back to Max Q!
In this issue:
SpaceX launches NASA asteroid mission
News from Relativity Space and more
Godspeed, Psyche.
The large NASA spacecraft is officially en route to a metal-rich asteroid (also named Psyche) after taking off on a SpaceX Falcon Heavy rocket last week. The mission marked the first time a NASA science mission has used SpaceX’s larger rocket for a launch.
Psyche (the spacecraft) will now embark on a six-year, 2.2 billion-mile journey to Psyche (the asteroid), which sits in the main asteroid belt between Mars and Jupiter. Before the spacecraft reaches its target, it will conduct a technology demonstration of the Deep Space Optical Communications experiment. If successful, it would be the first time optical communications are demonstrated beyond the Earth-moon system.
More news from TC and beyond
Astra Space is reportedly weighing up selling a 51% stake in its in-space propulsion business or selling other parts of the business, like equipment.
Evolution Space has a new deal with NASA to begin building a solid propulsion center and solid rocket motor testing at the agency’s Stennis Space Center.
Relativity has signed a launch agreement with Intelsat that would see the telecom giant’s satellites fly on a Terran R rocket as early as 2026.
Max Q is brought to you by me, Aria Alamalhodaei. If you enjoy reading Max Q, consider forwarding it to a friend.",https://techcrunch.com/wp-content/uploads/2019/12/tc-space-hero.gif?w=949,2023-10-16 22:00:41
https://techcrunch.com/2023/10/16/google-lobbies-against-legally-mandated-age-verification-for-minors/,Google lobbies against legally mandated age verification for minors,"Google is challenging proposed laws that would require online services to implement age checks in a new framework that theorizes how technology companies should approach children’s safety online. The framework, titled the “Legislative Framework to Protect Children and Teens Online,” is the tech giant’s response to congressional child online safety proposals.
In its set of principles, Google dismisses policies that would require online services to verify the age of their users before allowing them access to their platforms. For instance, Utah passed a law that aims to start requiring social media companies to verify the age of a user seeking to maintain or open an account. Google says that such age verification policies will lead to trade-offs and possibly restrict access to important information.
“Good legislative models — like those based on age-appropriate design principles — can help hold companies responsible for promoting safety and privacy, while enabling access to richer experiences for children and teens,” the company wrote in a blog post announcing the framework. “Of course, as policymakers contemplate these issues, they should carefully consider the broader impacts of these bills and avoid side effects like blocking access to critical services, requiring people (including adults) to submit unnecessary identification or sensitive personal information.”
The company states that “data-intrusive methods,” such as verification with government IDs, should be limited to “high-risk” services that deal with alcohol, gambling, or porn. For context, Louisiana recently passed a law that requires age verification to access adult websites in an attempt to prevent kids from seeing online porn. Google’s framework is not against age verification in this manner.
Google argues that instead of implementing legislation that would require online services to verify ages, these companies should be required to “prioritize the best interests of children and teens in the design of their products.” Google says that online services used by children and teens should be required to assess the collective interests of children based on “expert research and best practices, to ensure that they are developing, designing and offering age-appropriate products and services.”
In other words, Google says online services shouldn’t be forced to block teens and children from their platforms, and should instead be required to design products appropriately.
Today’s framework comes four years after the Federal Trade Commission (FTC) fined Google and YouTube $170 million for violating children’s privacy. The FTC said YouTube illegally collected personal information from children and used it to profit by targeting them with ads. As part of the settlement, the FTC said YouTube had to develop and maintain a system that asks channel owners to identify their child-directed content to ensure that targeted ads are not placed in such videos.
Interestingly, Google’s framework notes that there should be legislation banning personalized advertising for children and teens. Earlier this year, Senator Ed Markey (D-Mass.) announced the reintroduction of the Children and Teens’ Online Privacy Protection Act (COPPA 2.0), which would ban targeted ads to minors. Google argues that “for those under 18, legislation should ban personalized advertising, including personalization based on a user’s age, gender, or interests.”
In a separate online safety framework published today by YouTube, the video platform’s CEO Neal Mohan said the service doesn’t serve personalized ads to kids.
Despite this claim, a recent report from advertising performance optimization platform Adalytics alleges that YouTube continues to serve targeted ads to minors. In a blog post, Google stated that Adalytics’ report was “deeply flawed and uninformed.” The report caught the attention of Senator Marsha Blackburn (R-Tenn.) and Senator Markey, who sent a letter to the FTC asking the government agency to investigate the matter.","https://techcrunch.com/wp-content/uploads/2021/07/GettyImages-1207206237.jpg?resize=1200,800",2023-10-16 21:39:05
https://techcrunch.com/2023/10/16/websummit-derailed-by-founders-public-fight-with-those-supporting-israel-in-hamas-conflict/,Web Summit derailed by founder's public fight with those supporting Israel in Hamas war,"Web Summit, the big tech conference brand that runs events in several cities and whose 70,000 person flagship event in Lisbon is taking place next month, is running into a wall — a wall of outrage. Founders, investors and others from the technology community in Israel have gone ballistic over comments made by the founder and figurehead of Web Summit, Paddy Cosgrave, related to the fighting underway across Israel and Gaza, specifically his criticism of Israel’s retaliatory actions.
Now, the anger with Cosgrave has gone viral, and today it looked like it was about to overrun promotion efforts for Web Summit.
The situation also highlights how Israel’s tech industry, the country’s most valuable and arguably best-known export, built on business development and relationships, has been willing to cut those ties in the battle of public opinion in this most polarizing of conflicts.
“Here in Israel, we’re basically now in rage mode after the first week of shock and awe,” one tech source told TechCrunch. “We ain’t got time to f_ck around with anyone remotely suggesting Israel needs to sit it out and not put an end to Hamas.”
It all started the day that Hamas busted through Israel’s walls and rampaged through villages and a music festival on a murdering and pillaging spree, killing 1,400 people, mostly civilians. Hamas, the ruling party in the Palestinian territory, is considered a terrorist organization by the U.K., U.S. and other countries; they also took 199 hostages back into Gaza.
On that day, Cosgrave was in Doha, Qatar, the city where Web Summit will be holding its newest event in four months’ time. As some were taking to social media to express shock, or sympathy, or in some cases, advocating for restraint, some took more critical stances. Cosgrave, for his part, posted data on X of the human cost of the Israel-Palestine conflict between 2008 and 2023. It omitted the events (and casualties) of the weekend.
That stirred debate, but Cosgrave didn’t acknowledge that the numbers were out of date or comment about the attacks in Israel. He instead proceeded, over the next several days — between posts about Qatar, political comments related to Ireland, and rugby reactions — to put out several more posts, all highlighting the opinion that Israel was taking an unjust approach.
As the posts racked up a range of alarming and extreme responses on both sides of the argument, Cosgrave doubled down. On Friday, he noted he was “shocked by the rhetoric and actions of so many Western leaders” in supporting Israel. But as attention mounted, the rejections to Web Summit started, too.
Some of the highlights (or I guess you could say, lowlights):
David Marcus said he would never again attend, sponsor or speak at another Web Summit event. “Saddened by your ill-informed stance. You could’ve taken a more nuanced one, condemning these atrocities and calling for restraint. That would’ve been acceptable,” wrote David Marcus, the longtime fintech entrepreneur and Meta executive, in a tweet yesterday. “You chose to support terrorists. As such I’ll never attend/sponsor/speak at any of your events again.”
“Will refuse to work with anyone who speaks at this conference in Qatar for the rest of my career,” chimed in Keith Rabois, the Founders Fund partner and entrepreneur.
Ori Goshen, the co-founder and co-CEO of AI21 Labs, announced on LinkedIn that he would no longer be giving a keynote at Web Summit.
“It’s bad enough that summit CEO Paddy Cosgrave didn’t see fit to express horror at the sickening atrocities committed by Hamas on October 7th,” he said. “But as immoral as that is, Paddy Cosgrave chose to not only ignore these but instead post something against the policies of the Israeli government. Leaving aside his very partial understanding of history and geopolitics, this response was abhorrent. We at AI21 cannot be part of such indecency and moral bankruptcy. We will not attend WebSummit, and I will not give the keynote. #cancelwebsummit #standwithisrael”
Then the Israeli ambassador to Portugal, Dor Shapiro, waded in.
“Today, I wrote to the Mayor of Lisbon informing him that Israel will not participate in the #WebSummit conference due to the outrageous statements made by the conference CEO Paddy Cosgrave. Even during these difficult times, he is unable to set aside his extreme political views and denounce the Hamas terrorist activities against innocent people,” he wrote, also on LinkedIn. “Dozens of companies have already canceled their participation in this conference, and we encourage more to do so.”
By today, Cosgrave appeared to walk back his statement. “We are devastated to see the terrible killings and the level of innocent civilian casualties in Israel and Gaza,” Cosgrave wrote, nine days Hamas invaded Israel. “We condemn the attacks by Hamas and extend our deepest sympathies to everyone who has lost loved ones. We hope for peaceful reconciliation.”
Under so much fire, however, he subsequently dug in his heels, tweeting afterward: “To repeat: War crimes are war crimes even when committed by allies & should be called out for what they are. I will not relent,”
Damage has been done to Cosgrave and the Web Summit brand in the interim.
“Hard to take this statement at face value — given all the tweets @paddycosgrave has been liking over the last few days. I saved several of them on the attached google doc (so we have a record when the @WebSummit PR team asks him to delete them),” said Josh Kopelman, the founder of First Round Capital.
Kopelman separately suggested in a tweet that Cosgrave is in the pocket of Doha and Qatar, a country that many believe is connected to the financing of Hamas. That was enough to push Garry Tan, the head of Y Combinator, over the edge, too.
“I refuse to appear at Web Summit and am canceling my appearance,” he said. “I condemn Hamas and pray for peace for the Israeli and Palestinian people.”
We’ve reached out to Tan and Kopelman to ask if they are advising portfolio companies and partners at their firms against also attending. Tan declined to comment, and Kopelman has yet to respond.
Web Summit has provided us with a statement on the cancellations, saying that the organization is talking to “a number of people about their attendance at Web Summit” but is not in a position to discuss exacts and individuals.
“We understand that it is an incredibly sensitive and painful time during this utter tragedy of war. We want to reiterate our devastation for the loss of innocent life in Israel and Gaza. We strongly condemn the horrific attacks by Hamas on Israelis. Web Summit’s mission is to connect people and ideas changing the world from all around the globe. The more voices we have from around the world, the more we can help change the world for the better,” she added. “We are saddened to hear that some Israelis in the tech community will no longer be attending Web Summit. We regret any hurt caused and extend our deepest sympathies to everyone who has lost loved ones. We hope for peaceful reconciliation.”
The spokesperson said that last year’s event attracted about 71,000 people and this year it’s on track to “max out” at 70,000.
A public page in Notion titled “techcondemingterror” is tracking the growing response. It now includes press clippings, comments from a number of leaders in Israel’s technology industry and comments.",https://techcrunch.com/wp-content/uploads/2022/09/43965968860_0f0b8699b9_c.jpg?w=800,2023-10-16 21:14:56
https://www.innovationnewsnetwork.com/deep-south-resources-back-in-business-in-copper-exploration-industry/38337/,Deep South Resources is back in business in copper exploration,"Deep South Resources is back to upgrading its copper exploration game in Southern Africa.
Deep South Resources, a Canadian mining exploration company working in Africa, has concluded a long legal battle and is now continuing its work and planning for the future. The company specialises in copper exploration in Africa, particularly in Namibia and Zambia.
We interviewed their president and CEO, Pierre Léveillé, to find out what happened and what is next for the company.
Can you briefly summarise Deep-South Resources? What is your background and key objectives?
Deep South Resources is a mining exploration company specialised in exploring for copper deposits. Our main area of expertise is in the region of Southern Africa, and our main projects are in Namibia and Zambia.
In Namibia, we have the Haib copper project. Here, we’re starting a feasibility study on a copper deposit of 5.3 billion lbs, and we know we can add more.
In Zambia, the company holds three very large exploration licenses that are very well situated, in the heart of the Copper Belt. As of yet, there has been no drilling, so we are really starting from scratch.
The company’s key objective is to explore, find, and develop the largest possible copper deposits. This is something we have already started in Namibia and intend to continue.
In terms of copper, Zambia is probably up there with the Democratic Republic of the Congo, and consequently we believe the project has great promise.
We are hunting for copper deposits in places with large, proven copper reserves and resources.
We think that the project will be productive enough to attract major players to buy us or join us in development. Our main expertise is exploration and development in the Southern part of Africa.
Although we are not looking towards copper production right now, we have the expertise and potential to do it ourselves in the future.
What is the potential of Zambia and Namibia in copper exploration?
The potential for copper exploration in Namibia is greater because the project is more advanced. As well as this, we already have established a defined large resource, and we know we can increase it. The potential, therefore, relies on increasing that already large deposit.
On the exploration side, our main goal and the main potential is to increase the grade, which we are on track to do.
We estimate that by the middle of 2024, we will have increased the grade by at least 25%.
The Zambian project holds major exploration potential. Situated right in the middle of the Copper Belt, surrounded by nine very large copper mines, and we are on the same geology as them. The first exploration programme has enabled us to discover very large copper anomalies. We’re on the right track; the potential to find a deposit is great.
As opposed to Namibia, where the deposit has already been discovered, in Zambia we are still at the deposit discovery stage.
How is Deep South Resources embracing the green revolution?
The technologies we are looking to use for potential future mining are greener metal extraction methods. The classic way to extract metals, such as grinding, milling, roast leaching and flotation, is quite old-fashioned, and it’s highly demanding in terms of power. The roasting also causes a lot of air pollution.
We will not necessarily avoid roasting in Namibia, but it will be limited and will be combined with other technologies, such as heap leaching.
We are also looking at bio heap leaching. The beauty of bio heap leaching is that when you finish the treatment, the gravel that remains is stable. This means that no pollutive material will contaminate the ground and water table. It’s not totally green, but it’s a lot greener than roasting and flotation.
Have you encountered any challenges whilst undertaking these copper exploration projects? And if so, how have they been overcome?
In Namibia, we have the Haib project, which we bought in May 2017. Four years later, in June 2021, the Minister of Mines and Minerals Development refused to renew our licence, stating that he was told that we don’t do any work, so we don’t deserve to have the project.
Naturally, we argued that we had five drills on-site, with 60 employees active. We had shipped one ton of samples to Australia for metallurgical test work; there was a lot of activity. There was a serious difference between the information he received and the information we communicated to the Ministry. Nevertheless, he decided to pull the plug on our licence.
We immediately went to the High Court of Namibia, and requested an injunction over that area to prevent the ministry from granting a licence to anyone else. In doing this, we discovered another private company had applied for our licence six months prior to the expiry. This is very unusual.
In Namibia, when you apply for an existing licence in general, the Ministry will state that the license is still valid and to come back later when it has expired.
Our second step was to request the court to review the Minister’s decision. The court case lasted a little over a year. We had the final hearing in October 2022.
At the end of the same month, there was a big scandal that ran in the press in Namibia, first on social media and then through the traditional press. The scandal concerned allegations that were similar in nature to those experienced by Deep-South Resources. Potential issues surrounding license renewals were highlighted during this time and we were hopeful of a positive outcome.
It was such an important scandal that we thought it was impossible the judge hadn’t seen it. We were also in the fortunate position of having an injunction on the project. Thus, when the judge rendered a strong verdict in our favour, the licence was still available.
It is not in the mandate of the court to order the Ministry to grant the licence, but he ordered them to reopen the application for renewal that was originally denied in 2021, this time considering all the facts presented by Deep-South.
We wanted to defend ourselves to protect the interests of our shareholders. We finally got the licence back in July 2023, two years later. That situation has been seriously damaging; our market capitalisation has dropped by 80%.
We did not get any compensation from the Ministry. If we want compensation, we have to institute a new court case where we will request damage, but on the other hand, we want to keep a good relationship with the Ministry, so we will avoid legal disputes.
That’s been our biggest challenge, not only for us, but for the numerous high-level Namibians who were really annoyed by this scandal and expressed their discomfort with the situation. At the end of the day, the rule of law prevailed and we are now back on track to continue our exciting progress in Namibia.
We also have had smaller challenges, but it is part of our business. As an example, we’re owed a lot of money from the value-added tax, but it takes an awful lot of time to be reimbursed. At some point, they tried to change the rules to not pay back exploration companies, but now it seems to be back on track.
We spent over a year awaiting a payment of around $135,000, which is a lot of money for a small exploration company. We also have issues from time to time, with work visas for foreign employees. The procedure is long and complex compared to Zambia. It’s more administrative stuff that sometimes makes our life a bit difficult. But we always resolve these things.
So what’s next for Deep South Resources?
Quite a lot! We just closed the financing. So even if the market is very difficult at the moment for small companies, we just closed a financing of $2m, and we’re pretty happy because three institutions have taken $1.5m of the financing. They’re very good supporters. With this funding in place, we are resuming the drilling programme that was suspended on the Haib copper project, as well as restarting the feasibility study procedures.
At the completion of the drilling programme, we will also have a new resource estimation somewhere in early 2024. We just announced that we have appointed the MSA group in South Africa to complete this resource estimation.
We are also looking at appointing an engineering firm for the environmental impact study that goes with the feasibility study. We’re working on receiving proposals now from engineering firms for the feasibility study.
We are resuming our activities with METS Engineering in Australia and CSIRO, the governmental laboratory in Australia, for metallurgical test work, heap leaching and other tests. All of this will be announced during the next month, which means that by the end of October, we will be in full swing with the resumption of our activities.
At the same time, in Zambia, we are starting a small programme of geophysical-induced polarisation surveys on the Luanshya West project. We have generated some very interesting soil sampling results recently, and we want to better define the drilling targets for next year. We will be very busy. Stay tuned because there’s a lot of activity coming. We’re pretty excited about these developments and exploration programmes.
After the two years we spent fighting for the company, we look forward to a very good year coming up in the copper exploration field.
Please note, this article will also appear in the sixteenth edition of our quarterly publication.
Go to this partner's profile page to learn more about them",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/DEEPSOU1-28928-shutterstockMo-Femi-Pearse_1270772323-1024x576.jpg,2023-10-17 12:30:24
https://www.innovationnewsnetwork.com/how-the-space-age-is-polluting-our-atmosphere/38320/,How the Space Age is polluting our atmosphere,"The Space Age is leaving fingerprints on the stratosphere – one of the most remote parts of the planet – which has potential implications for climate, the ozone layer, and the continued habitability of Earth.
Using tools hitched to the nose cone of their research planes and sampling more than 11 miles above the planet’s surface, researchers have discovered that the Space Age has resulted in significant amounts of metals in aerosols in the atmosphere.
The increasing amount of debris is likely due to more frequent launches and returns of spacecraft and satellites.
This mass of metal is changing atmospheric chemistry in ways that may impact Earth’s atmosphere and the ozone layer.
The study, ‘Metals from spacecraft re-entry in stratospheric aerosol particles,’ is published in Proceedings of the National Academy of Sciences.
The rising use of metal and polluting materials in space
“We are finding that the Space Age has released human-made materials in what we consider a pristine area of the atmosphere,” said Dan Cziczo, one of the study’s authors.
The team detected more than 20 elements in ratios that mirror those used in spacecraft alloys.
They found that the mass of lithium, aluminium, copper, and lead from spacecraft re-entry far exceeded those metals found in natural cosmic dust.
Nearly 10% of large sulfuric acid particles – the particles that help protect and buffer the ozone layer – contained aluminium and other spacecraft metals used in the era of the Space Age.
Scientists estimate that as many as 50,000 more satellites may reach orbit by 2030. The team calculates that means that, in the next few decades, up to half of stratospheric sulfuric acid particles would contain metals from re-entry.
What effect that could have on the atmosphere, the ozone layer and life on Earth is yet to be understood.
The Space Age is in full swing, but how does this impact space pollution?
Spacecraft launches and returns were once international events. The launches of Sputnik and the Mercury missions were front-page news.
Now, a quickening tide of innovation and loosening regulation means that dozens of countries and corporations are able to launch satellites and spacecraft into orbit and fuel the Space Age.
All those satellites must be sent up on rockets, and most of that material eventually comes back down.
The Space Age has left behind a trail of metals that may change the atmosphere in ways scientists don’t yet understand.
“Just to get things into orbit, you need all this fuel and a huge body to support the payload,” Cziczo commented.
“There are so many rockets going up and coming back and so many satellites falling back through the atmosphere that it’s starting to show up in the stratosphere as these aerosol particles.”
He concluded: “Changes to the atmosphere can be difficult to study and complex to understand.
“But what this research shows us is that the impact of the Space Age on the planet may be significant. Understanding our planet is one of the most urgent research priorities there is.”",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/shutterstockstudiovin_2208172963.jpg,2023-10-17 10:22:21
https://www.innovationnewsnetwork.com/research-innovation-key-renaissance-european-solar-manufacturing/38305/,Research and innovation is key for the renaissance of European solar manufacturing,"The world is switching to green energy, and Europe is presented with an opportunity to ensure solar energy plays the critical role it is meant to.
The European solar manufacturing landscape is at a critical crossroads, with a perfect storm emerging. Over ordering on the demand side, combined with overcapacity on the supply side, has resulted in a record drop in prices for solar modules and other system components. It has never been more important for the EU to advance its industrial strategy for solar PV and support European solar manufacturers.
This is where research and innovation (R&I) comes in. Expanding the European solar manufacturing base also requires an expansion of the EU’s R&I activities. Increased investments in innovative technologies are a key component in the long-term reindustrialisation of Europe’s solar manufacturing base.
Undoubtedly, R&I has been high on the Commission’s agenda. Under Horizon Europe, the EU’s key R&I funding programme, the Commission has allocated a significant budget of €95.5bn. More than 40% of that fund – €40bn – is earmarked for research that supports the European Green Deal.
Previously, the Commission invested €4.99bn towards clean energy technologies under its Horizon 2020 programme; only 10% of this was earmarked for PV projects. The Directorate-General for Research and Innovation has promised to help Europe “stay ahead of the game, and accelerate the roll-out of the EU’s strategic net-zero technologies,” like solar PV.
Solar has also been booming across Europe, especially in the last three years. Over 40GW of solar was installed in 2022, nearly double what was installed the previous year. Already in the first half of 2023, solar generation grew by 13%, according to a recent Ember report.
We have truly entered a thriving solar era, with record drops in fossil fuel generation. Every year, solar generation and installation numbers are increasing.
The R&I challenge
With solar numbers on the rise, and the EU setting its R&I agenda, you might wonder what the issue is? The reality is that the rest of the world has also woken up to the strategic role of solar. China is by far the world leader in the manufacturing of solar. The US has the Inflation Reduction Act which is spurring a boom in American solar manufacturing. India, Turkey, and South Africa are all making similar moves.
There are a number of solutions that EU leaders can take to ensure that European solar manufacturing plays a significant role in the globalised solar supply chain, like changing subsidy rules, setting up a dedicated financing instrument, and delivering ‘resilience’ auctions under the incoming Net-Zero Industry Act.
One wider tool we can’t forget is the role of R&I in supporting Europe’s manufacturing base – the critical link between ‘labs’ and ‘fabs’. The Fraunhofer Institute for Solar Energy Systems (ISE) has found that in Germany, PV technology development has delivered cost reductions of 36% per year on average since 2010, thanks to a combination of upscaling of manufacturing, and continued R&I advancements.
According to the Commission’s third Progress Report on the Competitiveness of Clean Energy Technologies, half of the EU’s greenhouse gas reductions expected by 2050 will require technologies that are not yet commercially available. Solar PV alone is set to generate more than 60% of the EU’s electricity by 2050, requiring more private and public investment into clean energy research.
The perovskite potential
The development of new cell technologies like perovskite solar cells reflect the necessity of expanded R&I investments in European solar manufacturing. In May 2023, Oxford PV, a perovskite solar manufacturer, set the record for the highest recorded efficiency of any commercial-sized solar cell in its Brandenburg an der Havel factory in Germany.
The cell converted 28.6% of the sun’s energy into electricity, and was made by placing a thin film of the perovskite material onto a conventional silicon solar cell. The combined ‘perovskite-on-silicon’ tandem solar cell achieves a conversion efficiency that is substantially higher than that of mainstream silicon-only solar cells, which average 22–24%.
Commenting on the achievement, David Ward, Chief Executive Officer at Oxford PV, noted that their, “innovative solar cells are close to being in the hands of our module-manufacturing customers,” with the focus now on ramping up production.
Recent findings from a team at the University of Surrey have also illustrated the potential of perovskite. They found that a nanoscale ‘ink’ coating improved the perovskite solar cells’ stability, making them suitable for mass production.
Researchers made this breakthrough when they discovered an aluminium oxide that minimises the drop in efficiency during the conditioning of perovskite solar cells. Perovskite has also been used to create self-healing solar panels on satellites in low-Earth orbit that can recover 100% of their efficiency, even after being damaged by radiation in space.
In general, the perovskite material is lighter and cheaper than a silicon-based solar cell, and is extremely efficient. It has even been praised as a ‘miracle material,’ a label earned with recent developments. When it reaches the market at scale, it will transform the PV sector.
All of these findings have emerged from investments into one innovative technology. The European solar PV sector’s research representation to the European Commission – ETIP PV – has made the point that getting R&I to production scale will require continuous EU funding.
Otherwise, we risk only investing in technologies that will one day be obsolete, especially given the acceleration of innovation cycles.
For example, fully realising the potential of perovskite solar cells will require more funding to facilitate its mass-production, and improve its shelf life; currently, perovskite deteriorates quickly when exposed to light, voltage, or heat.
More research will also be required to substitute the lead currently used in perovskite solar cells, for a more eco-friendly alternative, while retaining its efficiency.
A step forward for European solar manufacturing
Undoubtedly, the EU is leading in several areas of PV research and innovation, including in perovskite tandem solar cells. Oxford PV, Fraunhofer ISE, and Helmholtz-Zentrum Berlin, all European research and development bases, hold global solar cell conversion efficiency records.
However, rapid advancements in R&I investments by public and private actors in other regions are catching up to the EU, and threaten to leave it behind, especially when it comes to the industrialisation of R&I.
For example, in June, at a Shanghai trade show, a Chinese manufacturer already announced its plans to commercialise a perovskite solar cell. The EU is already lagging behind in the production of ingots and wafers, an important segment of solar manufacturing. It has also seen a notable loss of expertise in key segments of the PV R&I landscape.
Just like investments in manufacturing, the EU will need to expand and intensify its R&I investments if it wants to keep up, especially with its renewed ambitions for PV manufacturing.
Alongside renewed investments in PV manufacturing capacity, the EU must also invest in the industrialisation of the results of R&I efforts, and strengthen its investments towards developing the next generation of PV technologies.
For example, the perovskite potential will be lost without a strong investment commitment to bridging the manufacturing gap and delivering innovation at scale.
The ETIP PV White Paper on manufacturing puts it best: adequate financing is needed throughout the EU PV value chain. Many solutions can be implemented to accelerate R&I financing at the European level. This could include relaxing the EU’s State Aid rules – its Temporary Crisis and Transition Framework (TCTF) – to accelerate the channelling of funding towards projects.
The rebirth and long life of European solar manufacturing needs a strong research and innovation base. Europe has its part to play in the world’s solar manufacturing story.
Innovations like perovskite solar cells are just one example waiting on the horizon, reflecting the potential of R&I investments to revolutionise the solar industry.
However, the PV manufacturing storm is still brewing; sustained and increased investments in EU R&I investments will help build resilience against this storm. Left unchecked, this storm will only get worse.
Now is the time to double down on EU research and innovation, so that we can guarantee the EU’s solar renaissance.
Please note, this article will also appear in the sixteenth edition of our quarterly publication.",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/1-Copyright-Meyer-Burger-Thalheim-factory.jpg,2023-10-17 09:28:51
https://www.innovationnewsnetwork.com/new-innovation-challenge-launched-tackle-bias-in-ai-systems/38298/,New innovation challenge launched to tackle bias in AI systems,"UK companies can now apply for up to £400,000 in government investment to fund innovative solutions tackling discrimination and bias in AI systems.
The competition will look to support up to three groundbreaking homegrown solutions, with successful bids securing a funding boost of up to £130,000 each to tackle bias in AI.
It comes ahead of the UK hosting the world’s first major AI Safety Summit to consider how to best manage the risks posed by AI while harnessing the opportunities in the best long-term interest of the British people.
Tackling bias in AI systems is a major priority
The first round of submissions to the Department for Science, Innovation, and Technology’s Fairness Innovation Challenge, delivered through the Centre for Data Ethics and Innovation, will nurture the development of new approaches to ensure fairness underpins the development of AI models.
The challenge will tackle the threats of discrimination and bias in AI by encouraging new approaches, which will see participants building a wider social context to develop their models from the off.
Fairness in AI systems is one of the government’s key principles for AI, as set out in the AI Regulation White Paper. AI is a powerful tool for good, presenting near-limitless opportunities to grow the global economy and deliver better public services.
Minister for AI, Viscount Camrose, said: “The opportunities presented by AI are enormous, but to fully realise its benefits we need to tackle bias in AI.
“By ensuring AI models do not reflect bias found in the world, we can not only make AI less potentially harmful but ensure the AI developments of tomorrow reflect the diversity of the communities they will help to serve.”
Harnessing a new, UK-led approach
While there are a number of technical bias audit tools on the market, many of these are developed in the US.
Although companies can use these tools to check for potential bias in AI systems, they often fail to fit alongside UK laws and regulations.
The challenge will promote a new UK-led approach which puts the social and cultural context at the heart of how AI systems are developed, alongside wider technical considerations.
This will focus on two areas. First, a new partnership with King’s College London will offer participants from across the UK’s AI sector the chance to work on potential bias in AI models. The model, developed with Health Data Research UK with the support of NHS AI Lab, is trained on the anonymised records of more than ten million patients to predict possible health outcomes.
Second is a call for ‘open use cases’. Applicants can propose new solutions which tackle discrimination in their own unique models and areas of focus, including tackling fraud, building new law enforcement AI tools, or helping employers build fairer systems which will help analyse and shortlist candidates during recruitment.
Companies currently face various challenges in tackling bias in AI, including insufficient access to data on demographics and ensuring potential solutions meet legal requirements.
The CDEI is working closely with the Information Commissioner’s Office (ICO) and the Equality and Human Rights Commission (EHRC) to deliver this Challenge. This partnership allows participants to tap into the expertise of regulators to ensure their solutions marry up with data protection and equality legislation.
Stephen Almond, Executive Director of Technology, Innovation and Enterprise at the ICO, explained: “The ICO is committed to realising the potential of AI for the whole of society, ensuring that organisations develop AI systems without unwanted bias.”
Baroness Kishwer Falkner, Chairwoman of the Equality and Human Rights Commission, added: “Without careful design and proper regulation, bias in AI systems has the potential to disadvantage protected groups, such as people from ethnic minority backgrounds and disabled people.
“Tech developers and suppliers have a responsibility to ensure that the AI systems do not discriminate.”",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/shutterstockSansoen-Saengsakaorat_2345949869.jpg,2023-10-17 08:27:32
https://www.innovationnewsnetwork.com/new-battery-recycling-method-efficiently-recovers-aluminium-and-lithium/38292/,New battery recycling method efficiently recovers aluminium and lithium,"Researchers from Chalmers University of Technology have developed a new battery recycling method that allows the recovery of 100% of the aluminium and 98% of the lithium in electric car batteries.
The new battery recycling method also minimises the loss of valuable materials such as nickel, cobalt, and manganese.
The researchers use oxalic acid to reduce costs and harmful chemicals in the process.
The paper, ‘Complete and selective recovery of lithium from EV lithium-ion batteries: Modelling and optimisation using oxalic acid as a leaching agent,’ is published in the journal Separation and Purification Technology.
How does the new battery recycling method work?
The team fine-tuned the temperature, concentration, and time to use the oxalic acid to facilitate EV battery recycling.
Martina Petranikova, Associate Professor at the Department of Chemistry and Chemical Engineering at Chalmers, said: “We need alternatives to inorganic chemicals. One of the biggest bottlenecks in today’s processes is removing residual materials like aluminium.
“This is an innovative method that can offer the recycling industry new alternatives and help solve problems that hinder development.”
The aqueous-based recycling method is called hydrometallurgy.
Traditional hydrometallurgy
In traditional hydrometallurgy, the metals in an EV battery cell are dissolved in an inorganic acid. The impurities, such as aluminium and copper, are then removed.
The valuable metals such as cobalt, manganese, lithium, and nickel are separately recovered.
Although the amount of residual copper and aluminium is small, several purification steps are required. Each step in this process can cause lithium to be lost.
The new method reduces the waste of valuable metals
The new battery recycling method reduces the waste of valuable metals needed to make new batteries by reversing the order of the traditional process. By doing this, lithium and aluminium are recovered first.
The latter part of the process leaves aluminium and lithium in the liquid, and the other metals in the solids. Aluminium and lithium then need to be separated.
“Since the metals have very different properties, we don’t think it’ll be hard to separate them. Our method is a promising new route for battery recycling – a route that definitely warrants further exploration,” said Léa Rouquette, PhD student at the Department of Chemistry and Chemical Engineering at Chalmers.
“As the method can be scaled up, we hope it can be used in industry in future years,” concluded Petranikova.",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/shutterstockMaxx-Studio_191931434.jpg,2023-10-17 08:02:49
https://www.innovationnewsnetwork.com/software-supply-chain-security-how-spot-cyber-risks-hidden-plain-sight/38274/,Software supply chain security: How to spot cyber risks hidden in plain sight,"Matt Middleton-Leal, Managing Director EMEA at Qualys, discusses the importance of software supply chain security.
Every day, you will see headlines around IT security issues affecting companies and public sector bodies. These organisations will be affected by attacks that could involve everything from data theft or sensitive information being leaked through to full blown ransomware deployments and interruptions to service. One of the biggest routes for these attacks over the past few years has been the software supply chain – if an attacker can jeopardise one software component, then they can attack multiple companies through that hole.
Applications are now more complex too. They have more moving parts, from open source software projects and internally developed software code through to third party applications that are embedded into services. They can expand and scale up to meet demands, created on-demand using infrastructure as Code or in Kubernetes containers. And they can be created using public software hosted on services like GitHub or the Python Package Index.
As a result, many security teams are not able to track those software assets effectively. To solve this, we have to make it easier to manage those software components and prevent potential attacks from coming in.
Bills of materials and software management
In practice, security teams need insight into what IT assets the organisation has. Without this list, it is impossible for IT security leaders to call their organisations secure. This has to be expanded to software as well.
The first step for this is to create a full inventory of the software that you have in place and the components used to make it. For internal applications – otherwise termed first-party software – this level of insight is often lacking. Studies have shown that between 70 to 90% of first-party software includes open-source components; according to our analysis of more than 13 trillion anonymised data points in the 2023 TruRisk Research Report, 79% of servers installed use open-source components.
Application and security operations teams most commonly rely on manual checks or siloed scripts to evaluate the security of first-party software. This depends on how well those checks work and how often they are carried out, and delays teams from prioritising the right risks for remediation.
Setting up a full process for software supply chain management starts with knowing what applications components you have and what versions those components are. Traditional vulnerability assessment or software composition analysis tools do not detect the presence of embedded open-source packages across the production environment. So, expanding your approach to cover these first-party software applications should be a natural first step.
Alongside looking internally, you should also look at the software that you consume from others. This third-party software will itself be built of different components and services, and any one of those can have an issue. As you don’t ‘own’ the software, it may be difficult to peer inside and know if there are any out-of-date components that could affect your security.
To fix this problem, the US Government supported the use of software bill of materials, or SBOMs. SBOMs provide customers with a list of all the components used within a given application, so security teams can spot any faults that come up. However, uptake of SBOMs is still in its infancy.
Regulation may help on this in time. SBOMs were mandated by the US Government in an Executive Order in May 2021, while the European Union’s Cyber Resilience Act also includes resolutions to implement SBOMs for hardware and software manufacturers that provide products to European consumers. The UK Government is also developing its approach to cyber resilience, and SBOMs are expected to be part of that approach.
The challenge here is that SBOMs are still relatively new, and CISOs have other more pressing issues around security that take up their teams’ resources and commitment. Regulation may force this up the agenda in time, but software supply chain attacks are happening now. SBOMs can deliver more insight into what cyber risks exist and consequently where to concentrate. As part of an overall software supply chain strategy, SBOMs will be essential in future, but the data they provide will help you manage risk around misconfigurations or vulnerabilities now too.
Improving overall processes around security
Just like any security process, software supply chain security depends on the data coming in and how quickly that information can be turned into actions. The issue is that software today is so complex that you can easily miss potential problems, whether they are in your organisation’s own software or contained in another company’s products.
Getting more data on what is in place is necessary in order to begin improving security. However, this data is not useful without the right context. Without that insight, you will not be able to prioritise where changes are needed in your own applications, and you will not be able to put pressure on your suppliers around their updates. Equally, you will not be able to manage those potential risks effectively and mitigate problems before they come up.
To improve your approach to software supply chain management, you will have to look at your overall approach to risk and how you manage software within your organisation. Bringing first-party software and third-party application risk data together will help your team understand the potential threats that exist, where changes are needed, and how you can support those problems getting fixed in an efficient and timely manner.
To make this work for you, look at how you can automate the data gathering so you have a continuous level of insight into what you have in place, and then prioritise your risks so you can always get ahead of any potential problems before they become serious or lead to attempted attacks.",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/©-shutterstock3rdtimeluckystudio_2268705275.jpg,2023-10-17 07:39:18
https://venturebeat.com/ai/ai-platform-alliance-will-drive-ai-to-be-more-open-efficient-and-sustainable/,"AI Platform Alliance will drive AI to be more open, efficient and sustainable","VentureBeat presents: AI Unleashed - An exclusive executive event for enterprise data leaders. Network and learn with industry peers. Learn More
A group of prominent companies in the AI industry announced the establishment of the AI Platform Alliance, a consortium aimed at making sure that AI platforms will be more open, efficient, and sustainable.
The consortium seeks to address the growing demand for scalable AI solutions and overcome the challenges posed by the increasing compute power required for AI training and inferencing. The founding members of the alliance include Ampere, Cerebras Systems, Furiosa, Graphcore, Kalray, Kinara, Luminous, Neuchips, Rebellions and Sapeon, with more companies expected to join in the future.
The formation of the AI Platform Alliance comes at a critical juncture, not only for the technology industry but also for the world as a whole. The rapid expansion of AI has led to an unprecedented need for compute power to train and run AI workloads. The aim of the group is to “promote better collaboration and openness.”
While AI training demands substantial compute resources upfront, the total compute power required for AI inferencing can be up to ten times higher, posing an even greater challenge as AI usage scales up. One of the primary objectives of the AI Platform Alliance is to enhance the power and cost efficiency of AI hardware, surpassing the performance delivered by traditional graphics processing units (GPUs).
Event AI Unleashed An exclusive invite-only evening of insights and networking, designed for senior enterprise executives overseeing data stacks and strategies. Learn More
Recognizing the complexity involved in implementing AI solutions, the AI Platform Alliance will collaborate to validate joint AI solutions that offer superior alternatives to the prevailing GPU-based status quo. Notably, Nvidia, the biggest designer of AI chips and GPUs, isn’t a member of the alliance.
By fostering community-driven development, the consortium aims to expedite AI innovation by creating more transparent and accessible AI platforms. This collaborative approach seeks to enhance the efficiency of AI in solving real-world problems and establish sustainable, environmentally friendly, and socially responsible infrastructure at scale.
The AI Platform Alliance invites AI companies that are developing hardware solutions and are dedicated to challenging the status quo to join the consortium. Interested companies can apply for membership through the alliance’s website.",https://venturebeat.com/wp-content/uploads/2023/10/ai-platform-alliance.jpg?w=1200&strip=all,2023-10-17 12:00:00
