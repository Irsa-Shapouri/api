Link,post_title,post_content,featured_image,Date-Publish,post_category,post_tag
https://techxplore.com/news/2024-03-large-language-simple-mechanism-knowledge.html,Large language models use a surprisingly simple mechanism to retrieve some stored knowledge,"Large language models (LLMs), such as the ones used in AI chatbots, are incredibly complex. Despite their widespread use in various fields, scientists still have a limited understanding of their inner workings. In an attempt to shed light on this, researchers from MIT and other institutions have investigated how these models retrieve stored knowledge.

Their findings revealed something surprising: LLMs often employ a simple linear function to decode and recover stored facts. These linear functions, which involve only two variables and no exponents, capture the direct relationship between these variables. What's more, the model uses the same decoding function for similar types of facts.

By identifying these linear functions for different facts, researchers can explore what the model knows about new subjects and where that knowledge is stored within the model. They developed a technique to estimate these functions and discovered that even when a model provides an incorrect answer, it often stores the correct information. This approach could be used in the future to identify and rectify falsehoods within a model, reducing the likelihood of incorrect or nonsensical responses.

While LLMs are typically complex and difficult to understand, there are instances where simple mechanisms operate within them. Evan Hernandez, an electrical engineering and computer science graduate student at MIT, emphasized the significance of this discovery. Hernandez, along with other researchers, authored a paper detailing these findings, which will be presented at the International Conference on Learning Representations (ICLR 2024).

Most large language models, like transformer models, are neural networks that consist of interconnected nodes or neurons. These networks encode and process data across multiple layers. The knowledge stored in a transformer can be represented as relations that connect subjects and objects. For example, the relation ""Miles Davis plays the trumpet"" connects the subject (Miles Davis) with the object (trumpet).

As a transformer acquires more knowledge, it stores additional facts about a particular subject across its layers. When a user queries the model about that subject, the model must decode the most relevant fact to provide a response. Researchers aimed to understand the mechanism responsible for this decoding process.

Through a series of experiments, they discovered that despite their complexity, LLMs use a simple linear function to decode relational information. Each function is specific to the type of fact being retrieved. For instance, the transformer might use one decoding function to output the instrument a person plays and a different function to output the state where a person was born.

The researchers developed a method to estimate these simple functions and computed functions for 47 different relations, such as ""capital city of a country"" and ""lead singer of a band."" While there are countless possible relations, the selected subset represents the types of facts that can be expressed in this manner.

To test the functions, the researchers changed the subject and observed if the correct object information could still be recovered. The results demonstrated the effectiveness of the linear decoding process for these relations.

Understanding how LLMs retrieve and decode stored facts is crucial for improving their accuracy and reducing the occurrence of incorrect responses. By uncovering these simple mechanisms within complex models, researchers can work towards optimizing their performance and enhancing their overall functionality.",https://scx2.b-cdn.net/gfx/news/hires/2024/large-language-models-2.jpg,2024-03-25 09:34:06,Innovation,Innovation
