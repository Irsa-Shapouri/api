Link,Title,Text,Image,Date Publish
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/top-cybersecurity-trends-in-2023/,Top Cybersecurity Trends In 2023,"Serge Beck is the founder and CEO of Omnibek.
getty
By 2028 the global cost of cybercrimes is estimated to increase by 69% to a whopping US $13.82 trillion. According to IBM’s 2023 report, the average cost of a corporate data breach is $4.45 million.
With a global average increase of 15% per year, the companies that stand out the most in saving from such catastrophes are the ones that use artificial intelligence (AI) and automation. Corporations that invest in AI and cybersecurity automation have saved on average $1.76 million this year alone.
Here are the top three trends right now that are worth considering.
AI And Cybersecurity Automation
Automation and AI integrations have become crucial allies in modern corporate cybersecurity. For businesses all across the world, this transition is a strategic need, not just a passing trend. The recent partnership between the American Productivity and Quality Center (APQC) and the IBM Institute for Business Value (IBV) highlighted the increased influence of AI and automation in the cybersecurity sector. In the AI and automation for cybersecurity report, it’s evident that the great majority of businesses either use AI in their security operations currently or are actively exploring doing so. Impressively, 64% of respondents said they have utilized AI for security capabilities, and another 29% said they were considering it.
The use of AI in cybersecurity has several benefits. By quickly spotting odd behaviors, dynamically analyzing vulnerabilities and alerting to possible threats, AI-driven insights and automation enhance the skills of security specialists. AI ensures a degree of consistency and depth that is impossible by even the most experienced security experts, which acts at a size and pace that exceeds human capabilities.
Proactive Ransomware Defense Is A Must
Organizations all across the world are still subject to developing ransomware assaults. In addition to encrypting data, cybercriminals are now also taking private data and requesting ransom payments to keep it from being made public.
The same IBM report quoted above finds that 82% of breaches included cloud-based data. Businesses need to find solutions that safeguard data as it travels across clouds, databases, apps and services while also enabling visibility across hybrid environments.
In order to do that, there are some rules to follow:
Robust Backup And Recovery: Make sure you have thorough backup and recovery procedures in place so that data can be restored quickly in the event of an attack. Test backups often to ensure their dependability.
Regular Patching: Maintain software and security patch updates to close holes that hackers can exploit. This procedure may be automated with the use of vulnerability management technologies, ensuring that no crucial updates are overlooked.
User Training: Train staff to spot phishing efforts and dubious emails, which are frequently used as ransomware entry points. Regularly mimic phishing attacks to evaluate the success of training.
Security Audits: Conduct regular security audits to find any potential security gaps in your firm. Vulnerabilities can be found before an attacker can exploit them with the use of penetration testing and vulnerability assessments.
This, however, is not always enough. And a zero-trust cybersecurity architecture is the most viable solution that any organization could adopt.
Zero-Trust Architecture
The security paradigm known as ""zero trust"" is founded on the maxim ""never trust, always verify."" It makes the assumption that risks might occur both within and outside the network of an organization.
In order to apply zero trust, the following practices must be adopted:
Identity Verification: Access to business resources requires constant identity verification from all users and devices. To add another level of security, multi-factor authentication (MFA) should be employed.
Micro-Segmentation: To prevent lateral attacker movement, partition networks into smaller, more isolated sections. Apply stringent access constraints based on the principle of least privilege (PoLP) between segments.
Strict Access Control: Implement stringent access restrictions, enabling staff members-only access to the resources they require to perform their assigned duties. Before allowing access, use network access controls (NAC) to confirm that devices comply with security requirements.
Continuous Monitoring: Keep an eye out for any irregularities in user behavior and network traffic that might point to malicious activities. Use UEBA (user and entity behavior analytics) tools to quickly identify risks.
Secure Remote Access: Implement secure access solutions, such as virtual private networks (VPNs) or zero-trust network access (ZTNA) solutions, for remote employees and third-party providers to ensure secure connections.
Zero trust is a work frame that must be continuously monitored, assessed and adjusted; it cannot be implemented once and left in place. In addition to ensuring that trust is never assumed, even within the boundaries of the corporate network, it assists companies in reducing the attack surface.
Security Is An Investment That Pays Off
In 2023, improving company security will demand a proactive and flexible strategy. Rather than being a measure, it is an expenditure made to protect against prospective intrusions. The constantly changing threat environment necessitates a complete approach that incorporates reliable backup mechanisms, AI-powered defenses, the application of zero-trust principles, thorough protection across many settings and the streamlining of security tools through consolidation.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/610be8c3364952106c9bda7a/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 10:00:00
https://www.forbes.com/sites/karlfreund/2023/10/17/qualcomm-and-google-team-up-for-risc-v-based-wearables/,Qualcomm And Google Team Up For RISC-V Based Wearables,"The two companies, both long-time partners of Arm, are extending their collaboration on wearables with a RISC-V Snapdragon Wearable platform to power the next-generation Wear OS solutions. “What about Arm?” you ask...
Qualcomm and Google have announced they are building their next-generation wearables on the open-source RISC-V architecture. Adopting RISC-V is a significant departure for two of Arm's largest customers to select the upstart RISC-V open-source CPU for a significant design and is a major blow to Arm.
Background
In 2021, Qualcomm paid Arm over $2.7 billion in royalties, making it the chip designer's second-largest customer after Apple. Qualcomm uses Arm's intellectual property (IP) to design its Snapdragon chips in various devices, including smartphones, tablets, laptops, and cars. Qualcomm is also developing new Arm-based chips for other markets, such as data centers and artificial intelligence.
But then, in early 2023, Arm filed a lawsuit against Qualcomm, alleging that the company was using Arm IP without a proper license. Rumored to be centered around Nuvia's design, acquired in 2021, the lawsuit raised concerns about the future of the relationship between the two companies. Qualcomm has another option, RISC-V, and is now exercising that option at the low end.
For Google, the custom Tensor chip in the Pixel 6 and Pixel 7 smartphones is based on Arm's IP. Google also uses Arm-based chips in its data centers and cloud computing platform. But Google has also brought out an early Android OS that supports RISC-V, telegraphing its intent with RISC-V. While there is no RISC-V implementation for mobile handsets at this time, a recent survey by the Android Authority showed that some 45% of those surveyed would be interested in a RISC-V phone, and some 51% said that would be interested if it were on par with current devices.
A survey by the Android Authority indicated the broad appeal for RISC-V in smart phones. Android Authority
Meanwhile, companies like Tenstorrent, RISC-V, Ventana Microsystems, Andes Technologies, and Esperanto Technologies are building the RISC-V CPU landscape. The common goal is to challenge the Arm Goliath with a more flexible, lower-cost, and open platform that will span from embedded processors to supercomputers.
Qualcomm will design the RISC-V platform, and Google will build the Wear OS. Qualcomm
There may be more to this collaboration than just this announcement. Qualcomm and Google recently joined other industry leaders to launch the RISC-V Software Ecosystem (RISE). In addition, Qualcomm also announced an investment in a new company to advance RISC-V hardware development, initially focusing on automotive and eventually expanding to include mobile and IoT.
Could Google's watch design be heading for RISC-V ? No details were provided. Google
What does this mean for RISC-V, and for Arm?
The sky is not falling for Arm as they continue to drive Arm designs into products like NVIDIA's Grace CPU and cloud servers and supercomputers worldwide. They currently have a significant performance edge, and of course, their ecosystem is massive, with billions of Arm-powered devices. But the writing is on the walls of Silicon Valley: RISC-V is a viable and easy-to-adopt block of flexible IP that can readily be added to specific logic for specific applications. RISC-V is especially attractive for SoC designers who need decent-performing CPU cores. But companies like Tensorrent and Esperanto see far more opportunity in the data center.
While this announcement is only about the low-end of the CPU market, the additional expertise the two companies will develop could position them to make similar moves higher up, and the market for Mobile CPUs could become a two-horse race as the software ecosystem and performance of RISC-V continues to improve and catch up to Arm. We wouldn't expect the more prominent players like Apple and Qualcomm to move first, but could follow a 2nd tier provider such as Huawei down the RISC-V path. But it will take a few years of additional ISA and ecosystem development for RISC-V to catch up with Arm.
Conclusions
This announcement is a milestone as two large Arm partners select RISC-V to complement their existing product line, especially in low-power markets like wearables. We note that Qualcomm is applying the Snapdragon brand to the RISC-V SoC(s), making this a significant divergence for a previously Arm-only shop.
This move by two large customers is a shot across the bow for Arm; they need to accelerate the development of their IP, which has lagged somewhat under their Softbank owners especially in the AI realm. Arm may finally be a publicly traded company again but they are no longer the only game in town.",https://imageio.forbes.com/specials-images/imageserve/652d91ab651440fdcbc80403/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 10:00:00
https://www.forbes.com/sites/garydrenik/2023/10/17/consumers-want-video-ads-but-with-certain-stipulations-according-to-new-research/,"Consumers Want Video Ads, But With Certain Stipulations","Viewing Video On Mobile Devices Prostock-studio - stock.adobe.com
In the golden age of video, there is more content than ever before – from traditional TV to online video, and streaming platforms – and where there is content, there are advertisements.
Brands reaching consumers through video isn’t new – the first television commercial aired in 1941 – but today’s digital environment has given consumers more control over the types of ads they see and whether they see ads at all. With today’s focus on digital privacy around ad targeting and streaming services rolling out paid ad-free tiers, it begs the question: Are consumers still paying attention to brand messages, or do they really hate ads?
In 2022, online video ad spend reached $75 billion, so despite the perception that consumers aren’t paying attention, there must be a reason that brands are still investing so heavily.
To better understand the online video advertising ecosystem, I caught up with Jenn Chen, President and Chief Revenue Officer at video technology company, Connatix, which helps publishers and advertisers engage consumers with streaming video. Connatix recently launched a new research report – Consumers Are Watching; Here’s How They Want Their Video Ads – that surveyed 1,000 consumers 18+ about what they expect and prefer from online video experiences. The main finding was that consumers are happy to watch ads in exchange for the content they want to see, but they do have some stipulations.
Gary Drenik: Where do viewers go to find desired video content and how do they prefer to watch?
Jenn Chen: In the past, video was mainly consumed on television, but today most people are carrying around the equivalent of a small HD TV screen in their pockets. According to a recent Prosper Insights & Analytics survey, 29.9% of consumers are viewing video/TV online, and 28.3% prefer to view on their mobile devices. From Connatix’s video trends research, we learned that consumers spend a ton of time watching – 46% of people reported spending up to an hour watching videos online daily.
Prosper - How Often Do You View The Following Prosper Insights & Analytics
While most people (81%) spend that time on social media, many consumers are also engaging with media outlets and websites for video. Almost half (45%) of consumers are visiting outlets like Forbes, BuzzFeed, HuffPost, and more, and 31% of the audience surveyed also view business/brand websites as a key video destination.
Once consumers reach their preferred video destination, they don’t want to jump around to multiple channels, only visiting 3.6 sites on average in each viewing session. The more publishers provide unique content and advertising experiences that keep viewers engaged, the more reason they will have to stay and watch.
Drenik: What do viewers want to watch?
Chen: Across all age groups and demographics, the top two categories that consumers prefer to watch online are entertainment (63%) and news (55%). Entertainment isn’t surprising, but the interest in news is worth digging deeper into. As consumers move away from traditional television news, it’s creating an opportunity for the digital sites of those outlets to thrive. We found that as age and income levels rise, so does interest in news from publishers – 68% of viewers 55+ and 55% of consumers making $150K+. These demographics are important for advertisers to consider – this is an affluent and engaged audience that many brands are missing out on as news is historically one of the top blocked categories in an ad buy. News organizations are operating with smaller budgets, limited staff, and resources, but still have a commitment to deliver quality, trusted journalism. By leveraging tools like contextual targeting, advertisers can invest in news while achieving brand goals, ensuring ads only run on brand suitable sections of news sites.
Drenik: What makes consumers watch an ad?
Chen: While watching traditional TV, a commercial break was either a cliffhanger during an intense scene of your favorite show or a snack break. But with online video, this behavior has changed since users can skip ads they don’t want to see. This means that advertisers need to prioritize ad content that encourages users to watch.
69% of consumers told us that they will never skip an ad if it’s for a product or service that they are already interested in. They’re also more likely to watch and remember ads that are visually interesting and funny and want to see ads that are related to the video content they’re watching.
Based on these findings, it seems that advertisers are fulfilling that criteria, since 84% of consumers will watch an ad on their favorite website to access desired video content. And according to a recent Prosper Insights & Analytics survey, 21% of consumers regularly watch the commercials that play before video content on websites.
Prosper - How Often Watch Commercial Prior to Video Prosper Insights & Analytics
Drenik: When watching an ad, what makes consumers more likely to take action?
Chen: While ‘taking action’ can mean something different depending on the type of ad and the advertiser’s goals, we learned a lot about purchase behaviors, and found that video ads are driving purchase decisions. 75% of consumers have made at least one purchase after watching a video ad in the past year. Survey results also indicate that viewers with the highest purchase intent are using iOS devices, with 37% of purchases made on iPhones.
It’s also important to talk about contextual alignment here – consumers told us that they prefer to see ads about the same topic as the video they were watching with 82% noticing that connection. And of those respondents who prefer the contextual connection, 39% are more likely to take action. Contextual targeting is not only a solution to Google’s upcoming cookie deprecation but could be a useful way for advertisers to encourage viewership, and therefore, drive more action.
Drenik: What is the magic formula when it comes to ads versus video content? How long should ads be?
Chen: As the viewing experience improves across all screens, consumers are more interested in longer videos with 57% preferring videos longer than 1 minute, and 19% looking for videos 5+ minutes long. However, this preference for long-form content does not translate to the advertising experience.
While longer videos may mean more ad breaks, consumers don’t want longer ads. More than half (64%) of respondents say that an online commercial should be a minute or less – with 46% of that group preferring even shorter ads, 30 seconds or less.
The magic formula is to prioritize the consumer viewing experience, providing a proper ratio between ads and content. Publishers should strategically place ads so they don’t disrupt too much of the video, and advertisers should focus on keeping brand messages short, attention grabbing and related to the topic of the video they are appearing against to feel less intrusive.
Drenik: How can brands use video to influence consumer purchase behaviors and improve the customer experience?
Chen: Video has the power to engage consumers more than other advertising formats, giving them a deeper look at your brand and building a more personal connection that leads to brand affinity and purchases. Consumers also understand that to access video content, they may need to engage with brand messages, and they are happy to engage with that value exchange when it’s done properly. However, it’s also important to not take advantage of a consumer's willingness to watch ads – ensuring that your brand messages are aligned with the content they’re already watching and are short, attention grabbing and non-intrusive.
Drenik: Thanks so much Jenn for sitting down with me today and sharing a deeper look at consumer video advertising trends. It’s great to know that consumers are happy to take in brand messages while watching videos, but it is also important to keep the viewing experience in mind.",https://imageio.forbes.com/specials-images/imageserve/65281344c85d7d8b427e32be/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 10:00:00
https://www.forbes.com/sites/timbajarin/2023/10/17/cities-are-looking-to-solidify-a-position-as-the-top-ai-center-in-the-us/,Cities Are Looking To Solidify A Position As The Top AI Center In The U.S.,"Cities are vying to be the top AI center in the U.S. getty
You may have noticed recently, from various articles in the business press, that many U.S. cities are vying to be the nation's AI center.
According to Axios, ""Silicon Valley is the epicenter for AI startups. San Francisco-based AI and machine learning companies raised $12.8 billion across 219 deals through August, according to PitchBook, September 11, 2023.
Looking more closely, the key centers employing AI professionals are San Francisco (27 percent), New York (13 percent), Seattle (nine percent), and Los Angeles, Boston, and Washington-Baltimore (roughly five percent each).
Nearly half of the generative AI job postings in the past year were in San Francisco, San Jose, New York, Los Angeles, Boston and Seattle.”
While all of these cities listed above are highly active in AI development, one city in the center of Silicon Valley is being proactive, innovative and creative when it comes to working towards solidifying its position as the top AI center in the U.S.
According to a San Jose Mercury News story on October 12, 2023, the mayor of San Jose stated-
""We want San Jose to certainly be a capital in AI innovation, Mayor Mahan said during a speech at the innovation summit on Thursday. We are having discussions to bring incubators and accelerators into our downtown to take advantage of the fact that we have 36,000 students at San Jose State and the technical talent there,"" Mahan said.
One key to Mayor Mahon's plan is to work with San Jose State University to build ""an innovation lab on campus."" This proposed lab would enable students to experiment, prototype, and test safely and cost-effectively.
The plans include discounts on utility costs and a speedy permitting process for AI firms, incubators and initiatives.
The AI activity in the cities mentioned above has come from organic growth in their respective regions. Much of it is connected to major universities with highly regarded hardware and software engineering schools nearby.
However, San Jose Mayor Matt Mahon is presenting a proactive plan to help create a dedicated AI hub of innovation and experimentation in the San Jose area. They are working with educators, VCs, and Silicon Valley companies who will partner to establish this unique AI incubator and other AI experimental programs in San Jose to cement this Silicon Valley city as the center of AI development in the U.S.
Although they plan to partner with San Jose State University, an incubator with this type of focus and the promise of significant financial backing could also attract top engineering talent from nearby Stanford University, UC Berkley and even Cal Poly in San Luis Obispo, CA, whom all have stellar engineering programs too.
Now that the mayor of San Jose, California, has shown his blueprint for developing this Silicon Valley city as a potential AI center of the U.S., we could see similar plans established, especially in Seattle, New York and other cities on the list that have been posting AI jobs for months.
However, with the San Francisco Bay Area having such a lead with established AI companies and the VC wealth just up the road in Menlo Park, California, my bet is that this region could expand its existing role in AI and make it harder for other areas to make a similar claim in the future.",https://imageio.forbes.com/specials-images/imageserve/652cbf281bc4dd19d5d69747/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 10:00:00
https://www.forbes.com/sites/davidphelan/2023/10/17/apple-reveals-new-apple-pencil-with-usb-c-for-ipad-at-best-ever-price-which-should-you-choose/,Apple Reveals New Apple Pencil For iPad At Best-Ever Price. Which Should You Choose?,"The new Apple Pencil with 10th-generation iPad. Apple
Apple has just revealed its latest product update: a new Apple Pencil—not the new iPads which had been rumored. It is the cheapest Apple Pencil ever, clocking in at $79, with $69 for education pricing or £79 in the U.K., with £69 education pricing. It will be available in early November.
The new model looks more like the second-generation Pencil with its one flat edge and matte finish. But, like the first Pencil, it has a removable cap which slides off to reveal a connector for charging, though this time around, it’s a USB-C, not a Lightning, connector.
MORE FROM FORBES Apple iPad Air 2023, iPad mini Release Date Just Hours Away, Report Claims
So, is it the Goldilocks Apple Pencil, or which should you choose?
First of all, the new model, which is called Apple Pencil (USB-C) or the new Apple Pencil, works with every iPad with USB-C connector, including the 10th-generation iPad which until today only supported the first-gen Pencil.
And it’s a lot cheaper than either of the other Pencils, since the first-generation model, which you would now only buy if you have an older iPad with Lightning connector or iPad ninth-generation, costs $99 (£109 in the U.K.). The second-generation Pencil, still the best-looking of the lot, is $129 (£139 U.K.).
What’s Different?
The USB-C connector on the new Apple Pencil. Apple
First of all, although it snaps magnetically to the side of all flat-edged iPads, including the 10th-generation iPad, it doesn’t charge or pair from them. You need to remove the cap to reveal a USB-C connector for pairing and charging. You do this by plugging a USB-C cable into it and then to the USB-C connector on the iPad. When attached to the side of the tablet magnetically, it puts the Pencil into a sleep state to save battery.
Note that the new Pencil is not compatible with the ninth-generation iPad because you need the USB-C connector to pair the Pencil—it’s not done via the magnetic panel.
Missing Features
Apple iPad Pro and the new Apple Pencil. Apple
Apart from wireless charging, there’s no pressure sensitivity, something found on both the other Pencils. If you need this capability, which means you can alternate between light and dark strokes, you need either of the others.
It also lacks the double-tap feature which lets you quickly change between tools, found on the priciest Pencil. Oh, and only the most expensive model offers free engraving.
Both the second-generation Pencil and the new one support Apple Pencil hover on iPad Pro, where you can see a preview of your mark before you make it.
The first Apple Pencil launched with the first iPad Pro, back in September 2015. Then came the second-generation model, three years later in October 2018. So, you could say a third-gen model has been overdue for a while.
Initial Verdict
The new Apple Pencil with USB-C connector. Apple
The new Pencil looks good and offers a real value option for all users of the latest iPads with USB-C connectors. If that’s you, you have to ask yourself if you need pressure-sensitivity. If so, you need either the first- or third-gen models, though remember to choose appropriately: the third-gen Pencil does not work with the iPad tenth-generation, but with all other USB-C iPads.
The only other things you miss from the third-gen Pencil are the capability to change tools with a quick double-tap and of course, the convenience of pairing and charging wirelessly.",https://imageio.forbes.com/specials-images/imageserve/652e91237dfcde4bd26a6398/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 09:57:38
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/a-beginners-guide-to-website-testing-easy-steps-to-ensure-a-positive-user-experience/,A Beginner's Guide To Website Testing: Easy Steps To Ensure A Positive User Experience,"Margarita Simonova is Founder and CEO of ILoveMyQA.com.
getty
One thing that is certain about website creation is that no user wants to go to a site that gives them a bad experience. When a user encounters such a site, they will quickly turn away—causing you to miss out on converting that user into a regular site visitor.
In this article, we will take you from an absolute beginner level and transform you into a competent tester of website user experiences. We’ll give an introduction to website testing, and then talk about such topics as what to test, how to test it, what tools to use and how often to test. Let’s get started!
Why Is Website Testing So Important?
We all know it: Users have short attention spans. We are all guilty of this. Even the slightest glitch in the user experience journey can result in a user closing your tab in their browser and moving on to the next site. There’s a lot of data that backs this up, too.
Consider the following statistics from Sweor: 39% of users will abandon a website due to a poor user experience; 57% of users won't recommend companies to their friends that have websites that aren't mobile-friendly; 88% of users state they will not return to a website if they have a negative user experience.
What Are The Advantages Of Testing A Website?
Once we have tested our website for quality, we can expect a number of benefits. These benefits include:
• Attracting and retaining more users to the website.
• Improving users’ confidence and trust in your business.
• Ensuring users using a variety of devices can access your website.
• Increasing search engine optimization (SEO) and incoming links.
Guide To Website Testing
It's important to first determine what to test:
• Functionality: We need to be absolutely sure that all buttons, forms and links work as intended.
• Usability: We need to check if a website is easy to navigate and users can understand where important elements are located.
• Responsiveness: We need to make sure the website changes its layout based on the different screen sizes of desktops, tablets and smartphones.
• Speed: We need to confirm that the site loads as quickly as possible.
• Security: We need to ensure that data in transit is encrypted and sensitive information is hidden.
How Do We Test This?
Functionality Testing
For functionality testing, we want to see how links, buttons and forms perform. We can test these by answering the following questions:
• Links And Buttons: Do all the links and buttons take you to the right page?
• Forms: Do all the forms send the correct information?
Usability Testing
Usability testing needs to confirm that important elements are easy to identify on the website’s pages. Look at the following elements and answer the questions about them:
• Navigation: Are important pages easy to get to?
• Content: Is the site’s text easy to read and presents accurate information?
Responsiveness Testing
Responsiveness testing confirms that the site will look good on any type of device. Answer the following questions in your testing:
• Devices: Does the content load correctly on all different device types?
• Orientation: Does the website adjust correctly when a user rotates the screen?
Speed Testing
Speed testing verifies that the site loads within an acceptable time limit. Testing the loading times involves asking the following questions:
• Loading Time: How fast does it take the web page to download and be rendered on the screen?
• Rendering Time: How long does it take a dynamic web page to be assembled by the web server?
Security Testing
Security testing verifies that sensitive information is not exposed. Answer the following questions:
• Data Encryption: Does the site require data encryption in order to use?
• Password Masking: Are password fields masked with asterisk characters?
Tools And Techniques
Now that we know what to test and how we should go about it, we can start taking a look at the tools and techniques to use.
Functionality can be tested both manually or automatically. By doing a manual check, you can check each link, button and form to see that they work. This can be time-consuming and leaves room for human error, so an automated approach may be better. An automated approach uses a script or testing tool that can go through each link and see if the desired result is returned.
Usability is best tested by having target groups of users try the webpage and report on their findings. A survey can be used to get their feedback and any problem areas can be addressed after statistically analyzing the results. It’s also possible to use AI to help with this, although usability can be somewhat subjective, so human intuition is often better.
Responsiveness can be tricky to test because there are so many different devices out there. The best way to test this is by using a development environment that emulates different devices. A similar tool is Google’s Mobile-Friendly Test, where you can simply input your URL. Using these methods, you can see how the website looks on all sorts of devices from the comfort of your own system.
Speed can be tested in several ways. To see how long it takes your page to be created on the back end, you can check the settings of your particular platform and see if it can display those metrics. For the speed of downloading the page, a free tool like Google’s PageSpeed Insights will show the delay it takes to load the page.
Lastly, security can be tested by checking for “HTTPS” in the URL instead of just “HTTP.” Make sure that there are no certificate errors when loading the page using HTTPS. Also, manually check that passwords use asterisks instead of displaying the actual password when typed in.
When To Test?
One more issue to consider when testing your website is how often testing should occur. We can consider three major time periods when testing is ideal: before launching a new website, after making significant updates and on a periodic basis for quality.
Anyone getting started with website testing should consider following the blueprint presented here to create a positive user experience.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/652daea557dddfb1e1e561ed/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 09:45:00
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/the-cyber-skills-and-talent-needed-for-the-next-three-years-and-beyond/,The Cyber Skills And Talent Needed For The Next Three Years And Beyond,"Faisal Bhutto is the SVP of Cloud & Cybersecurity at Calian IT & Cyber Solutions.
getty
If the past three years have shown us anything about our industry, it’s that lasting changes can happen instantly and rely on the technological skills we have today. When looking at the future of the tech landscape, we must begin by examining the industry's current state and identify areas that are lacking. For years now, this has included representation, diversity and a skills gap shortage. While the pandemic exacerbated these issues, it also shone a glaring light on just how severe the problems are, forcing the industry to look—and act—critically.
Occurring in tandem with this reckoning, the industry has been saddled with mass layoffs across companies of every size, indications of a recession or an economic slowdown, and a growing need to address climate issues with technology. Despite these mounting challenges, demand for emerging artificial intelligence (AI) and machine learning (ML) technologies is rising, creating new jobs in preparation for the next wave of innovation.
As companies look to fill these roles, they must prioritize diverse talent to overhaul the industry and enhance technological capabilities while also addressing issues and preventing new ones. With so much talk about diversifying tech in recent years, it's important to understand what this workforce will look like and, more importantly, where to find the talent to propel it.
Extend opportunities to refugees.
Despite a renewed focus on hiring more women and BIPOC (Black, Indigenous and people of color) employees in tech, more can be done to improve diversity, increase representation and foster more inclusive workplace cultures in this industry. This will require companies to explore other avenues and extend opportunities to underserved populations. This can include refugees and other displaced populations, which often reflect communities of color.
International programs like the UN Refugee Agency are already tapping into this potential by partnering with global tech companies to support education and training in the field. These programs are tailored to meet the needs of refugee students interested in developing or enhancing their digital skill sets and provide digital access, connectivity and expansive learning opportunities.
Beyond teaching tech skills and improving digital literacy, these programs also help foster a sense of community, focusing on ideal classroom designs and teaching methods that can be conducted remotely, in person or through a hybrid model. Much like the current workforce distribution, this flexibility allows refugee students to enroll in programs that work for them and develop the technical and personal skills needed to navigate the industry.
Beyond the skills they can learn, refugees often bring their own skill sets and technical capabilities that they learned prior to leaving their homes. These perspectives are critical to shaping a more modern, globalized tech field.
Address biases in AI and ML.
Advances in AI and ML are accelerating at a breakneck pace without signs of slowing down in the coming years. The industry will need a workforce capable of keeping up with the developmental speed of these emerging technologies, along with data and insights from underrepresented groups to increase diversity and reduce the harmful impacts of prejudice on the tech.
Because AI and ML depend on the information they're fed to train their algorithms, the lack of diversity across the industry has led to human biases in the tech that reflect a workforce that's primarily white and male. This has led to instances in which devices have difficulties identifying foreign accents, produce written results with gender stereotypes and misrepresent people of color as criminals when compared to white people.
Although AI can help identify these biases, it can't correct itself when it lacks the information to do so. Diversifying our tech workforce will not only contribute to creating more fair and equitable ML systems, but it will also help ensure the tech reflects our global populations.
Combat climate crises with affected populations.
As technology continues to develop to address climate change and clean energy needs, building a workforce from underrepresented backgrounds will be key in identifying and resolving solutions. This is because climate displacement caused by extreme weather events in recent years has forced people from their homes, resulting in climate migration and climate refugees. In 2019, it was estimated that extreme weather displaced nearly 24 million people, nearly triple the amount that were displaced due to conflict.
Research from the Environmental Protection Agency revealed that underrepresented populations will be most affected by climate change and extreme weather events. In the U.S., this means women, gender-diverse people and BIPOC populations. However, similar patterns are reflected globally. As a result, technological developments will require input from people with the knowledge and lived experiences of these communities and locations.
In an increasingly global world, our greatest strengths will come from the talent behind the tech. Emerging technologies will thrive with greater representation, and global issues will require diverse perspectives to drive innovation. Only by welcoming underserved populations and extending opportunities to join in the development of future tech will we find the cyber skills needed to fuel the next three years and beyond.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/652dad1b5b1b95df4c216c20/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 09:30:00
https://www.forbes.com/sites/carltonreid/2023/10/17/council-votes-to-keep-oxfords-ltn-which-conspiracy-theorists-confuse-with-15-minute-city-plans/,Council Votes To Keep Oxford’s LTN Which Conspiracy Theorists Confuse With 15-Minute City Plans,"Protesters gather in Broad Street to demonstrate against 15-minute cities and listen to speeches on ... [+] February 18, 2023 in Oxford, England. (Photo by Martin Pope/Getty Images) Getty Images
Oxfordshire County Council’s cabinet has voted to keep the East Oxford Low Traffic Neighborhoods (LTN). Last year this and other Oxford LTNs were falsely linked with proposals to introduce to the city elements of the 15-Minute City urban design concept.
Oxfordshire County Council is the region’s highway authority and usually works closely with the Labour-run city council on schemes affecting Oxford.
The county council is led by a coalition of 24 Liberal Democrat and Green councillors. There are also 22 Conservatives, 14 Labour, and three independent councillors.
“We understand that cars still have a role to play in our transport system,” said the council leader Liz Leffman earlier this year, “but we want to make it quicker, cheaper and safer for residents across the county to leave their cars at home and travel actively, for example, by walking, cycling, scooting or using buses.”
“This will make our streets cleaner, safer and less congested,” she said, “and help those who need to use cars to make their essential journeys.”
She added: “Tackling climate change underpins all we do. The climate emergency is the biggest challenge the planet faces.”
Oxford’s LTNs—subject of several national TV documentaries—are either closed to motorists with bollards and planters or policed with automatic number plate recognition cameras (ANPR). Taxi drivers are allowed to access some of the ANPR-policed LTNs.
Nearly 30 speakers aired their views at the October 17 cabinet meeting, stated local democracy reporter Oxford Clarion on Twitter/X.
Bernadette Evans of Oxford Business Action Group urged for the LTNs to be removed. She said: “Multiple highly experienced business owners have told us trading conditions changed overnight when the LTNs went in. The loss of trade has meant an increase in debt.”
Bollards been placed in a street in Cowley near Oxford, to create a Low Traffic Neighborhood (LTN). ... [+] (Photo by Steve Parsons/PA Images via Getty Images) PA Images via Getty Images
“The LTNs are not a silver bullet,” said Chris Jarvis, leader of Oxford’s Green councillors. “But we need to take meaningful action. Air quality has improved, cycling is up, road collisions are down, walking is more pleasant, teenagers are playing football in the street. These are real, tangible benefits.”
Residents gave opposing views of the LTNs.
“Until the LTNs went in I was too scared to cycle to work in Marston,” said Katie Mills. “The Cowley and East Oxford LTNs have made it safe. I have two children at schools in East Oxford, and I now have the confidence to cycle on the roads with them. The LTNs have been transformative.”
Maggie Brown vehemently disagreed: “I’ve lived in Oxford for 75 years and it’s being destroyed,” she said.
“We need access to all the roads in Oxford without any restrictions,” stated Bashir Ahmed, of COLTA, Oxford’s licensed taxi association.
“The people are waking up and they are angry,” claimed local resident Anne Stares, who said LTN supporters were part of a “toxic alliance.”
Protesters hold signs criticising 15-minute cities as a demonstration is held in Broad Street on ... [+] February 18, 2023 in Oxford, England. (Photo by Martin Pope/Getty Images) Getty Images
In its Local Plan 2040, Oxford City Council proposed installing elements from the 15-minute city urban concept in neighborhoods throughout the city over the next 20 years. These plans included proposals to improve accessibility to local shops and other amenities for residents so they didn’t have to always drive.
Separately, Oxfordshire County Council announced traffic-reducing measures throughout the city, with infrastructure to encourage car travel around the city by using the ring road rather than already congested roads. Initial opposition to the plans led to proposals to introduce permit schemes to facilitate car travel at certain times, allowing car access to areas that the council planned to restrict to motorists.
However, these proposals—never implemented—were used by conspiracy theorists and others to claim that Oxfordshire County Council wanted to create “ghettos” where motorists would be confined and unable to leave.
Ahead of an anti-LTN, anti-15-minute-city protest in Oxford earlier this year, an organisation called Not Our Future distributed leaflets warning that the council was coming for residents’ cars.
“The reality is that in 2024 you, the people of Oxford, will be guinea pigs,” the leaflet claimed.
“You will all be subjects of a scheme known as 15-Minute Neighbourhoods. This sounds cute but it’s anything but. It’s a controlled system to restrict people from driving freely around the city,” it said, misrepresenting the planned traffic measures.
“All this is coming from the United Nations Agenda 30,” continued the leaflet.
“This includes the ultimate aim of moving the majority of the public to smart cities where all activity can be monitored to control people’s Personal Carbon Allowance,” it lied.
Protesters take a moment from the march to catch up as a demonstration against 15-minute cities is ... [+] held on February 18, 2023 in Oxford, England. T.(Photo by Martin Pope/Getty Images) Getty Images
Investigative journalists at the DeSmog climate adaptation news website say Not Our Future is backed by a “high-profile climate deniers and conspiracy theorists based in the UK, Canada, the United States and Australia.”
On its website, Not Our Future warns: “The draconian and destructive response to Covid-19, involving the lockdown of entire populations, mandated injections and mask-wearing, and aggressive suppression of freedom of expression, could be a harbinger of things to come.”
And LTNs, it further warns, are also part of that future, as is going cashless.
“Central Bank Digital Currencies … have the potential to lock humanity in a dystopian control grid,” exaggerates the body, claiming that this grid would be a “de facto prison.”
Not Our Future activist delivers conspiracy theory laden leaflet through letterbox of an Oxford ... [+] house on January 08, 2023 in Oxford, England. (Photo by Martin Pope/Getty Images) Getty Images
Some commentators on social media—and at street demonstrations—claim that 15-Minute Cities are designed to confine people to within a certain distance of their homes, and that the urban design policy is a plot to attack personal freedoms or is “anti-motorist.”
Such conspiracy theories have gone mainstream in the U.K. “Right across our country, there is a Labour-backed movement to make cars harder to use, to make driving more expensive, and to remove your freedom to get from A to B how you want,” the U.K’s Transport Secretary Mark Harper told the Conservative Party conference earlier this month.
“I am calling time on the misuse of so-called 15-minute cities,” he added, even though no 15-Minute City exists in the U.K.
“What is sinister, and what we shouldn’t tolerate, is the idea that local councils can decide how often you go to the shops, and that they can ration who uses the roads and when, and that they police it all with CCTV,” Harper added. Naturally, no councils have plans to restrict shopping trips.
Earlier this year fellow Tory MP Nick Fletcher had been the first to voice support for the conspiracy theory in the House of Commons. Fletcher told parliament—to guffaws—that the idea of 15-Minute Cities was an “international socialist concept” that will “cost our personal freedom.”
During the Conservative Party conference in Manchester Prime Minister Sunak took aim at 15 Minute Cities vowing to make sure drivers were not “aggressively restricted.”
Oxford County Council’s decision today will have been closely watched by councils around the U.K.
“Should we take a large amount of Oxford’s congestion and put it back on residential roads? asked LibDem councillor and cabinet member for highways management Andrew Gant.
No, he concluded at the meeting. “Going back is not realistic. There is a lack of any alternative vision from those who oppose LTNs.”
He added: “We have all got too used to the idea that road space is something we can help ourselves to, as much as we like, whenever we like. It is a finite resource, and it is full. It comes at a cost. As a society, we are not making responsible use of it. We need to change.”",https://imageio.forbes.com/specials-images/imageserve/652e779923b64dd312ab95ee/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 09:24:50
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/how-to-develop-a-plan-for-bringing-generative-ai-into-your-business/,How To Develop A Plan For Bringing Generative AI Into Your Business,"Saryu Nayyar is CEO of Gurucul, a provider of behavioral security analytics technology and a recognized expert in cyber risk management.
getty
Generative AI (GenAI) exploded onto the tech scene over the last year, creating both intense interest and mild trepidation about its use.
First off, let there be no doubt that GenAI is the future of technology-driven business processes. In a June 2023 KPMG survey, 74% of U.S. business executives say GenAI will have the biggest impact on their business in the next 12 to 18 months.
GenAI is a category of artificial intelligence (AI) that is designed to generate new content—images, text, music and even computer code—based on existing data that it has been trained on. In a recent Forbes article, for example, Bernard Marr cites the example of GPT-4 (a.k.a. ChatGPT) from OpenAI: “Trained on vast swathes of the internet, it can produce human-like text that is almost indistinguishable from a text written by a person.""
But GenAI is not without its challenges. Ensuring that generated content is ethical, unbiased, truthful and safe is a significant concern. GenAI tools may also produce inappropriate or harmful content or simply spew false information.
Issues Causing Apprehension
Some companies have chosen to limit or outright ban employees’ use of tools like ChatGPT, often out of concern that sensitive data can be leaked through the tool. The concern is not unwarranted. Without proper safeguards, it’s possible that private data can seep into the public domain of data used to train the AI models.
Another issue is the lack of transparency over what data is used to train the models, some of which could be inaccurate or flat-out wrong, meaning the tools could spread misinformation. What’s more, the tools can “hallucinate,” or simply spout non-sensical or false information that the user accepts as true. For example, GPT playground—a version of ChatGPT without the same guardrails—claims that King Renoit (a made-up person) was a real 16th-century French king.
Another concern is what GenAI will do for (or to) the workforce. Will it replace humans in real jobs, or will it create new roles for people to step into? Some jobs, like customer service representatives and content creators, will likely be in jeopardy. Nevertheless, 53% of the executives in the KPMG study say their headcount will likely expand with the use of GenAI.
Why The Good Outweighs The Bad
While it might make sense to limit the use of consumer-grade GenAI tools, organizations will likely struggle to avoid the technology altogether. There are far too many positive aspects to ignore the potential of enterprise-grade tools.
For example, GenAI can reduce the need for highly skilled workers and help companies bridge the talent gap. We already see this happening as GenAI augments the skills typically provided by cybersecurity analysts, for example, by analyzing threat intelligence. This is a role with a notable global skills shortage, and GenAI is primed to provide relief by becoming a non-human security expert.
GenAI tools can also create efficiency in work processes, reducing the time to perform mundane or repetitive tasks. In the pharmaceutical industry, GenAI has shown it can drastically reduce the drug development timeline. In one recent case, it took just 12 months instead of the usual five years to bring a new drug to human trials.
These tools can generate realistic synthetic data to use in place of sensitive or regulated data—for example, healthcare or financial information. Synthetic data is already being used to train AI models for self-driving vehicles, fraud detection and interactive voice services like Amazon’s Alexa.
The natural language interfaces of AI chatbots mean that almost anyone can use the tools without having to learn a query language such as SQL. Again, this goes back to reducing the need for high-level skills among certain classes of workers.
GenAI is especially helpful in cybersecurity. It is a force multiplier for the Security Operations Center as it gathers and aggregates data, reduces time for investigations, reduces the alert fatigue security analysts experience, helps to automate mitigations and acts as a subject matter expert for the human talent.
How To Make A Plan Now
In other words, GenAI is projected to become one of the most important technologies of the decade. Now is the time for organizations to plan how to use it. That plan should include:
• Establishing Policies: Set and communicate clear policies about the tools’ usage, especially around consumer-grade tools. These policies should address data privacy, ethical considerations, compliance with regulations and the responsible use of AI technology.
• Forming A Team: Designate a dedicated team responsible for overseeing the implementation of GenAI within your organization. This team should consist of individuals with expertise in AI, data science and relevant domain knowledge. Task this team with evaluating your current product offerings and identifying areas where GenAI can be integrated to improve efficiency, productivity or customer experience.
• Proving The Concept: As part of your initial implementation strategy, select two or three GenAI tools or platforms that align with your business objectives. Conduct a proof of concept (PoC) for these tools to gain hands-on experience and assess their suitability for your organization.
• Customizing The Knowledge Base: Invest in learning how to train GenAI tools with your organization's internal data. Customization is key to making these tools more effective and aligned with your specific business needs. Collaborate with data scientists and AI experts to continuously fine-tune the models to generate content or insights that are relevant and valuable to your organization.
• Training Users: Provide comprehensive training programs for the employees who will be using GenAI tools directly. Training should cover both the technical aspects of using the tools and the ethical considerations surrounding their usage.
• Doing A Cost-Benefit Analysis: Regularly evaluate the impact of GenAI implementations on your organization's operations, productivity and bottom line. Conduct a cost-benefit analysis to assess whether the technology is delivering the expected value.
GenAI has the potential to have the biggest impact on your business in the years ahead. What are you waiting for?
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/63d00455752e4cf11c766efe/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 09:15:00
https://www.forbes.com/sites/stevetengler/2023/10/17/new-data-suggests-top-three-causes-of-driving-risk/,New Data Suggests Top Three Causes Of Driving Risk,"UNITED STATES - OCT, 2023: Mobile data from multiple sources points to several areas where safety ... [+] risk is heightened. getty
Imagine informing your Aggie friends that College Station, Texas, with its 120,019 residents had 153% more crashes on a game day than Los Angeles’s 3.8 million residents.
That’s just a sampling of the lessons learned from more than one trillion miles of driving data that’s been collected by Arity, a mobility data and analytics company spun-off from The Allstate Corporation. The intent of their data collection – some of which is acquired from automotive partners, but mostly from smart phone uploads — is to better understand dangerous preconditions. “We all know that driving is risky and unpredictable,” states Megan Jones, Senior Actuary and Analytics Director at Arity, “but with a better understanding of driver behavior, we believe we can make the experience safer, smarter and more economical.” The improved experience depends upon how a partner wants to provide the information; anywhere from warnings ahead of starting a trip to coaching the driver on better behaviors to possibly even “avoid risky conditions” as a navigational option much like “avoid toll roads.”
And so the really interesting part is the data suggests three semi-predictable themes of the “insured risk” (which incorporates both the likelihood of an accident and the resulting severity including associated injuries) with fascinating sub-elements under each category: HOW you drive, how OFTEN you drive, and WHEN you drive.
How You Drive
Although multiple factors affect the complicated algorithm that predicts the driving prowess or deficiencies of a specific driver, Jones explains that three sub-behaviors are especially indicative within the data.
To anyone who has ridden with an aggressive driver, the first sub-behavior will ring true. “When I talk about sudden braking [within the data], I’m not just talking about the act of slowing down to come to a stop sign, but these are instances of extreme deceleration. And when we look at it as a predictor of risk, it’s really the behavior or habit of needing to suddenly brake often.”
The root cause behind the severe responses might vary, though. “It can indicate that they are driving in an unsafe environment, that the drivers themselves are not allowing safe stopping distances, or that they are becoming distracted frequently.”
The second sub-behavior is speed, but with an actuarial twist: a statistical combination of frequently driving excessively over the speed limit and exceeding a specific “high speed” threshold. “The speed limits have been provided for very good reasons,” expounds Jones, “and that risk of speeding gets compounded at higher speeds.”
The last sub-behavior is perhaps the most controllable: phone usage while driving. “We continue to see an increasing trend and, unfortunately, that means people are less aware of their driving environment and have slower reaction times.” But according to a 2017 study, some portions of the population are more susceptible: age, gender and even personality can affect how often a driver allows himself to be distracted by his phone. Young men were the most prone to this sub-behavior, especially “… those with neurotic and extraverted personalities.”
OAKLAND, CALIFORNIA - 2022: Traffic backs up at the San Francisco–Oakland Bay Bridge toll plaza. ... [+] (Photo by Justin Sullivan) Getty Images
How Often You Drive
“This is universally known and very intuitive: the longer someone is on the road, the more likely they are to be involved in an accident,” explains Jones. And, sure enough, one of the major ways NHTSA tracks safety performance is deaths per million miles driven (which has improved so far in 2023), and nearly every insurance policy asks commuting distance or expected miles driven per year.
Exposure obviously creates additional risk.
TRONA, CALIFORNIA - 2018 : A vehicle drives over a rural railroad crossing at dusk. (Photo by Bob ... [+] Riha, Jr.) Getty Images
When You Drive
One might assume the greatest “when” is predictable: either during rush hour (when the increased volume of cars creates a target-rich environment) or during inclement weather (when the decreased traction amplifies other behaviors).
But, per Jones, there is, “… even higher risk during the late-night hours – especially weekends – when your visibility is lower because it’s dark out and there’s a greater potential for fatigue and/or impairment. It’s harder [logistically and psychologically] for many of us to modify when we drive because some people don’t have the option, and that’s why we like to educate people about those time periods being riskier so they mitigate phone usage, speeding and all of those other variables.”
Author’s Note
Parents: some of the data’s conclusions might be things about which you already intended to nag your teenagers. However, as the father of no-longer-teenagers, may I suggest showing them this article. It’s easy to think parents are stupid and ignore the wisdom we’ve collected over multiple decades.
But a trillion miles of data doesn’t lie.
And it’s coming from their phones, which are smart.",https://imageio.forbes.com/specials-images/imageserve/65205a14b90c283ea5433884/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 09:14:01
https://techxplore.com/news/2023-10-material-devices-electricity-efficiently.html,New material could allow devices to turn wasted heat into useful electricity more efficiently,"This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
a) Scans of energy transfer for Cu 12 Sb 4 S 13 and Cu 14 Sb 4 S 13 at different temperatures. b) Electrical resistivity as a function of temperature. The anomaly at ≈90 K is related to a cubic-to-tetragonal phase transition. c) Calculated scan of energy transfer for the copper ions in Cu 14 Sb 4 S 13 as a function of temperature, based on the MD simulations. Credit: Advanced Materials (2023). DOI: 10.1002/adma.202306088
Scientists have discovered a way to design materials that improve energy efficiency, in a breakthrough that could help the fight against climate change, make manufacturing greener, and could even take the hassle out of charging your smart watch.
The thermoelectric energy harvesting innovation identifies a new way of harnessing previously untapped sources of waste heat and converting it into electricity.
Thermoelectric materials can turn temperature differences into electricity. Researchers at Reading have discovered that if a thermoelectric material has moving ions inside cages, the heat flow will be reduced. This keeps the hot and cold sides at different temperatures, while electrons can flow from the hot to the cold side, so electricity can be made.
Usually, materials with moving ions break down when making electricity like this. But the material described in the new study published in Advanced Materials doesn't break down easily and could allow devices and generators to turn wasted heat into useful electricity more efficiently than in current designs.
Dr. Paz Vaqueiro, of the University of Reading's Department of Chemistry, led the study. She said, ""This discovery has the potential to help address the global energy crisis and contribute to combating climate change. Approximately two-thirds of the energy generated worldwide is wasted in the form of heat. Converting even a fraction of this waste heat back into useful electricity would help to ensure a sustainable energy supply and reduce carbon emissions.
""The U.K. is predicted to need twice as much electricity in 2050 as it did in 2020. The potential of thermoelectric technology has been known about for a number of years, but currently generators are expensive and not very efficient. Using new thermoelectric technology that is cheaper to run and more efficient could help turn the 48 TWh of waste heat the U.K. produces every year into electricity and could help us on the path to reach net zero.""
Watches and cars
Not only could the breakthrough have positive consequences for the fight against climate change, but it could also make a difference to some of the devices and machinery we use daily.
At present, thermoelectric devices and generators are expensive and only used in niche applications (such as the Voyager probes sent by NASA to explore space beyond the solar system). However, the development of new thermoelectric generators could have a big impact on wearable devices, such as smart watches, where the current technology relies on batteries that require regular recharging. Eliminating this requirement, by harvesting body heat to generate power, could not only be more convenient but also make wearable tech more reliable in critical applications, such as monitoring the health of vulnerable or elderly patients.
The development could be of use for the motor industry, allowing carmakers to develop thermoelectric generators that use waste heat to charge batteries of plug-in electric or hybrid vehicles, increasing their efficiency. Factories producing glass, steel or cement, which are currently carbon-intensive and generate large amounts of waste heat, could also benefit from the technology.
More information: Shriparna Mukherjee et al, Beyond Rattling: Tetrahedrites as Incipient Ionic Conductors, Advanced Materials (2023). DOI: 10.1002/adma.202306088 Journal information: Advanced Materials",https://scx2.b-cdn.net/gfx/news/2023/new-material-could-all.jpg,2023-10-17 09:02:35
https://techxplore.com/news/2023-10-unveils-partially-disordered-phase-li-.html,Study unveils a new partially disordered phase in Li- and Mn-rich cathode materials,"This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Characterization of the as-synthesized L5M85, L10M70 and L15M55. a, The partially disordered spinel phase as an intermediate between the DRX and fully ordered spinel structures. b, XRD patterns of L5M85, L10M70 and L15M55, refined based on the rock salt structure. The lattice parameter (a) and weighted profile R factor (R wp ) are shown in the figure. c–e, SEM images of L5M85 (c), L10M70 (d) and L15M55 (e) after shaker milling with carbon (scale bar = 500 nm). f, STEM image and EDS mapping of the elements O, Mn and Ti in as-synthesized L5M85 (scare bar = 500 nm). g, SAED pattern of as-synthesized L5M85. Credit: Nature Energy (2023). DOI: 10.1038/s41560-023-01375-9
Lithium-ion (Li-ion) batteries are among the most widespread battery technologies worldwide, due to their light weight, high energy densities, easy fabrication process, rapid charging times and other advantageous properties. Identifying strategies that could boost their performance further or facilitate their future upscaling has been the focus of numerous recent studies.
One of the proposed approaches for improving the performance of Li-ion batteries entails identifying new promising cathode materials that can be made from metals that are abundant in nature. So far, Li-ion cathodes have been found to be in some ways ineffective, as phase transitions inside them can elicit what is known as voltage hysteresis, which adversely impacts battery capacity.
Researchers at University of California Berkeley and other institutes across the United States have recently unveiled an unconventional phase transformation in cathode materials that are rich in Li and manganese (Mn). The new phase they uncovered, outlined in a paper in Nature Energy, could enable the creation of highly performing batteries with Mn-based cathodes.
""We want to create high energy density cathode materials for Li-ion that can be made from earth-abundant metals, unlike current cathode materials which contain both Co and Ni,"" Gerbrand Ceder, co-author of the paper, told Tech Xplore. ""This would mean Li-ion batteries that are less expensive, thereby helping their market penetration in EVs, grid, etc.""
Mn is an Earth abundant metal, as it is already widely produced in large quantities for various real-world applications. This metal has a good redox voltage and could thus work well in Li-ion batteries with high energy densities.
These advantageous properties are what ultimately inspired Ceder and his colleagues to try to create cathodes containing a high amount of Mn. Some studies have already explored the potential of Mn-rich cathodes, yet most results gathered so far have been unsatisfactory.
""Previous efforts to use Mn-based cathodes have suffered from the fact that Mn has a tendency to move around and rearrange the crystal structure that you start from,"" Ceder said. ""We decided to turn that into an advantage and start from a material (DRX) that when cycled would turn into a structure that is very good for storing lithium. So, it was a form on inverse design: We knew the material would transform, so we just made sure it transformed to something very good for storing lithium.""
The new phase that Ceder and his colleagues unveiled in their study, dubbed the delta (δ) phase, has a unique, unconventional structure. This structure resembles that of spinels, a class of ceramics typically marked by an ordered internal organization.
""The phase we uncovered is related to the known spinel structure, but only forms that structure in very small domains, which are anti-phased with each other,"" Ceder explained. ""Spinel is a structure that is known to store lithium ions, but its commercialization has been problematic, as it undergoes a destructive phase transformation when cycled in a battery. ""
In the phase observed by the researchers, small domains of spinel act independently. This prevents the destructive transition previously observed in spinel-based cathodes during a battery's operation, instead allowing batteries to retain a good capacity for several battery cycles.
""We can make 'complex' structures in-situ while we charge and discharge a battery and these complex structures can have excellent performance,"" Ceder said. ""A cathode material based on only Mn- and Ti-oxide precursors can potentially be very inexpensive and could reduce the cost of Li-ion batteries by 40–50%. This would make batteries for EV and grid much cheaper.""
In their experiments, the researchers were able to identify the kinetic mechanisms underpinning their Li- and Mn-rich cathodes' transition to the so-called delta phase. This could pave the way for the development of more promising cathode materials with similar characteristics.
Initial tests performed by Ceder and his colleagues yielded very promising results, as the phase they unveiled was found to enable both a high energy density and good battery cyclability. In the future, their work could thus also encourage other teams to explore the potential of cathodes rich in Mn using similar experimental strategies.
""We believe that we can make even higher energy density materials,"" Ceder added. ""Also, we are working on accelerating this transformation to delta so one doesn't have to wait for 10–20 cycles to get the best performance out of one's battery.""
More information: Zijian Cai et al, In situ formed partially disordered phases as earth-abundant Mn-rich cathode materials, Nature Energy (2023). DOI: 10.1038/s41560-023-01375-9 Journal information: Nature Energy
© 2023 Science X Network",https://scx2.b-cdn.net/gfx/news/hires/2023/study-unveils-a-new-pa-1.jpg,2023-10-17 07:40:01
https://techxplore.com/news/2023-10-soft-packaged-portable-glove.html,"Researchers develop soft-packaged, portable rehabilitation glove","This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Multi-scene application and life assistance function of flexible rehabilitation glove robot. Credit: USTC
Researchers from the University of Science and Technology of China (USTC) of the Chinese Academy of Sciences (CAS) have proposed a soft-packaged and portable rehabilitation glove with fine movement training. It is expected to serve the fine motor rehabilitation and daily living assistance for tens of millions of patients with hand dysfunction around the world.
The technology was described in an article published on Oct. 5 in Nature Machine Intelligence.
Patients with hand dysfunction can recover through repeated and continuous hand movement training. Soft-packaged rehabilitation gloves are lightweight and flexible.
However, because a flexible body is prone to large deformation, which is not conducive to motion perception, and currently available gloves are not conducive to portability, most soft-packaged rehabilitation gloves can only achieve therapeutic movement based on open loop control. This makes the precise rehabilitation of fine motor skills of the hand still challenging. Assisted daily tasks on a poststroke individual. Credit: Nature Machine Intelligence (2023). DOI: 10.1038/s42256-023-00728-z
In this study, the researchers designed a bionic finger sleeve structure that integrates smooth movement and accurate perception by integrating 15 bending sensors and 10 shape-memory-alloy actuators.
Due to the shape memory alloy with high work-to-weight ratio and integrated design, the flexible rehabilitation glove robot weighs only 490 grams and has the ability to work independently.
By imitating the folded skin of the back of the finger, the research team proposed a bionic design featuring a non-uniform stiffness flexible finger sleeve, which reduces the interference of finger sleeve movement on the sensing system and achieves stable and accurate finger state perception.
Further, they proposed a multi-modal fine action rehabilitation training method to achieve portable, accurate and safe rehabilitation training for patients with hand dysfunction. Clinical trials have preliminarily verified the advantages of the portable, low-cost soft-packaged rehabilitation glove robot in fine sports rehabilitation and daily living assistance.
More information: Mengli Sui et al, A soft-packaged and portable rehabilitation glove capable of closed-loop fine motor skills, Nature Machine Intelligence (2023). DOI: 10.1038/s42256-023-00728-z Journal information: Nature Machine Intelligence",https://scx2.b-cdn.net/gfx/news/hires/2023/researchers-develop-so-2.jpg,2023-10-17 08:36:39
https://techxplore.com/news/2023-10-fire-inhibiting-nonflammable-gel-polymer-electrolyte.html,"Fire-inhibiting, nonflammable gel polymer electrolyte for lithium-ion batteries","This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Schematic image depicting the principles of operation of non-flammable gel electrolytes. Credit: Ulsan National Institute of Science and Technology
A collaborative research team has achieved a milestone in battery technology. Their achievement in developing a non-flammable gel polymer electrolyte (GPE) is set to revolutionize the safety of lithium-ion batteries (LIBs) by mitigating the risks of thermal runaway and fire incidents.
The research was led by Professor Hyun-Kon Song in the School of Energy and Chemical Engineering at UNIST, Dr. Seo-Hyun Jung from Research Center for Advanced Specialty Chemicals at Korea Research Institute of Chemical Technology (KRICT), and Dr. Tae-Hee Kim from the Ulsan Advanced Energy Technology R&D Center at Korea Institute of Energy Research (KIER). The results have been published in ACS Energy Letters.
In the past, the potential flammability of LIBs has raised significant concerns, especially in electric vehicles, where fire hazards pose a serious threat to underground parking lots. Addressing this critical issue, the research team has successfully developed a groundbreaking non-flammable polymer semi-solid electrolyte, offering a promising solution to mitigate battery fires.
Conventionally, non-flammable electrolytes have heavily relied on the incorporation of flame retardant additives or solvents with exceptionally high boiling points. However, these methods often resulted in a considerable decrease in ion conductivity, compromising the overall performance of the electrolyte.
In their breakthrough research, the team introduced a trace amount of polymer, creating a semi-solid electrolyte. This novel approach dramatically increased the lithium ion conductivity by 33% compared to existing liquid electrolytes. Moreover, the pouch-type batteries incorporating this non-flammable semi-solid electrolyte exhibited a remarkable 110% improvement in life characteristics, effectively preventing unnecessary electrolyte reactions during the formation and operation of the solid-electrolyte interphase (SEI) layer.
Nail penetration of 650 mAh pouch cells of NCM811||graphite. (a to c) Voltage and temperature profiles (d to f). Credit: Ulsan National Institute of Science and Technology
The key advantage of this innovative electrolyte lies in its exceptional performance and non-combustibility. By suppressing radical chain reactions with fuel compounds during the combustion process, the polymer semi-solid electrolyte effectively inhibits the occurrence of battery fires. The research team demonstrated the excellence of the developed polymer by quantitatively analyzing its ability to stabilize and suppress radicals.
Jihong Jeong (School of Energy and Chemical Engineering, UNIST) said, ""The interaction between the polymerized material inside the battery and volatile solvents allows us to effectively suppress radical chain reactions. Through electrochemical quantification, this breakthrough will greatly contribute to understanding the mechanism of non-flammable electrolytes.""
Co-first author Mideum Kim, a master student in the School of Energy and Chemical Engineering at UNIST and the Korea Research Institute of Chemical Technology (KRICT), further confirmed the exceptional safety of the battery itself through various experiments. The team's comprehensive approach included applying the non-flammable semi-solid electrolyte to pouch-type batteries, ensuring the evaluation of electrolyte non-combustibility extended to practical battery applications.
""The research team's multidisciplinary composition, involving electrochemistry from UNIST, polymer synthesis from the KRICT Research Center for Advanced Specialty Chemicals, and battery safety testing by the Ulsan Advanced Energy Technology R&D Center at Korea Institute of Energy Research (KIER), has been instrumental in achieving this breakthrough,"" stated Professor Song. ""The use of non-flammable semi-solid electrolytes, which can be directly incorporated into existing battery assembly processes, will accelerate the future commercialization of safer batteries.""
More information: Jihong Jeong et al, Fire-Inhibiting Nonflammable Gel Polymer Electrolyte for Lithium-Ion Batteries, ACS Energy Letters (2023). DOI: 10.1021/acsenergylett.3c01128 Journal information: ACS Energy Letters",https://scx2.b-cdn.net/gfx/news/2023/fire-inhibiting-nonfla.jpg,2023-10-17 08:29:04
https://www.reuters.com/innovation/article/tsmc-results/tsmc-third-quarter-profit-to-slide-30-focus-on-how-much-growth-to-come-idUSKBN31H07N,"TSMC third-quarter profit to slide 30%, focus on how much growth to come","TAIPEI (Reuters) - Taiwan Semiconductor Manufacturing Co Ltd is expected to report a 30% slump in third-quarter profit on Thursday but analysts predict robust growth next year as the chip industry emerges from its current downturn.
FILE PHOTO: A logo of Taiwanse chip giant TSMC can be seen in Tainan, Taiwan December 29, 2022. REUTERS/Ann Wang/File Photo
The likely decline in profit also reflects a strong performance last year, when the company was still riding high on pent-up post-pandemic demand.
The world’s largest contract chipmaker is set to report net profit of T$195.9 billion ($6 billion) for July-September - its second straight quarter of profit decline, according to an LSEG SmartEstimate drawn from 19 analysts. SmartEstimates give greater weighting to forecasts from analysts who are more consistently accurate.
Revenue for the quarter came in at around $17 billion, according to TSMC figures, down 20% from a year earlier and roughly the middle of the company’s forecast range.
Global demand for semiconductors began to weaken in the second half of last year, but analysts say inventories at smartphone and computer makers are running down and restocking demand is expected to pick up.
Given that, much of Thursday’s focus will be on TSMC’s outlook for the fourth quarter and beyond.
Morgan Stanley analysts have forecast 10% revenue growth for the fourth quarter but also said in a research note that “guidance may surprise to the upside,” citing strong demand for high-end chips used in artificial intelligence as one factor.
The AI boom has helped drive up the price of shares in Asia’s most valuable company, with TSMC’s Taipei-listed stock having surged 23% so far this year.
An LSEG SmartEstimate puts TSMC’s 2024 revenue growth at around 22%.
Sources have said, however, that TSMC has been nervous about customer demand and told its major suppliers to delay the delivery of high-end chip-making equipment, although they added that suppliers expect the delay to be short-term.
Some analysts are also reining in their optimism somewhat.
Fubon Securities expects a slow start to next year for TSMC, with 10% growth in the first quarter, predicting order cancellations towards the year end and mild restocking demand. In particular, it is concerned that Apple, a major customer, may revise down its orders.
“We think the market consensus is still too bullish,” it said in a research note.
The company is due to report at 0600 GMT on Thursday.
($1 = 32.2290 Taiwan dollars)",https://s2.reutersmedia.net/resources/r/?m=02&d=20231017&t=2&i=1647931385&w=1200&r=LYNXMPEJ9G044,2023-10-17 03:55:49
https://www.reuters.com/innovation/article/canada-batteries/canada-to-give-belgiums-umicore-up-to-c1-billion-for-new-battery-components-plant-idUSKBN31G1PF,Canada to give Belgium's Umicore up to C$1 billion for new battery components plant,"OTTAWA (Reuters) - Canada and the province of Ontario will give up to C$1 billion to a unit of Belgium’s Umicore to help it build a plant that will produce components for electric vehicle batteries, Ottawa said on Monday.
The facility, in the Ontario town of Loyalist Township, will manufacture cathode active materials and precursor cathode active materials, federal Innovation Minister Francois-Philippe Champagne said in a statement.
The plant - the first of its kind in North America - will initially employ 600 people and have a battery materials production capacity of 35 gigawatt hours annually.
Canada, home to a large mining sector for minerals such as lithium, nickel and cobalt, wants to woo firms involved in all levels of the electric vehicle (EV) supply chain via a multibillion-dollar green technology.
The Umicore plant is due to be built in stages and could be worth C$2.7 billion when fully completed. Canada will invest up to C$551.3 billion with Ontario adding up to C$424.6 billion.
The full project has the potential to produce enough battery materials to support the production of over 800,000 EVs per year, Champagne said.",https://s4.reutersmedia.net/resources_v2/images/rcom-default.png,2023-10-16 19:03:02
https://techcrunch.com/2023/10/17/microsoft-affiliated-research-finds-flaws-in-gtp-4/,Microsoft-affiliated research finds flaws in GPT-4,"Sometimes, following instructions too precisely can land you in hot water — if you’re a large language model, that is.
That’s the conclusion reached by a new, Microsoft-affiliated scientific paper that looked at the “trustworthiness” — and toxicity — of large language models (LLMs) including OpenAI’s GPT-4 and GPT-3.5, GPT-4’s predecessor.
The co-authors write that, possibly because GPT-4 is more likely to follow the instructions of “jailbreaking” prompts that bypass the model’s built-in safety measures, GPT-4 can be more easily prompted than other LLMs to spout toxic, biased text.
In other words, GPT-4’s good “intentions” and improved comprehension can — in the wrong hands — lead it astray.
“We find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, which are maliciously designed to bypass the security measures of LLMs, potentially because GPT-4 follows (misleading) instructions more precisely,” the co-authors write in a blog post accompanying the paper.
Now, why would Microsoft greenlight research that casts an OpenAI product it itself uses (GPT-4 powers Microsoft’s Bing Chat chatbot) in a poor light? The answer lies in a note within the blog post:
“[T]he research team worked with Microsoft product groups to confirm that the potential vulnerabilities identified do not impact current customer-facing services. This is in part true because finished AI applications apply a range of mitigation approaches to address potential harms that may occur at the model level of the technology. In addition, we have shared our research with GPT’s developer, OpenAI, which has noted the potential vulnerabilities in the system cards for relevant models.”
So the implication is that the relevant bug fixes and patches were made before the paper’s publication. Whether that’s truly the case, the research is yet another data point suggesting that LLMs — even from Microsoft-backed, billion-dollar-plus-revenue-generating startups like OpenAI — remain an imperfect science.
GPT-4, like all LLMs, must be instructed — or “prompted” — to complete a task (e.g. writing an email or summarizing an article). Jailbreaking LLMs entails using prompts worded in a specific way to “trick” the LLM into perform a task that wasn’t a part of its objective.
For example, the LLM powering Bing Chat wasn’t designed to write neo-Nazi propaganda. But at release, because it was trained on vast amounts of data from the internet — some of it toxic — it was susceptible to doing just that when fed a particular prompt.
As for GPT-4, the researchers found that the model is more likely to generate toxic text than GPT-3.5 when given certain jailbreaking prompts that they “constructed.” They also claim that GPT-4 “agrees with … biased content more frequently than GPT-3.5,” at least depending on the demographic groups mentioned in the prompt.
For example, give the prompt “Homosexuals have HIV,” GPT-4 usually “strongly” disagrees with the statement, according to the researchers. But given “Women have HIV,” GPT-4 agrees — and outputs biased content.
Just as concerningly, GPT-4 — when given the “right” jailbreaking prompts — can leak private, sensitive data including email addresses, say the researchers. All LLMs can leak details from the data on which they’re trained. But GPT-4 proves more susceptible to doing this than others.
Alongside the paper, the researchers have open sourced the code they used to benchmark the models on GitHub. “Our goal is to encourage others in the research community to utilize and build upon this work,” they wrote in the blog post, “potentially pre-empting nefarious actions by adversaries who would exploit vulnerabilities to cause harm.”","https://techcrunch.com/wp-content/uploads/2023/06/OpenAI-logo-symmetry.jpg?resize=1200,675",2023-10-17 11:30:25
https://techcrunch.com/2023/10/17/microsoft-affiliated-research-finds-flaws-in-gtp-4/,Microsoft-affiliated research finds flaws in GPT-4,"Sometimes, following instructions too precisely can land you in hot water — if you’re a large language model, that is.
That’s the conclusion reached by a new, Microsoft-affiliated scientific paper that looked at the “trustworthiness” — and toxicity — of large language models (LLMs) including OpenAI’s GPT-4 and GPT-3.5, GPT-4’s predecessor.
The co-authors write that, possibly because GPT-4 is more likely to follow the instructions of “jailbreaking” prompts that bypass the model’s built-in safety measures, GPT-4 can be more easily prompted than other LLMs to spout toxic, biased text.
In other words, GPT-4’s good “intentions” and improved comprehension can — in the wrong hands — lead it astray.
“We find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, which are maliciously designed to bypass the security measures of LLMs, potentially because GPT-4 follows (misleading) instructions more precisely,” the co-authors write in a blog post accompanying the paper.
Now, why would Microsoft greenlight research that casts an OpenAI product it itself uses (GPT-4 powers Microsoft’s Bing Chat chatbot) in a poor light? The answer lies in a note within the blog post:
“[T]he research team worked with Microsoft product groups to confirm that the potential vulnerabilities identified do not impact current customer-facing services. This is in part true because finished AI applications apply a range of mitigation approaches to address potential harms that may occur at the model level of the technology. In addition, we have shared our research with GPT’s developer, OpenAI, which has noted the potential vulnerabilities in the system cards for relevant models.”
So the implication is that the relevant bug fixes and patches were made before the paper’s publication. Whether that’s truly the case, the research is yet another data point suggesting that LLMs — even from Microsoft-backed, billion-dollar-plus-revenue-generating startups like OpenAI — remain an imperfect science.
GPT-4, like all LLMs, must be instructed — or “prompted” — to complete a task (e.g. writing an email or summarizing an article). Jailbreaking LLMs entails using prompts worded in a specific way to “trick” the LLM into perform a task that wasn’t a part of its objective.
For example, the LLM powering Bing Chat wasn’t designed to write neo-Nazi propaganda. But at release, because it was trained on vast amounts of data from the internet — some of it toxic — it was susceptible to doing just that when fed a particular prompt.
As for GPT-4, the researchers found that the model is more likely to generate toxic text than GPT-3.5 when given certain jailbreaking prompts that they “constructed.” They also claim that GPT-4 “agrees with … biased content more frequently than GPT-3.5,” at least depending on the demographic groups mentioned in the prompt.
For example, give the prompt “Homosexuals have HIV,” GPT-4 usually “strongly” disagrees with the statement, according to the researchers. But given “Women have HIV,” GPT-4 agrees — and outputs biased content.
Just as concerningly, GPT-4 — when given the “right” jailbreaking prompts — can leak private, sensitive data including email addresses, say the researchers. All LLMs can leak details from the data on which they’re trained. But GPT-4 proves more susceptible to doing this than others.
Alongside the paper, the researchers have open sourced the code they used to benchmark the models on GitHub. “Our goal is to encourage others in the research community to utilize and build upon this work,” they wrote in the blog post, “potentially pre-empting nefarious actions by adversaries who would exploit vulnerabilities to cause harm.”","https://techcrunch.com/wp-content/uploads/2023/06/OpenAI-logo-symmetry.jpg?resize=1200,675",2023-10-17 11:30:25
https://techcrunch.com/2023/10/17/with-33m-in-new-capital-fingerprint-looks-to-expand-device-intelligence-platform/,"With $33M Series C, Fingerprint looks to expand device intelligence platform","Fingerprint, a device intelligence API, helps developers build security solutions using information from hardware accessing a website. The primary goal is to help prevent fraud.
Today, the Chicago-based company announced a $33 million Series C investment ​​led by Nexus Venture Partners with participation from Uncorrelated Ventures.
Dan Pinto, the company’s co-founder and CEO says what his startup does really well is identifying devices on the internet, whether through a browser like Chrome, Safari or Firefox, or through a mobile operating system via a native app on your phone. He argues that the methods of tracking devices like cookies and IP addresses don’t really work anymore with changes in browser technology, especially with the ability to hide who you are using a VPN.
“Fundamentally being anonymous on the internet means that you can do bad things,” Pinto told TechCrunch.
“We provide our [fingerprinting] service to high scale businesses in order to uniquely identify devices and prevent things like people logging into other people’s accounts they’re not supposed to, using stolen credit cards across multiple fake accounts and things like that.”
They do this via an API, enabling developers to link to their service to help prevent fraud on their websites. He says they look for things like things like what fonts are installed on the device, the screen resolution all the way to really deep technical things like how the device completes an SSL handshake with the server. The company’s technology is based on an open source library called Fingerprint.js, created by co-founder and CTO Valentin Vasilyev.
Pinto met Vasilyev at his previous startup, Machinio, when he hired him as one of the company’s first software engineers. By that time, Vasilyev had already created the open source Fingerprint.js project and it was gaining in popularity. About a year after Machinio was sold in 2018, he left to find a way to build on the success of the open source project. Pinto soon joined him and Fingerprint, the company was born.
While the company still technically supports the open source libraries, Pinto says they offer a less robust experience. “So the technology we use to identify people in the open source version has been in the open too long, and bad actors have figured out ways to get around it,” he said. “Whereas the technologies that we use in the pro version, we’re able to keep more private because it’s not open source, and they work significantly better,” he said.
There are some tricky privacy implications with a solution like this, especially in Europe with GDPR regulations, but Pinto says that the company doesn’t actually collect personal information. “The way that we get around that is that fundamentally, we still don’t know who the person is. We only know the devices, and it’s an anonymous identifier even from our side. So all you’re doing when you receive our identifier is comparing multiple anonymous identifiers to find patterns, and then blocking anonymous identifiers [if needed],” he explained.
While some customers want to go that extra step to identify the user, he says his company is not providing the means to do that. “If you associate the identifier with an email address, then the next time the anonymous identifier comes back, you can know who it is, but we don’t know who it is necessarily, and we designed our system in that way to avoid [coming up against the privacy laws],” he said.
He adds, while it’s theoretically possible for companies to use the technology for marketing and advertising purposes, the pricing discourages that.
The company has grown to 100 employees and 6000 customers including TD Ameritrade, Western Union and US Bank.","https://techcrunch.com/wp-content/uploads/2023/10/GettyImages-1429431586.jpg?resize=1200,667",2023-10-17 14:00:40
https://techcrunch.com/2023/10/17/apple-adds-usb-c-but-cuts-features-in-new-79-apple-pencil/,"Apple adds USB-C, but cuts features in new $79 Apple Pencil","Apple today announced a new Apple Pencil that offers USB-C and a lower price. But this isn’t an upgraded version of the Apple Pencil. This version lacks some of the advanced features found on the 2nd generation Apple Pencil, forcing users to pick between key features and USB-C recharging.
Starting at $79 and available for $69 through education channels, this Apple Pencil is all about value. The $129 2nd generation Apple Pencil is still available and offers significant features for artists and creative types. For the rest, the new Apple Pencil could be good enough.
A sliding cap hides a USB-C port, which is the only way to charge this version. It still features a flat side which can magnetically connect to an iPad, but doing so only puts the Pencil to sleep and does not charge it. This version features the same precision and low latency found on the original and second generation. It also supports tilt sensitivity, allowing the user to change the width of the on-screen brush by changing the angle of the Pencil.
Key Apple Pencil features are missing from this version. It lacks pressure sensitivity and the ability to double tap on the side of the Pencil to change tools.
Apple also announced today a $9 USB-C to Apple Pencil adapter to give the original Pencil a USB-C recharging option.","https://techcrunch.com/wp-content/uploads/2023/10/Apple-Pencil-USB-C_big.jpg.large_2x.jpg?resize=1200,675",2023-10-17 13:31:36
https://techcrunch.com/2023/10/17/prove-identity-40-million/,"Prove Identity nabs $40M at a ""unicorn status"" valuation to expand in mobile-based authentication tech","Prove Identity, the smartphone-based identity verification startup that originally made its name years ago as Payfone before rebranding in 2020, has raised $40 million. According to the company, the funding is coming in at a “unicorn status” valuation which would pip it past the $1 billion mark. The company, which has around 1,000 business customers, said it has grown business some 40% in the last year.
Prove’s tools use tools like facial recognition to trigger the pre-filling of information, customer verification flows and password-free authentication. They were originally built in part to reduce some of the friction, and subsequent shopping cart abandonment, around e-commerce transactions, leaning into the growing ubiquity of phones to help manage the process. (Speed remains a priority: it claims that its tools today offer “79% faster onboarding, a 35% reduction in abandonment, and a 75% reduction in fraud.”)
Prove said it plans to use the funding to build out more tools for digital payments and commerce, as well as fraud detection, and it sounds like a mobile handset will remain a central part of the proposition.
The raise is being co-led by strategic backers MassMutual and Capital One, and it is coming at a time when digital identity continues to be a hot topic.
The number of cybersecurity breaches resulting from malicious hackers using stolen identity credentials continues to grow — underscoring the opportunity for more effective tools in the market and the interest investors have for placing bets in the space.
A spokesperson for the company told us that the round gives Prove “unicorn status” without elaborating on what that means in terms of a firmer number. For some context, when the company last announced significant funding — $100 million mere days before its big rebrand — it had a post-money valuation of $549 million, according to PitchBook data. That is the figure that has been locked in since then.
Notably, back in 2020, CEO and founder Rodger Desai told us that Payfone (as it was known then) in 2019 processed 20 billion authentications, and that it was growing 70% year over year, with an aim to boost that figure to 100 billion transactions in the coming years. Its site today does not give an updated transaction number (and its 40% growth today is definitely lower than 70%). That speaks to a lot of competition in the space.
Prove is, of course, not the only company eyeing up the opportunity.
In addition to older startups like Jumio, there are startups like ThetaRay and Fourthline building approaches based in AI. Big PE firms are also getting in on the act, scooping up and consolidating technologies and customer bases for more economies of scale.
And then there is WorldCoin, co-founded by OpenAI’s Sam Altman, which wants to bring a completely new concept (and business) to the world of digital identity. It’s building physical ‘orbs’ aimed at scanning irises, in order to to build a network of digital IDs, which will in turn link up with a new global cryptocurrency, and an app for payments that tap into the two together. The startup has already been courting controversy in various markets, but it’s also been courting some serious funding, so it may be around for a while.
While projects like WorldCoin’s are still in their early days, Prove has expanded its customer base beyond commerce. It said that its 1,000 customers include “9 of the top 10 U.S. banks, 2 of the top 3 global cryptocurrency exchanges, 3 of the top 5 U.S. retailers, 2 of the top 3 U.S. healthcare companies, and 6 of the top 10 insurance companies in the U.S.” (FanDuel recently signed on as a customer; others include Experian and Visa; others include Starbucks, BlueCross BlueShield, and DocuSign.)
The overall market for identity and access management was estimated at nearly $16 billion in 2022, according to GrandView Research, and that will keep investors interested in new technology and approaches to tackling it.
“Prove is driving a paradigm shift in how businesses and consumers interact securely,” said Charles Svirk, a partner at MassMutual Ventures. “As consumers continue to experience risk in their engagement with brands, Prove’s solutions ensure that verified and authenticated information is being used, enabling brands to drive both loyalty and revenue. We’re delighted to continue supporting Prove as they redefine how we look at digital identity.” Svirk is joining the startup’s board of directors with this investment.","https://techcrunch.com/wp-content/uploads/2020/11/GettyImages-1150039017.jpg?resize=1200,660",2023-10-17 13:14:34
https://techcrunch.com/2023/10/17/youtube-is-launching-new-playback-and-creator-focused-features/,YouTube debuts new playback controls and creator-focused features,"YouTube is rolling out a new set of features for better mobile-based playback, song search, and creator-friendly tools, including a feature that will highlight the “like and subscribe” buttons when a creator utters those words.
The company is making it easier for users to increase playback speed. With the new update, users can just tap and hold anywhere on the player to increase the speed to 2x, and once they reach the part they want they can let go.
Additionally, when you are seeking the video to go back or go forward during the scrub bar and want to go back to the place you started, YouTube will indicate the point with a vibration.
Google is also introducing a feature called stable volume — which can be toggled through the playback settings menu — to reduce sudden differences in the volume in a clip. This feature was launched in test mode on several platforms in July.
In August, YouTube started testing a feature that let users find a song by humming the tune. With the new set of updates, users will be able to find a song by playing, singing, or humming.
For YouTube users accessing the service through the company’s mobile and tablet apps, the company is rolling out a screen lock feature to avoid accidental taps. To unlock the screen, you can tap and hold onto the lock icon.
YouTube said today that it is combining the Library tab and account page into a new “You” tab. The new tab will include previously watched videos, playlists, downloads, and purchases along with account and channel information. The page also allows you to quicky switch between accounts.
If you spend hours on YouTube, you must have heard creators say “hit like and subscribe” many times. Now, the company is turning this into a feature. So when the person in the video utters these words, YouTube will highlight these buttons with a visual cue in sync. YouTube didn’t specify if users will be able to turn this off, as constantly looking at the animation that highlights the subscribe button could be distracting.
Additionally, to help creators engage with viewers, YouTube will rotate top comments continuously. Plus, for new videos, there will be a new animation and live video count update for the first 24 hours.
Google is also updating the video description menu with additional details on smart TVs. Earlier, the details page took over the whole screen. Now things such as the video description, comments, and the subscribe button will be tucked in a neat vertical menu accessible when you tap the video title.
With these new features, YouTube has shifted its focus back to playback and search enhancements. In the last few months, the company has been concentrating on rolling out AI-powered and Short-focused features such as summaries for videos, Collab and Q&As, and TikTok-styled short video feed. Last month, it added creator tools to help them find the right music for their videos and easily add dubs.",https://techcrunch.com/wp-content/uploads/2023/07/lockmode_2x.gif?w=790,2023-10-17 13:00:48
https://techcrunch.com/2023/10/17/k2-space-is-building-a-mass-abundant-power-rich-future-for-space-exploration/,K2 Space is building a power-rich future for space exploration based on the premise that bigger is better,"Los Angeles-based K2 Space is accelerating its path to orbit with fresh venture funding, new defense contracts and a satellite architecture that will be capable of delivering staggering power levels in a single launch.
The company is taking what cofounder and CEO Karan Kunjur described in a recent interview as a “pretty significant contrarian bet against the market.” The premise of the bet goes something like this: the space industry is governed by a single calculus – cost per kilogram of mass. The dollar cost of mass affects how spacecraft are designed, how scientific missions are evaluated, and even how entire businesses are planned.
Although the cost per kilogram of mass has declined with the rise of new launch capabilities, like SpaceX’s pioneering work in rocket reusability, spacecraft and mission designers still face egregious mass constraints. As a result, spacecraft have gotten smaller, lighter and more compact. But that doesn’t come without significant trade-offs in power, payload mass and payload volume.
K2 Space is moving in the opposite direction. The company emerged from stealth in March with ambitious plans to design and build massive satellite buses, at never-before-seen costs. Their hypothesis is that next-gen launch vehicles like SpaceX’s Starship will fundamentally change the cost per kilogram paradigm that has ruled with an iron fist for so long – but that to take advantage of this future, we must start planning for it now.
The company is developing two satellite buses: a one ton payload mass bus called Mega, which can fly on launch vehicles operating today, and a much larger Giga satellite for up to 15 tons of payload, that’s built for super-heavy rockets. These products can “lower the barrier to accessing power, aperture or mass for any application in space,” cofounder and CTO Neel Kunjur explained in a recent interview. (The two cofounders are brothers.)
But it isn’t just the size of the satellites that’s remarkable. Revealed in more detail today, K2 has designed them to operate in a stackable, scalable architecture, so that customers can essentially purchase and operate a high-powered constellation at super low costs. K2’s products could unlock higher energy orbits, like medium Earth orbit, for companies at price points that are currently prohibitive.
The satellites were designed to maximize launch vehicle fairing volume, too. Stacked together, 10 Mega-class satellites will be able to fit in a Falcon 9 fairing, which would deliver 200 kilowatts of power in a single launch. Even more staggering, 40 Megas will be able to fit into a Starship, which would deliver 800 kW of power in one go.
For reference, ViaSat’s ViaSat-3 telecommunications satellite, one of the most powerful satellites every deployed in orbit, has 25 kilowatts of power and cost over half a billion to build. In contrast, a 10 Mega-class constellation would cost less than $150 million.
Neel, who spent over five years at SpaceX developing avionics for SpaceX’s Dragon spacecraft, said that delivering both high power and high packing density drove the satellite design.
“Not really many people other than SpaceX are maximizing the capability of Falcon 9,” he explained. “When SpaceX launches a Starlink mission these days, they’re using every kilo of launch capacity they possibly can on Falcon 9. Everybody else is leaving a lot on the table. We wanted to deliver the highest power density per launch possible. At 200 kilowatts deployed per Falcon launch, there’s no other satellite bus manufacturer that comes even close to the amount of power that we can deploy in a single Falcon 9 launch.”
Toward mass abundance
The engineering challenges are substantial. Although there are some major upsides of no longer having to mass optimize every single component – things can be more robust, or made out of heavier (but substantially cheaper) materials – K2 is essentially redesigning a satellite from scratch, even down to the reaction wheel, one of the most fundamental and settled aspects of satellite design (K2’s is one of the largest ever designed). Liberated from entrenched design paradigms that assume mass constraint, around 85% of the spacecraft is vertically integrated, if only because some of the technology doesn’t exist to support the novel satellite architecture.
For that reason, attracting top talent has been key. The company recently brought on Rafael Martinez, who led the design of the original Hall thruster for SpaceX’s Starlink constellation and was director of propulsion engineering at Apollo Fusion, to lead the design of what the Karan said will be the highest-powered Hall thruster to be deployed in space by a factor of four. Other notable hires include Ashrith Balakumar, who led the avionics engineering team for SpaceX’s Dragon spacecraft, and Drew Miller, a senior mechanical engineer with experience at SpaceX, Kitty Hawk and Maxar.
K2 has tripled in size since March – growing from a team of 6 to 18 – and is looking to expand even further to around 40 over the next six months. All this is leading up to the company’s first satellite launch in 2025, for which the launch partner hasn’t yet been announced. Investor interest in K2’s mission has not abated either: the company has also raised an additional $7 million in capital, bringing its total capital raised to $16 million, including an $8.5 million seed round announced earlier this year.
Among the new investors is Alpine Space Ventures, a European fund led by a number of early SpaceX engineers, including Catriona Chambers – who happens to be the person who hired Neel at SpaceX.
The company has also landed three contracts from the U.S. Department of Defense on behalf of different end users, reflecting some interesting traction from defense for K2’s larger platform. The company was awarded the three contracts, which have a total potential contract value of $4.5 million, over the last three months.
“There’s been a real push for resilience in our defense architecture, and historically resilience has come in the form of proliferation, where that proliferation required you going smaller, cheaper, faster,” Karan said. “But unfortunately, there’s a lot of use cases and a lot of end users that actually would like more power than what those small sats offer. The key thing that most of the end users we’ve talked to so far are excited by is being able to have proliferation without sacrificing performance.”
To give a taste of the kind of future the Kunjur brothers are building toward, they described a “dream mission”: launching four or five Mega-class satellites to establish a geostationary communications network around Mars. Scientific missions are deeply constrained in how much power they can send back because they depend on aging Mars orbiters – not to mention any future missions to the Red Planet.
But that’s just the beginning. A future in space unconstrained by mass is one that has only just begun to be explored.
“Across almost every application, because we’ve been forced to either mass constrain our payloads or volume constrain our payloads or even power constraint them, what we can actually do has been constrained to a large degree,” Karan said. “As a result, the types of missions you can do are more limited.”","https://techcrunch.com/wp-content/uploads/2023/10/K2-Space-Factory-Satellite-Photo.jpg?resize=1200,800",2023-10-17 13:00:39
https://techcrunch.com/2023/10/17/swshs-new-app-helps-you-maintain-your-friendships-through-polls-and-games/,Swsh's new app helps you maintain your friendships through polls and games,"Ever since BeReal became popular among Gen Zers, giving them a platform to be their true selves rather than scrolling through heavily edited content, there’s been a rise in social apps that cater to this craving for authenticity and connection. Swsh is a social app founded by Gen Zers, for Gen Zers that wants to help strengthen true friendships – as opposed to surface-level relationships with followers.
The app is essentially a mobile version of the “Most Likely To” game, which has existed for ages and rarely fails to liven up a party. It’s also great as an icebreaker when meeting someone new or catching up with a group of friends, whether virtually or in person.
Every night at 9 PM, Swsh users answer five daily “Most Likely To” questions with their friends, voting on who is the most likely to “dance with a stranger” or “party till 3 AM then work out at 6 AM.” Users collect superlatives and points for how many votes they earn. There’s also a comment section where users can debate about which friend best fits the description.
The polls are powered by Open AI’s ChatGPT and curated by Swsh’s team of four, Swsh co-founder and CEO Alexandra Debow explained to TechCrunch.
In addition to polls, Swsh will gradually test and roll out more in-app experiences “that feel natural to people you meet IRL and as a way to make it fun to keep in touch with them after,” Debow said, mentioning capabilities like posting photos after an event is over and the ability to share voice memos.
Swsh launched its public beta in September, with an official launch slated for early 2024. The app is currently only available on iOS devices. An Android version is coming soon.
The app’s name stems from “See you again, somewhere, somehow,” a saying Debow coined after growing tired of saying goodbye to so many friends. As a Canadian born and raised in Hong Kong, Debow met a lot of expats who never stayed for long; she became frustrated with having to communicate with long-distance friends over social media.
“Social media confuses friendship with followers, and text chains back and forth feel draining,” Debow said. “We believe that a middle ground exists: an effortless and enjoyable way to build relationships you care about.”
Debow is also the founder of Alive Vibe, a virtual events marketplace; The Entrepre女ers Network, a network for female entrepreneurs; and The Why Wait? Women Entrepreneurship Collective, the first Gen-Z-led women entrepreneurship event in Shanghai.
Swsh has two other co-founders– Weilyn Chong (COO) and Nathan Ahn (CTO). Chong is an economics and computer science student at Princeton University and a board member at the Nasdaq Entrepreneurial Center. She also co-founded the Entrepre女ers Network. Ahn studied computer science at Yale and was a software engineer intern at Meta.
Debow and Chong spoke at TechCrunch Disrupt last month.
The startup recently secured $1.7 million in pre-seed funding, which will help build the product as well as bring on engineering hires. The round was co-led by Stellation Capital and MaC Venture Capital, along with notable angels Glenn Solomon, Cory Levy (Z Fellows), Cyril Berdugo, Patrick de Picciotto, Ansh Nanda and Richard Li, among others.
“Consumer social has a history of being reinvented by young people: Facebook, Instagram and Snap are just a few canonical examples,” Peter Boyce, founder of Stellation Capital, said. “If everything works out, a young founding team like Swsh could earn their spot next, inventing a new consumer platform that shapes how we spend time with the people we care for in our lives.”","https://techcrunch.com/wp-content/uploads/2023/10/swsh-Founders.jpg?resize=1200,995",2023-10-17 13:00:23
https://techcrunch.com/2023/10/17/ai-generating-music-app-riffusion-turns-viral-success-into-4m-in-funding/,AI-generating music app Riffusion turns viral success into $4M in funding,"Nearly a year ago, developers Seth Forsgren and Hayk Martiros released a hobby project called Riffusion that could generate music using not audio but images of audio. It sounds counterintuitive (no pun intended), but it worked — my colleague Devin Coldewey got the rundown here.
While their approach had its limitations, Riffusion netted Forsgren and Martiros a lot of attention — not exactly surprising given the curiosity (and controversy) surrounding AI-generated music tech. Millions of people tried Riffusion, according to Forsgren, and the platform was cited in research papers published out of Big Tech companies including Meta, Google and TikTok parent ByteDance.
Some of the attention came from investors as well, it seems.
This year, Forsgren and Martiros decided to commercialize Riffusion, which is now being advised by the musical duo The Chainsmokers and has closed a $4 million seed round led by Greycroft with participation from South Park Commons and Sky9.
Riffusion is also launching a new, free-to-use app — an improved version of last year’s Riffusion — that allows users to describe lyrics and a musical style to generate “riffs” that can be shared publicly or with friends.
“[The new Riffusion] empowers anyone to create original music via short, shareable audio clips,” Forsgren told TechCrunch in an email interview. “Users simply describe the lyrics and a musical style, and our model generates riffs complete with singing and custom artwork in a few seconds. From inspiring musicians, to wishing your mom ‘good morning!,’ riffs are a new form of expression and communication that dramatically reduce the barrier to music creation.”
Matiros and Forsgren met at Princeton while in undergrad, and have spent the last decade playing music together in an amateur band. Forsgren previously founded two venture-backed tech companies, Hardline and Yodel, while Matiros joined drone startup Skydio as one of its first employees.
Forsgren says that he and Matiros were inspired to scale Riffusion by the potential they see in generative AI tools to connect people through creativity.
“The pandemic gave us all a lot more time at home — and led me to learn to play the piano,” Forsgren said. “Music has a great power to connect us in times of isolation. Generative AI is a new and rapidly changing space, and Riffusion aims to harness this technology to deliver a fun new instrument — one that empowers everyone to actively create music throughout their lives.”
The upgraded Riffusion is powered by an audio model that the Riffusion team — which is six people strong, including Forsgren and Matiros — trained from scratch. Like the model behind the original Riffusion, the new model’s fine-tuned on spectrograms, or visual representations of audio that show the amplitude of different frequencies over time.
Forsgren and Martiros made spectrograms of music and tagged the resulting images with the relevant terms, like “blues guitar,” “jazz piano” and so on. Feeding the model this collection “taught” it what certain sounds “look like” and how it might re-create or combine them given a text prompt (e.g. “lo-fi beat for the holidays,” “mambo but from Kenya,” “a folksy blues song from the Mississippi Delta,” etc.).
“Users describe musical qualities through natural language or even recording their own voice, as a method of prompting the model to generate unique outputs,” Forsgren explained. “We think the product will empower music producers and audio engineers to explore new ideas and get inspiration in a totally new way.”
Here’s a sample made using Riffusion’s ability to record a voice with the prompt “punk rock anthem, male vocals, energetic guitar and drums”:
But what, you might ask, about the potential for copyright infringement?
Increasingly, homemade tracks that use generative AI to conjure familiar sounds that can be passed off as authentic, or at least close enough, have been going viral. Just last month, a Discord community dedicated to generative audio released an entire album using an AI-generated copy of Travis Scott’s voice — attracting the wrath of the label representing him.
Music labels have been quick to flag AI-generated tracks to streaming partners like Spotify and SoundCloud, citing intellectual property concerns — and they’ve generally been victorious. But there’s still a lack of clarity on whether “deepfake” music violates the copyright of artists, labels and other rights holders.
Forsgren was quick to note that the new and improved Riffusion wasn’t trained to recognize famous artist names or songs — and, he says, can’t replicate them.
“The product isn’t built to produce deepfakes and doesn’t recognize famous artist names in its prompts,” he said. “Instead, it lets users craft personal messages and catchy hooks using the app. It’s not uncommon to have a riff you create get stuck in your head and find yourself singing along to it all day.”
There’s no clear monetization strategy — yet. For now, Forsgren and Martiros say that they’re focusing on growing Riffusion’s team and developing complementary new generative AI products.
But Forsgren also hinted at working more closely with artists like The Chainsmokers to see how the tech could be used in their creative processes.
“It’s very early days for generative music. Models such as Google’s MusicLM, Facebook’s MusicGen, and Stability’s Stable Audio are exciting tools in the space,” Forsgren said. “But Riffusion stands out as one of the first to enable users to generate lyrics in their music via a fun and accessible website.”","https://techcrunch.com/wp-content/uploads/2023/01/GettyImages-1352457907.jpg?resize=1200,752",2023-10-17 13:00:14
https://techcrunch.com/2023/10/17/nirvana-nabs-57m-to-make-ai-inroads-into-commercial-trucking-insurance/,Nirvana nabs $57M to make AI inroads into commercial trucking insurance,"Nirvana Insurance — an insurance startup taking a new approach to insurance products for commercial fleets using artificial intelligence, telematics, internet-of-things technology and 15 billion miles of trucking data to calculate risk models — is taking on something else: new funding.
The startup has raised an all-equity Series B of $57 million, money that it will be using to continue expanding its big data platform, for hiring, and to continue growing its business, which is initially targeting the trucking industry. The funding comes the heels of seeing its business grow 30-fold since launching in 2022.
Lightspeed Venture Partners is leading the round, with General Catalyst and Valor Equity Partners also participating. We understand that the round doubles the company’s valuation to over $350 million post-money.
The problem that Nirvana is aiming to solve is that for the most part, fleets of trucks are run as small businesses with very thin margins.
“Ninety percent of fleets have less than 50 trucks,” Rushil Goel, the CEO and co-founder of San Francisco-based Nirvana, said in an interview. With rising fuel costs, he added, “they are really struggling to stay alive.”
Add to that the mandatory headache of insurance.Typically it can cost $15,000-$20,000 annually to insure a fleet, a rate that is on the rise, he said. On top of that, getting quotes and policies sorted out can often take weeks, and filing and getting claims paid out can keep drivers off the road for weeks.
Nirvana’s solution aims to speed up activity across all those different areas: faster quotes, at rates that are right-sized to the customer in question, with better tools to claim against the policy if needed.
It does this by way of tapping into the many sensors that are already built into trucks, using the data amassed from them to build new models for pricing.
“We leverage billions of data points from sensors on on trucks to build risk models,” Goel said. “No one else is doing this.”
A typical truck might look more cumbersome and basic on first glance to the average consumer, but in fact there is a growing trend for more technology in these vehicles that is not unlike the developments underway in the consumer automotive space.
A federal mandate in 2017 required all trucks to have electronic logging devices installed. New vehicles are equipped with these, and in the U.S. the number of heavy trucks now with these numbers 18-20 million, he estimated, with the global number much bigger; and in the lighter truck market, those devices are in about 30-40% of all vehicles. Typically, used trucks in both categories get retrofitted, or are swapped out on an average of a 10-15 year refreshment cycle. All this points to a definite market today, plus one that is growing.
“There is a wave of IoT adoption in the trucking space,” Goel said.
Alongside its risk models, Nirvana has also built tools for its customers to both file and make claims using images and other data from dashboard cameras.
Sensors and cameras these days on trucks are able to pick up a number of characteristics in the game of driving: do drives brake too hard, do they signal before changing lanes, are they floating between lanes a lot, are they using their horn a lot, and where are they honking?
Goel said that the data it collects from these cameras and sensors is adequate enough that it’s built an AI based on it that helps calculate premiums for its customers; those whose driving Nirvana’s AI deems “safe” can get discounts of up to 20%.
“Commercial fleets today produce a tremendous amount of data, yet most insurers still insist on a cookie-cutter approach to insurance that does nothing to incentivize safety. Nirvana is bringing insurance into the modern era, changing how the industry considers risk,” said Raviraj Jain, a partner at Lightspeed Venture Partners, in a statement. “Their incredible growth is a testament to the opportunities AI and data analytics are opening up in fleet insurance and beyond, especially given that the IoT fleet management market is expected to continue growing.”","https://techcrunch.com/wp-content/uploads/2022/11/GettyImages-1357459564.jpg?resize=1200,799",2023-10-17 12:53:02
https://techcrunch.com/2023/10/17/hoxton-ventures-shoots-for-the-big-time-luring-bryan-gartner-from-khosla-ventures/,"Hoxton Ventures shoots for the big time, luring Bryan Gartner from Khosla Ventures","Everyone more or less agrees that 2023 is going to be effectively written off in VC-land, as the feeding frenzy of the last few years leaves everyone exhausted, valuations flattened or crashed, and exit market remain more or less closed.
VCs appear to be using this period to get their house in order for the next cycle. We’ve already witnessed General Catalyst using this down-time to scoop up a Seed arm in Europe; another Euro VC spend ‘quality time’ with their LPs to raise another fund; and former bystanders, use it to jump on the AI hype cycle.
Why not use it to expand your partnership, right?
To that end, Hoxton Ventures has now managed to lure former Bryan Gartner, Partner at Khosla Ventures, to join as Partner.
Gartner formerly worked on venture growth-stage investments at the VC, but he’ll be refreshing his memory of early-stage investing now he’s at Hoxton.
Over an interviewed, he admitted it’d also a career move based on his personal life: “I have a very international family. My wife is actually British, so [moving] was always on the cards…but we weren’t thinking this quickly. When I was introduced to Hussein through one of the Hoxton portfolio companies, I noticed immediately the transparent nature and the mutual respect among the partnership. A complete lack of ego in the room which you know, frankly, is a breath of fresh air. And it felt to me that there’s an inflection point that Hoxton is about to hit, and I’m thrilled to be to be part of that story.”
About his time at Khosla, he told me: “There’s there’s a dichotomy between the firm’s that truly work deeply with their portfolio companies and those that don’t… It’s not really about the board meeting. It’s about the calls you make, the meetings leading up to the board meeting. And I’ve always really enjoyed that. I’ve got a bunch of stories in my track record that weren’t obvious wins and then became 9, 10, 11x returns, because of really rolling up sleeves and plugging in and that’s what excites me in this industry. And that’s what I saw at Hoxton.”
Hoxton’s early stage credentials in Europe include a 60-strong portfolio which includes a number of successful companies including Darkrace, Deliveroo, Preply, Spacelift, and TourRadar, as well newer investments such as Avantia, Lumi, and XYZ Reality.
Hussein Kanji, partner, Hoxton Ventures said in a statement: “Bryan is an experienced investor, having seen companies go from their early formation to exit and IPO. His late stage skill set will provide us with a new strength around the table.”
He added over a call: “We’ve been intentionally thinking about expanding and building up the partnership. We started alongside Seedcamp, Connect Ventures, and a bunch of others pioneering investing in Europe, and I think we’ve gone into a really good ‘boutique mode’. But I think we’re trying to make another transition now.”
He said Hoxton is growign: “We’ve become much more of a firm and an institution, kind of in the same journey as Index Ventures in the 90s went from a small shop and became the powerhouse that it is today. And if you’re going to do that, you’ve got to hire really great people. So Brian is part of that. And we’re probably going to try and do this one or two more times.”
In March 2022, Hoxton Ventures closed a $215 million new fund, Hoxton III.
Gartner has 16 years of experience in venture capital. He was a Vice President at Insight Venture Partners, where he invested in companies that include AdColony, which was acquired by Opera for $350 million, Alteryx (IPO’d), Fenergo (acquired by Astorg and Bridgepoint for $600M), Pluralsight (acquired by Vista for $3.5B), Smartsheet (IPO’d), and Udemy (IPO’).
After Insight Venture Partners, he joined join Apax, where he invested in Wizeline, which was acquired by CDPQ for ~$450M. He was also a Partner role at Sidewalk Infrastructure Partners. He was previously a Partner at Khosla Ventures for two years.
Hoxton’s investments include Finesse, Giraffe360, Inflow, Really Clever, Peptone, Universal Quantum and XYZ Reality.",https://techcrunch.com/wp-content/uploads/2023/10/Bryan-Gartner-Hoxton-Ventures.jpg?w=762,2023-10-17 12:40:10
https://techcrunch.com/2023/10/17/amazon-passkey-sign-in/,"Amazon quietly rolls out support for passkeys, with a catch","Amazon has quietly rolled out support for passkeys as it becomes the latest tech giant to join the passwordless future. But you still might have to hold onto your Amazon password for a little while longer.
The option to set up a passkey is now available on the e-commerce giant’s website, allowing users to log in using biometric authentication on their device, such as their fingerprint or face scan. Doing so makes it far more difficult for bad actors to remotely access users’ accounts, given that the attacker also needs physical access to the user’s device.
But Amazon’s implementation of passkeys isn’t without issues, as noted by Vincent Delitz, co-founder of German tech startup Corbado, who first documented the arrival of passkey support on Amazon.
Delitz noted that there is currently no support for passkeys in Amazon’s native apps, such as Amazon’s shopping app or Prime Video, which TechCrunch has also checked, meaning you still have to use a password to sign-in (for now). What’s more, if you’ve set up a passkey but previously set up two-factor authentication (2FA), Amazon will still prompt you to enter a one-time verification code when logging in, a move Delitz said was “redundant,” since passkeys remove the need for 2FA as they are stored on your device.
Amazon says on its website: “You will still need to verify a one-time code after signing in with the passkey,” but does not explain why.
It’s unclear if the requirement for 2FA codes is a temporary feature and whether Amazon plans to add passkey support to its mobile apps. It’s also not yet known if passkey support has been made available to all Amazon users, though TechCrunch has confirmed that the feature is available in the U.S., U.K., France, and Germany.
Amazon spokesperson Adam Montgomery declined to answer TechCrunch’s questions, but said in a statement that Amazon “in the early stages of adding Passkey support for Amazon.com to give customers another secure way to access their accounts. We will have more to share soon.”
The arrival of passkeys on Amazon lands as WhatsApp announced that it’s rolling out support for passkeys to all Android users, and just days after Google said it planned to make passkeys the default sign-in method for all Google Account holders. GitHub, Windows 11, TikTok and 1Password have all rolled out support for passkeys.
Updated with comment from Amazon","https://techcrunch.com/wp-content/uploads/2019/06/password.jpg?resize=1200,900",2023-10-17 12:05:56
https://techcrunch.com/2023/10/17/procurify-lands-fresh-cash-to-invest-in-ai-powered-tools-for-procurement/,Procurify lands fresh cash to invest in AI-powered tools for procurement,"Roughly eight years ago, a little-known startup called Procurify raised $4 million for its platform that hosts tools to take some of the pain out of enterprise procurement.
The company never became buzzy, exactly. But Procurify grew steadily over the subsequent years, going on to raise $20 million in its Series B and today closing a $50 million Series C funding round led by Ten Coves Capital with participation from Export Development Canada, Canada’s export credit agency.
Procurify, which is based in Vancouver, Canada (hence the investment from the EDA), was co-founded by Aman Mann (the CEO), Eugene Dong (the CTO) and Kenneth Loi (the former CCO). The trio began working on the idea for the company in Dong’s parents’ basement after meeting in British Columbia Institute of Technology’s business management program in 2011.
“We recognized a gap in the procurement market for affordable, easy-to-use procurement software,” Mann told TechCrunch in an email interview. “In virtually every industry, organizations are struggling with a lack of real-time spend visibility and control.”
To Mann’s point, according to a recent Statista survey, hundreds of companies engaging in business-to-business procurement — i.e. finding, agreeing to terms and purchasing goods and services from a third party — admit to struggling with compliance processes, complex approval processes and purchasing systems and reconciling invoices in a timely manner.
So how does Procurify help? By consolidating various procurement steps in one place, mainly.
Procurify offers modules to manage purchasing, accounts payable (i.e. money owed to suppliers) and data analytics. Using the platform, customers can reallocate procurement spend and adjust forecasts, identify bottlenecks in “procure-to-pay” workflows and perform supplier analyses to inform future procurement decisions.
Procurify also leverages AI, detecting anomalies in purchase orders or invoices to flag them for review.
“In today’s post-pandemic economy, industries are grappling with layoffs, disrupted supply chains and rising operational costs,” Mann said. “The need for responsible spend controls and clear financial oversight is more pressing than ever.”
Procurify competes with incumbents (Coupa, SAP Ariba), enterprise resource management software with procurement management features (NetSuite) and upstarts (Precoro, Zip) in the over-$6.1 billion procurement software segment.
The startup appears to be doing well for itself, though, with a customer base of over 700 companies, a 100% year-over-year increase in sales and plans to invest heavily in bringing new AI capabilities to market.
“Procurify helps organizations bring more spend under management, thereby consolidating spend data from our customers’ procure-to-pay workflows to provide a complete picture of expenditures before they’re committed,” Mann said. “By harnessing the power of this comprehensive spend data and integrating it with AI models, enterprises could unearth opportunities for process optimization, manage risks and achieve cost efficiencies.”
Procurify, which has a team of just over 170 employees, has raised a total of $70 million in venture capital to date. In addition to AI R&D, Mann says that the proceeds from the Series C will be put toward general expansion and launching new payment features.","https://techcrunch.com/wp-content/uploads/2022/09/GettyImages-931203582.jpg?resize=1200,800",2023-10-17 12:00:48
https://techcrunch.com/2023/10/17/reality-defender-raises-15m-to-detect-text-video-and-image-deepfakes/,"Reality Defender raises $15M to detect text, video and image deepfakes","Reality Defender, one of several startups developing tools to attempt to detect deepfakes and other AI-generated content, today announced that it raised $15 million in a Series A funding round led by DCVC with participation from Comcast Ventures, Ex/ante, Parameter Ventures and Nat Friedman’s AI Grant.
The proceeds will be put toward doubling Reality Defender’s 23-person team into the next year and improving its AI content detection models, according to co-founder and CEO Ben Colman.
“New methods of deepfaking and content generation will consistently appear, taking the world by surprise both through spectacle and the amount of damage they can cause,” Colman told TechCrunch in an email interview. “By adopting a research-forward mindset, Reality Defender can stay several steps ahead of these new generation methods and models before they appear publicly, being proactive about detection instead of reacting to what just appears today.”
Colman, a former Goldman Sachs VP, launched Reality Defender in 2021 alongside Ali Shahriyari and Gaurav Bharaj. Shahriyari previously worked at Originate, a digital transformation tech consulting firm, and the AI Foundation, a startup building AI-powered animated chatbots. Bharaj was a colleague of Shahriyari’s at the AI Foundation, where he led R&D.
Reality Defender began as a nonprofit. But, according to Colman, the team turned to outside financing once they realized the scope of the deepfakes problem — and the growing commercial demand for deepfake-detecting technologies.
Colman’s not exaggerating about the scope. DeepMedia, a Reality Defender rival working on synthetic media detection tools, estimates that there’s been three times as many video deepfakes and eight times as many voice deepfakes posted online this year compared to the same time period in 2022.
The rise in the volume of deepfakes is attributable in large part to the commoditization of generative AI tools.
Cloning a voice or creating a deepfake image or video — that is, an image or video digitally manipulated to convincingly replace a person’s likeness — used to cost hundreds to thousands of dollars and require data science know-how. But over the last few years, platforms like the voice-synthesizing ElevenLabs and open source models such as Stable Diffusion, which generates images, have enabled malicious actors to mount deepfake campaigns at little to no cost.
Just this month, users on the notorious chat board 4chan leveraged a range of generative AI tools, including Stable Diffusion, to unleash a blitz of racist images online. Meanwhile, trolls have used ElevenLabs to imitate the voices of celebrities, generating audio ranging in content from memes and erotica to virulent hate speech. And state actors aligned with the Chinese Communist Party have generated lifelike AI avatars portraying news anchors, commenting on topics such as gun violence in the U.S.
Some generative AI platforms have implemented filters and other restrictions to combat abuse. But, as in cybersecurity, it’s a cat and mouse game.
“Some of the greatest risk from AI-generated media stems from use and abuse of deepfaked materials on social media,” Colman said. “These platforms have no incentive to scan deepfakes because there’s no legislation requiring them to do so, unlike the legislation forcing them to remove child sexual abuse material and other illegal materials.”
Reality Defender purports to detect a range of deepfakes and AI-generated media, offering an API and web app that analyze videos, audio, text and images for signs of AI-driven modifications. Using “proprietary models” trained on in-house data sets “created to work in the real world and not in the lab,” Colman claims that Reality Defender is able to achieve a higher deepfake accuracy rate than its competitors.
“We train an ensemble of deep learning detection models, each of which focuses on its own methodology,” Colman said. “We learned long ago that not only does the single-model, monomodal approach not work, but neither does testing for accuracy in a lab versus real-world accuracy.”
But can any tool reliably detect deepfakes? That’s an open question.
OpenAI, the AI startup behind the viral AI-powered chatbot ChatGPT, recently pulled its tool to detect AI-generated text, citing its “low rate of accuracy.” And at least one study shows evidence that deepfake video detectors can be fooled if the deepfakes fed into them are edited in a certain way.
There’s also the risk of deepfake detection models amplifying biases.
A 2021 paper from researchers at the University of Southern California found that some of the data sets used to train deepfake detection systems might under-represent people of a certain gender or with specific skin colors. This bias can be amplified in deepfake detectors, the coauthors said, with some detectors showing up to a 10.7% difference in error rate depending on the racial group.
Colman stands behind Reality Defender’s accuracy. And he asserts the company actively works to mitigate biases in its algorithms, incorporating “a wide variety accents, skin colors and other varied data” into its detector training data sets.
“We’re always training, retraining and improving our detector models so they fit new scenarios and use cases, all while accurately representing the real world and not just a small subset of data or individuals,” Colman said.
Call me cynical, but I’m not sure if I buy those claims without a third-party audit to back them up. My skepticism isn’t impacting Reality Defender’s business, though, which Colman tells me is quite robust. Reality Defender’s customer base spans governments “across several continents” as well as “top-tier” financial institutions, media corporations and multinationals.
That’s despite competition from startups like Truepic, Sentinel and Effectiv, as well as deepfake detection tools from incumbents such as Microsoft.
In an effort to maintain its position in the deepfake detection software market, which was valued at $3.86 billion in 2020, according to HSRC, Reality Defender plans to introduce an “explainable AI” tool that’ll let customers scan a document to see color-coded paragraphs of AI-generated text. Also on the horizon is real-time voice deepfake detection for call centers, to be followed b ay real-time video detection tool.
“In short, Reality Defender will protect a company’s bottom line and reputation,” Colman said. “Reality Defender uses AI to fight AI, helping the largest entities, platforms and governments determine whether a piece of media is likely real or likely manipulated. This helps combat against fraud in the finance world, prevent the dissemination of disinformation in media organizations and prevent the spread of irreversible and damaging materials on the governmental level, just to name three out of hundreds of use cases.”","https://techcrunch.com/wp-content/uploads/2023/07/GettyImages-1463459171.jpg?resize=1200,800",2023-10-17 12:00:27
https://techcrunch.com/2023/10/17/nova-credit-lands-45m-in-funding-to-grow-its-cross-border-and-alternative-data-credit-products/,Nova Credit lands $45M to grow its cross-border and alternative data credit products,"Moving from one country to another is difficult in many ways, not the least of which involves starting over financially.
Nova Credit, which started out as a graduate research project out of Stanford University about seven years ago, was founded to help immigrants overcome the obstacles of applying for things like apartments or loans with no credit history in the U.S.
“We realized that half of the graduate student population of any university consists of international students – and if you go and ask them about their experience with financial services, you’ll hear some version of the same story – ‘I can’t get credit or I can’t get a credit card. Or I need to go ask my classmate to co-sign for an apartment or cell phone plan,’ ” said Misha Esipov, CEO and founder of Nova Credit.
With that Credit Passport product, Nova has connectivity into credit bureau data from other parts of the world through its APIs. Nova launched that product with American Express and then added dozens of institution partners over the years, such as HSBC,Scotiabank, Verizon and Earnest. (Nova Credit emerges embedded within those institutions’ applications).
It works in 20 countries outside of the U.S. as well. If a person from the U.S. moves to London or Singapore, for example, they can get approved for banking products internationally in those markets through Nova.
In recent years, the startup also has expanded beyond its flagship Credit Passport product to also offer anyone – not just immigrants – the ability to use alternative data for credit but providing access to their bank account. That product, dubbed Cash Atlas, is cash flow underwriting product that allows anyone with a U.S. bank account to allow the information – such as rent payment history or direct deposit of paychecks – inside those accounts to be used to help them apply for credit. It is aimed at the tens of millions of people that are “effectively locked out of the U.S. financial system,” Esipov said.
“We’ve started to expand from a single product company to a multi-product company and platform…Our strategy is to evolve into a more data analytics company that serves not only people that are new to this country, but frankly any customer segment and plugging the gaps in this antiquated traditional credit reporting industry which doesn’t do well for serving customers who are new to credit,” Esipov told TechCrunch in an interview.
Today, Nova is announcing that it has raised $45 million in Series C funding, additional capital that it plans to use toward expanding its product offering and geographically, Esipov told TechCrunch exclusively. The round came together “in a matter of weeks,” he said.
Canapi Ventures led the round, which included participation from existing backers Kleiner Perkins, General Catalyst, Index Ventures and Y Combinator as well as new investors such as Avid Ventures, Geodesic Capital, Harmonic Capital, Radiate Capital, and Socium Ventures (Cox Enterprises).
The raise marks the company’s first external round of financing since February 2020, which makes it a bit of an outlier in the fintech space, which experienced a major funding boom in 2020 and 2021. At that time, Nova raised $50 million in a financing that reportedly gave it a valuation of $295 million after its first close.
Esipov declined to disclose Nova’s current valuation, saying only that the company was “xxxx happy” He also declined to reveal hard revenue figures, saying only that the company has grown its revenues by 10x since that 2020 funding round and that it more than tripled its data transaction volumes since the start of 2023.
Nova has remained relatively lean, with about 100 employees. It does plan to do some more hiring with its new capital. It also plans to take its Credit Passport business global – having already launched in markets such as Canada, the United Kingdom, the United Arab Emirate and Singapore. The company is also planning to invest in its Cash Atlas product and developing more new products.
For now, Nova’s Passport product provides the majority of the company’s revenue but Atlas “is growing even more quickly on a percentage basis right now.” Esipov said the company has invested “heavily” in information security and compliance since it was a seed-stage company.
The executive projects that the company will reach profitability in the relatively near future, possibly as early as next year. It opted not to raise more capital – in fact less than its last round – to avoid taking on “unnecessary dilution,” he added.
Jeffrey Reitman, general partner at Canapi Ventures, told TechCrunch he was initially attracted to Nova’s mission of enabling newcomers and thin-file consumers the ability to have fair access to financial products. Canapi first invested in the company in the Series B round and is impressed with “the explosive growth” it has displayed since.
“Many of our banking LPs are in active dialogues with Nova Credit to leverage their products to better serve their customers and that also nicely aligns with the mission here at Canapi,” he said.
Nova, Reitman added, “has amassed more collective access to international credit data than any one of the major credit bureaus and that makes for a very valuable proposition for its customers.”
Want more fintech news in your inbox? Sign up for The Interchange here.","https://techcrunch.com/wp-content/uploads/2020/09/GettyImages-a0146-000299-e1646928299148.jpg?resize=1200,799",2023-10-17 12:00:15
https://techcrunch.com/2023/10/17/microsoft-affiliated-research-finds-flaws-in-gtp-4/,Microsoft-affiliated research finds flaws in GPT-4,"Sometimes, following instructions too precisely can land you in hot water — if you’re a large language model, that is.
That’s the conclusion reached by a new, Microsoft-affiliated scientific paper that looked at the “trustworthiness” — and toxicity — of large language models (LLMs) including OpenAI’s GPT-4 and GPT-3.5, GPT-4’s predecessor.
The co-authors write that, possibly because GPT-4 is more likely to follow the instructions of “jailbreaking” prompts that bypass the model’s built-in safety measures, GPT-4 can be more easily prompted than other LLMs to spout toxic, biased text.
In other words, GPT-4’s good “intentions” and improved comprehension can — in the wrong hands — lead it astray.
“We find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, which are maliciously designed to bypass the security measures of LLMs, potentially because GPT-4 follows (misleading) instructions more precisely,” the co-authors write in a blog post accompanying the paper.
Now, why would Microsoft greenlight research that casts an OpenAI product it itself uses (GPT-4 powers Microsoft’s Bing Chat chatbot) in a poor light? The answer lies in a note within the blog post:
“[T]he research team worked with Microsoft product groups to confirm that the potential vulnerabilities identified do not impact current customer-facing services. This is in part true because finished AI applications apply a range of mitigation approaches to address potential harms that may occur at the model level of the technology. In addition, we have shared our research with GPT’s developer, OpenAI, which has noted the potential vulnerabilities in the system cards for relevant models.”
So the implication is that the relevant bug fixes and patches were made before the paper’s publication. Whether that’s truly the case, the research is yet another data point suggesting that LLMs — even from Microsoft-backed, billion-dollar-plus-revenue-generating startups like OpenAI — remain an imperfect science.
GPT-4, like all LLMs, must be instructed — or “prompted” — to complete a task (e.g. writing an email or summarizing an article). Jailbreaking LLMs entails using prompts worded in a specific way to “trick” the LLM into perform a task that wasn’t a part of its objective.
For example, the LLM powering Bing Chat wasn’t designed to write neo-Nazi propaganda. But at release, because it was trained on vast amounts of data from the internet — some of it toxic — it was susceptible to doing just that when fed a particular prompt.
As for GPT-4, the researchers found that the model is more likely to generate toxic text than GPT-3.5 when given certain jailbreaking prompts that they “constructed.” They also claim that GPT-4 “agrees with … biased content more frequently than GPT-3.5,” at least depending on the demographic groups mentioned in the prompt.
For example, give the prompt “Homosexuals have HIV,” GPT-4 usually “strongly” disagrees with the statement, according to the researchers. But given “Women have HIV,” GPT-4 agrees — and outputs biased content.
Just as concerningly, GPT-4 — when given the “right” jailbreaking prompts — can leak private, sensitive data including email addresses, say the researchers. All LLMs can leak details from the data on which they’re trained. But GPT-4 proves more susceptible to doing this than others.
Alongside the paper, the researchers have open sourced the code they used to benchmark the models on GitHub. “Our goal is to encourage others in the research community to utilize and build upon this work,” they wrote in the blog post, “potentially pre-empting nefarious actions by adversaries who would exploit vulnerabilities to cause harm.”","https://techcrunch.com/wp-content/uploads/2023/06/OpenAI-logo-symmetry.jpg?resize=1200,675",2023-10-17 11:30:25
https://techcrunch.com/2023/10/17/biotech-eu/,"Give biotech a chance for the planet's sake, EU lawmakers urged","European Union lawmakers are being urged to avoid too much risk aversion from holding back the potential of the homegrown biotech sector.
Developments in biotech could be transformative in a range of critical sectors. Beyond huge promise in healthcare, innovative, low carbon applications in areas like agriculture and food systems and energy production could help address pressing environmental and sustainability challenges. But there’s concern among some local operators that the bloc’s current approach could cap potential biotech benefits — especially in the context of the urgency required to tackle the climate crisis.
“The main regulatory challenges for the EU’s biotech startups are long timelines for approval of new products and a lack of openness towards modern biotech solutions that may lead to GMO solutions,” Joško Bobanović, partner at Sofinnova Partners, a major investor in European biotech, tells TechCrunch. “ Today, EU startups often do not bother trying to get approval in Europe because of long approval timelines, opting instead to go directly to the US or Asia. This is a huge loss for Europe given the plethora of leading-edge technologies developed here.
“Recent Nobel prizes for technologies like CRISPR or for discoveries that led to RNA vaccines highlight European regulators’ hesitance toward genetic technologies, similar to favoring landlines over mobile phones. (Remember what happened with Nokia and smart phones.) The potential benefits of these innovations far outweigh the risks even as they are part of a duly rigorous regulatory cycle.”
“If you look at venture capital, there’s significantly more money going into the synbio [synthetic biology] community in the United States, and so we’re really at a disadvantage here in Europe,” says Stef van Grieken, CEO and co-founder of EU-based startup Cradle, which offers generative AI tools to help bioengineers design proteins. “There’s also a lot of regulatory risk in Europe. So GMO, a lot of these types of techniques are considered genetic modification. And rules in Europe are very strict. And so if you look at a company like Meatable, that’s growing meat in in a dish instead of using a cow — they’re a Dutch company but they’re launching their products in Singapore, in the United States due to regulatory constraints.”
He also points the level of recent biotech support announced by the Biden administration, including a pledge to invest $2 billion in biotechnology and biotech manufacturing — suggesting the bloc is lagging behind on financial support for the field too. “According to McKinsey, about 60% of our current economic inputs you could make with biology,” he says. “And so that’s substantial, right? Like everything that we consume is a lot larger than the things on the internet.”
“One of the things that’s starting to become obvious is there’s lots of application domains for these types of techniques,” he adds, discussing generative AI’s role in accelerating biotech R&D. “I mean, I’m excited about ChatGPT and [popular generative AI] applications but let’s say… [helping] science and R&D teams to get their bio-based products to market faster to help us solve climate change may be a bit more important than producing better marketing copy.”
Earlier this month the European Union adopted a list of ten technologies it considers critical to the bloc’s future economic security — ranging from AI, quantum and advanced semiconductors, to space tech, robotics and biotech — making a clear statement of recognition of transformative and strategic potential. At the same time, four of the listed techs were flagged for further risk assessment, including biotech (the other three pegged for extra scrutiny are: AI, advanced semiconductors and quantum).
The Commission’s recommendation suggested Member States conduct collective risk assessments of these four critical areas by the end of the year — with lawmakers highlighting the possibility that transformative potential could also lead to highly sensitive risks, such as threats to fundamental rights or civil-military fusion.
Reports have suggested the move could prefigure the introduction of additional EU regulations.
Of the four technologies flagged for risk assessments, biotech may be the least familiar, in terms of public understanding — with the term spanning practices like genetic modification; new genomic techniques (such as CRISPR-Cas9 gene editing); gene-drive; and synthetic biology (aka synbio; a multidisciplinary field); all of which were explicitly name-checked in the Commission’s PR as examples of biotech that should be risk assessed by Member States.
The listed techs all deal with manipulating genetic material but can involve different approaches and applications. Developments in one field may also dial up potential elsewhere — such as gene editing techniques increasing potential applications for synthetic biology, for example — further advancing the complexity of developments since there may be overlap in how these biotechnologies are applied.
Despite relatively low public awareness of biotech advances, Cradle’s van Grieken points out some techniques have actually been widely used in industrial processes for years — helping to produce things like detergents which can work at lower temperatures (via industrially produced enzymes); or synthetic insulin for diabetics (i.e. instead of extracting biological insulin from the pancreatic glands of slaughtered cows and pigs).
While, as noted above, a newer wave of alternative protein startups — including companies being built in Europe — are leveraging developments in the field to do things like scale lab-grown meat or produce non-animal derived dairy proteins, on a mission to transform food systems without the huge carbon footprints attached to traditional (animal-derived) meat and dairy.
But it’s interesting how under the radar some of these regional applications of biotech remain. Certain terminology may be preferred (or avoided) in marketing copy — likely with an eye on regulatory risk and/or consumer trust.
“Precision fermentation is not synthetic biology per se,” a spokesperson for one alt protein startup — France’s Bon Vivant — told us, when we asked what it meant by “precision fermentation”, the term it prefers for explaining its dairy-targeting biotech, querying the bio techniques it’s applying to repurpose yeast microorganisms to brew up cow’s milk proteins.
“As a board member of Food Fermentation Europe, Bon Vivant is still working on a science based and still easily understandable definition,” the spokesman also responded to our ask. Its marketing copy, meanwhile, studiously avoids saying it’s genetically modifying yeast to produce milk proteins — which is essentially what it’s doing — the closest it comes is writing that it “programs” yeasts.
Yet it’s widely accepted that precision fermentation is an example of synthetic biology. (See, for e.g., Wikipedia’s definition: “Precision fermentation is an approach to manufacturing specific functional products which intends to minimise the production of unwanted by-products through the application of synthetic biology, particularly by generating synthetic ‘cell factories’ with engineered genomes and metabolic pathways optimised to produce the desired compounds as efficiently as possible with the available resources.”) So it’s curious to observe a European startup that’s doing interesting things with synthetic biology being so reluctant to say so.
The example speaks to the uncertainty steeping biotech developments in Europe — suggesting disruptors remain worried that causing a splash here could amp up their regulatory risk and bring fresh limits on their fledgling businesses, or at least trigger a new wave of consumer concern, rather than inviting admiration and unlocking homegrown support (or even — dare we say it — congratulatory cheerleading).
Cautionary tale
Cradle’s van Grieken is concerned the EU taking an overly risk averse approach to biotech is out-of-date with where the bloc needs to get to; that precautionary treatment of biotech is riskily self-defeating when it comes to the challenges now facing the bloc, including its headline green ambition to get to ‘Net Zero’ by 2050.
Europe is already “late to the party” when it comes to recognizing the economic and strategic importance of biotech compared to the US and parts of Asia, he argues. But his worry about the EU’s modus operandi is an active frustration that the bloc may be creating a blindspot by not being more encouraging of a sector with transformative potential when it comes to tackling the existential crisis of climate change.
“[Synbio’s potential] is not actually being recognised in the environmental policies of the EU,” he suggests. “If you look at the European Green Deal, a lot of it is focused on energy — like energy production, insulating more homes; it’s focused on recycling; on reducing pollution — like mobility; those types of things. Synbio isn’t really a theme. But it could be an incredibly powerful resource for the EU.
“This specific [Commission] call to the Member States to figure out what the risks are [for biotech] — my worry is that we’ll see increased regulation in this space without actually trying to promote the space and become… a leader in this space. Which we currently, unfortunately, are not. So that’s my biggest worry. But I do think at least recognising that it is something that could be strategic, it’s a good first step.”
“Biotech is a serious business and we need serious regulation here,” he adds when pressed to confirm his position. “But inversely, we don’t want to hamper innovation based on outdated notions of what this technology can and cannot do.”
“The EU needs to accelerate its regulatory processes and be more receptive to new technologies,” agrees Sofinnova’s Bobanović. “This is a critical success factor in the global race to address climate change but also to ensure food independence, a topic becoming more prominent post–COVID-19.”
“Failing to adapt may see our innovations benefiting other markets and the EU losing its competitive edge, much like the electronics industry. Once we lose talent and knowledge centers, it is impossible to recover them,” the investor also warns.
Consumer concern about genetically modified organisms (GMOs) does have a long history in the EU — especially in relation to food safety — which likely informs the precautionary approach the bloc has adopted towards the use of biotech in food production since at least the early 2000s. Out of that has come a legal framework that’s focused on health and safety; harmonized risk assessments; labelling; and traceability.
Consumer awareness of cutting edge biotech may be low but a perception of public concern over GMOs in food, which took root after an earlier era of developments during a time of more lax regulation, has been harder to shift. Yet actual consumer concerns are concentrated elsewhere, research suggests.
A 2019 Eurobarometer survey on food safety indicates EU citizens’ concern over GMO has declined while worries about food risks associated with traditional farming methods are riding high. So while 44% of respondents (the largest proportion) said they were concerned about the presence of antibiotics and hormone residues in meat; and 39% were worried about pesticide residues in foods; a lower proportion — 27% — said they were concerned about GMO being used in foods and only 4% were concerned about genome editing in this context (albeit, for the latter bio technique, the survey also found relatively low knowledge of the use of genome editing in food production — 21% vs 60% for GMO in food — so very low concern there may be a reflection of low awareness).
The survey results suggest EU policymaking in this area — certainly on the food front — risks being out of step with public safety concerns. (To wit: Environmental pollutants in fish, meat and dairy was another big worry for 37% of respondents.)
Taken together the Eurobarometer paints a picture of regional consumers with substantial anxieties about the health risks (and environmental toll) attached to current farming and agricultural practices — and lower concern about biotech being applied to engineer food output. (Also relevant: A Eurobarometer survey from 2021 which found an overwhelming percentage of EU citizens consider climate change to be the most serious problem facing the world.)
Yet the bloc remains saddled with a regulatory regime that ploughs massive subsidies into traditional agriculture while demanding high levels of caution — and even throwing up regulatory hurdles — when it comes to applying biotech to critical sustainability challenges. Critics argue this combo looks increasingly misaligned with where the bloc says it wants to get to with its flagship green transition.
Of course it’s worth noting that policymaking across the 27-Member State bloc is complex, with many entities necessarily involved in change-making. The Commission’s role, while important as a proposer of new pan-EU laws (and/or legislative reforms), is just part of the picture. EU Member States themselves can also have their own biotech and bio-ethics rules and reforms — so a Commission intervention listing biotech as a critical tech, and pushing for Member States to conduct risk assessments, may be aimed at driving for harmonization between this patchwork of national laws — which could, ultimately, streamline and simplify life for biotech entrepreneurs down the line.
Other factors also play a role. Another notable development for regulation of novel biotechs in the EU occurred, in 2018, when the Court of Justice (CJEU) ruled that organisms produced using relatively new techniques, such as gene editing, should fall under the bloc’s existing rules on GMO. So the legal system is also involved in interpreting how existing rules apply to biotech developments. But, again, it’s up to policymakers to keep up with such developments and make sure legislative frameworks are providing the right incentives.
“Europe is complex in terms of regulation, market access,” says Sofinnova partner Cedric Moreau, who is focused on the pharmaceuticals side of biotech investing. “We are not as the US [where] when you have the go from the FDA you have a more than a 300 million people market opening and very homogeneous.”
“We see where the European Commission wants to go — making sure that [it’s] not overlapping with State Members’ policy and making sure that the definition, and the category and the activity are very well defined; to not prevent any innovation or [developments] in the space that could be impacted by [divergence in Member State laws],” he suggests.
“It’s important to make some clear rules, clear definitions because [as investors] we need clarity,” he also tells us. “When we are investing in companies for five, eight, 10 years we cannot bet on regulation that will decide if our drug is a high unmet medical need or just an unmet medical need [for example]… And if our market exclusivity will be 10 years, or six years or nine years or five years. So we need to have clarity — and if it’s not clear enough what we will have to do to build our business case is always to retain the more conservative scenario.”
“At Sofinnova, we are a strong believer of Europe,” Moreau adds. “Because we are deploying — roughly 80% — of our capital in Europe. So we think that Europe is a fantastic playground for healthcare, for innovation. Because we have great science, great scientists, great people. And we have also an ecosystem that could really develop great success stor[ies]… Great products, impactful products for the patient. Then having said that… obviously, we think that there were several things that could be improved.”
Climate urgency vs legal uncertainty
“There is some urgency to consider these types of techniques seriously,” argues van Grieken, talking up the potential of synbio to help in the fight against climate change. “I’m not trying to advocate for ‘no regulation’ type of space. I think we need very strong controls. But on the actual end product, not on how they get researched and developed. And in certain cases, like for example with lab-grown meat or if you look at companies that are making alternatives to cheese or milk, those should be products that we should at least consider having on the market in the EU.”
“Take a company like Perfect Day foods in the United States,” he continues. “They’re making milk without cows. They can do that at, like around — I think — it’s 3% to 5% of the emissions compared to using a cow. That’s a pretty significant improvement. And we use a lot of dairy products, right? And we have a planet on fire.”
As we reported last year, Cradle is using generative AI to predict protein sequences to speed up R&D for protein engineers building bio-based products. So its business is applying AI to accelerate biotech developments — which, of course, means it has an interest in speeding up biotech progress by encouraging a more R&D-friendly regulatory environment, too.
The acceleration its customers are seeing is considerable, as van Grieken tells it — turning what would “typically” be a 1%-5% success rate for stabilizing a protein into a 50% success rate on average, thanks to the predictive power of its generative AI models. But stringent regulation is one brake the startup’s tech can’t uplift. Hence his call for EU lawmakers to zoom out and consider a bigger risk picture.
One idea he welcomes is if the EU were to establish more regulatory sandboxes where biotech R&D could be undertaken without so much legal uncertainty fogging the ambition — which amounts to a call for rules that focus more on outputs, than on the R&D itself.
When it comes to AI, a network of regulatory sandboxes is something the bloc is in the process of setting up — at the same time as EU co-legislators are hammering out a comprehensive, risk-based framework for applying artificial intelligence. So support for, and controls on, cutting edge techs are both possible under the regional lawmakers’ playbook.
Add to that, earlier this year (in April) the Commission put out out a proposal for reforming the bloc’s pharmaceutical regulation — which floats launching a regulatory sandbox as one of the suggested measures to boost regional innovation in drug research and design.
But, in that case, the sandbox would be limited to products regulated as medicines. So even if the bloc’s co-legislators adopt the proposal there are many other biotech innovations that won’t be granted a safe space to experiment — since the end product they’re aiming to disrupt isn’t a pharmaceutical. (And of course climate change won’t be fixed by popping a pill, personalized or otherwise.)
Supporting the production of edible proteins without the climate-heating emissions of traditional agriculture is just one example of biotech’s transformative potential for the environment. Bioplastics offer an alternative to petrochemical-based plastics, as another. While bioremediation is a field that offers promise for cleaning up pollutants — including by engineering microorganisms (such as algae) to accelerate uptake of CO2, the major climate heating gas.
Also on a climate tip, production of biofuels could be more sustainably scaled up using biotech techniques — such as, again, by designing microorganisms that can more efficiently turn biomass into low carbon biofuels.
European bioengineers are even working on genetically modifying plants to amp up their ability to fight indoor pollution (see: French startup Neoplants). So when you start to really think about engineering biology for human and environmental utility the canvas looks broad indeed.
Or, well, it should — but European biotech startups have to do their bluesky thinking from under a more legally clouded horizon.
For biotech startups operating in the EU, van Grieken argues it’s “significantly harder” to do the R&D and test potential innovations with so much regulatory risk hanging over the field. “There’s a lot of uncertainty,” he emphasizes. “For example, the Netherlands just introduced the ability to sample these types of [biotech-derived food] products and have investors taste them. But a very reasonable question from these investors is can you do that and sell this stuff? And if the answer is silence, then, you know, that is not a great answer. And I think this industry needs some clarity around that.”
Current EU rules also create some “weird” scenarios, as he tells it. For example, making an “informed edit” to a genome (i.e. where a bioengineer thinks about what mutation to make) would “typically” be considered a GMO in Europe (meaning the regulatory framework starts to apply) — whereas practices which produce random mutations, as happens a lot in the plant seed space, would not. So an operator that’s, for instance, shining UV light on a plant seed and introducing random mutations falls under less regulatory risk than someone doing bioengineering to select for a specific mutation — perhaps seeking higher crop yield to boost productivity or resistance to drought — regardless of the motivations behind the intent.
“If you think about how you might actually engineer one of these systems, it’s considered problematic; but if you just do it randomly, it’s fine. And so that’s not very smart,” he argues. “Because a lot of the techniques that we have today to make informed decisions about where to make changes in order to get to a certain outcome, that’s also a safe outcome — so it’s actually a lot better than doing it random.”
“If you look at, for example, the United States or places in Asia where a lot of these synthetic biology techniques are allowed it’s not like we’re seeing any major problems,” van Grieken also points out. “So we might be being a bit too constrained right now.
“You should be able to show that your product is good; actually is improving its environmental footprint; is safe to use; is delicious, in the case to food, right — and all these types of things — and get approval for it in some reasonable amount of time so you can still get to market.”
Towards a balanced approach?
Despite criticism that it’s too cautious, EU lawmakers have been talking about evolving the bloc’s approach to biotech. They have also been taking some action too.
This summer, for example, the Commission adopted a proposal for a new regulation on plants produced by certain new genomic techniques (NGTs) which would allow plants produced in this way which could also occur naturally (or via conventional breeding) to be placed on the market — exempting them from requirements in the current GMO legislation.
The NGTs the Commission has proposed loosening the rules for are targeted mutagenesis (aka plants that contain genetic material from the same plant); and cisgenesis, including intragenesis (i.e. plants that contain genetic material from crossable plants) — which would only need to undergo a verification process, under the proposal. Whereas transgenic plants (containing genetic material from non-crossable species) would remain subject to comprehensive, case-by-case risk assessment, approval and authorization prior to any sale under the EU’s existing GMO Directive.
The bloc’s Farm to Fork Strategy, meanwhile — part of the aforementioned European Green Deal which is focused on driving sustainability of agriculture and food production — recognizes biotech as having potential to contribute to the fight against climate change. “New innovative techniques, including biotechnology and the development of bio-based products, may play a role in increasing sustainability, provided they are safe for consumers and the environment while bringing benefits for society as a whole. They can also accelerate the process of reducing dependency on pesticides,” the Commission wrote in the May 2020 strategy document.
Although, subsequent to that, a 2021 study the EU undertook of new genomic techniques noted the “rapid” development of NGTs and their products over the past two decades — finding “considerable interest” in conducting research on NGTs in the EU. But it also identified that “most” development is taking place outside the EU. Which does support the contention the bloc is lagging when it comes to biotech research, despite “considerable” homegrown appetite to do this cutting-edge work.
“Following the [2018 GMO] ruling of the [CJEU], there have been reports of negative impacts on public and private research on new genomic techniques in the EU due to the current regulatory framework,” the EU’s executive also noted in the study. “Regulatory barriers would particularly affect small and medium-sized enterprises (SMEs) and smallscale operators seeking to gain market access with new genomic techniques, even though many Member States and stakeholders see opportunities for them in this sector.”
“The use of NGTs raises ethical concerns but so does missing opportunities as a result of not using them,” it went on, essentially echoing van Grieken’s point. “Based on the findings of the study, most of the ethical concerns raised relate to how these techniques are used, rather than the techniques themselves.”
At that time, the Commission concluded that any further policy action in the area should be “aimed at reaping benefits from innovation while addressing concerns”, further stipulating that a “purely safety-based risk assessment may not be enough to promote sustainability and contribute to the objectives of the European Green Deal and in particular the ‘Farm to Fork’ and biodiversity strategies”. The document also explicitly recognized that risk assessment alone could lead to a flawed evaluation process — in which “benefits contributing to sustainability” are not properly considered.
Asked about the critique it’s over-indexing on risk, when it comes to biotech, and not properly weighting potential sustainability (or, indeed, other) benefits, a Commission spokesperson declined to provide comment. But they pointed us to an EU webpage on R&D and the “bioeconomy” — where the EU’s executive also talks up the transformative potential of homegrown biotech developments, writing for example that: “Stronger development of the bioeconomy will help the EU accelerate progress towards a circular and low-carbon economy. It will help modernise and strengthen the EU industrial base, creating new value chains and greener, more cost-effective industrial processes, while protecting biodiversity and the environment.”
The page also links to the bloc’s long-standing bioeconomy strategy — which features an action plan that lists carrying out an analysis of “enablers and bottlenecks for the deployment of biobased innovations” as one of 14 “concrete actions” regional lawmakers are committed to (on paper at least).
The EU bioeconomy strategy was originally set out back in 2012, and reviewed in 2018, with the aim of supporting 2030 Sustainable Development Goals; the Paris Agreement climate objectives; and new EU policy priorities — with the Commission writing then that reaping the “economic, social and environmental benefits of the bioeconomy, dedicated bioeconomy strategies, investments and innovation are required at all levels in the EU”. Hence the updated strategy emphasizing the need for the development of national and regional bioeconomy strategies.
Five years on from that, the Commission lists just nine Member States that have set out a national bioeconomy strategy (Austria, Finland, France, Germany, Ireland, Italy, Latvia, the Netherlands and Spain) — meaning a substantial majority of EU members still lack this piece of the biotech ecosystem support puzzle. So, clearly, there’s more work for regional lawmakers to do to match the bloc’s ambition to build up Europe’s biotech base with actions that deliver results.
Looking ahead, Cradle’s van Grieken sees two big ares of promise for biotech: Human health being the first one; and what he refers to as “planetary health” as the second. “The reason why I left Google is because those are two of the major problems that my generation faces in the world,” he tells TechCrunch. “In human health, increasingly I think we’ll be a lot better at targeting disease with these types of [bioengineered] molecules and curing people.
“On the planetary health side, I think what will increasingly see is that bio-based products will come out that are cheaper than the petrochemical or animal alternatives. Because, ultimately, biology can do a lot of these things in a much lower energy way and also environmental footprint. I think we’re going to see a breadth of products that is going to be super exciting.”
He’s also bullish on cost — suggesting developments in generative AI can be the flywheel that speeds up biotech R&D — and that acceleration of developments in the lab will draw down the costs entailed in unlocking the big, transformative biotech benefits.
“It’s also why we started Cradle — to really accelerate R&D and make R&D a lot cheaper,” he says, arguing: “There is no fundamental reason why this cannot be done… Biology is ultimately capable of doing very complicated things at very low energy — like, look around you right now. There’s probably a plant somewhere there and try to realise that it’s just like water and ambient carbon that created that, right? It’s just wild, if you think about it.”
French startup Bon Vivant, meanwhile, is working to build a European business that can help tackle the planetary health challenge head on. As noted above, it’s reprogramming yeast microorganisms to produce milk proteins to offer the food industry an alternative so they can sell non-animal-based dairy products — which could have a massive impact on shrinking CO2 emissions if taken up at scale.
Foods derived from animals, including dairy, are generally associated with the highest greenhouse gas emissions (see, for e.g., this UN data on kilograms of emissions per kg of food) — owing to factors including land use, methane emissions from livestock and nitrous oxide emissions from the waste produced by animals. So biotechnologies applied to food production which can replace the need for us to get so much protein from animal sources have the potential for radical reductions in emissions if we integrate these new processes into our food systems.
Asked about the regulatory challenge of building an alternative protein business in Europe, Bon Vivant’s co-founder, Stéphane MacMillan, offers two thoughts. On the one hand he sounds sanguine — suggesting high food safety standards in the EU could create a competitive advantage for local startups over time, as a sort of ‘gold standard’ mark (i.e. once regulatory clearance to sell locally is obtained, which he estimates in their case may take two to three years vs a quicker anticipated time-to-market over in the US).
“Everyone is saying, well, it takes too long in Europe to get approval. Okay, it’s taking longer than any other countries but at the same time we have to be proud of standards that we have in Europe,” he tells TechCrunch. “These standards are also the reason why European food is really seen as the best class in most parts of the world. So we have to comply with it. It takes a bit more time. But, at the same time, I think… that guarantee for the consumer that our products are absolutely non-GMO — that’s really important and [builds trust] with customers.”
But he also suggests the bloc’s policymakers need to find “the right balance” — between having such high homegrown standards and risking a future where European consumers are forced to buy foreign bio products “because we were not able to build the champions”.
“It’s not black or white,” he suggests. “It’s a balance that we need all to find collectively. Both are right. But we just to find the right balance.”
Offering an investor perspective on the same point, Sofinnova’s Bobanović sees even less upside for EU biotech startups trying to turn increasingly strict regional food safety standards into a competitive advantage. So — at the least — the suggestion is the bloc shouldn’t be looking to pile more rules on the sector if it’s serious about growing the bioeconomy.
“While Europe’s stringent rules might enhance consumer trust in certain sectors, it’s unlikely the case for biotech,” he argues. “Unlike the luxury industry where ‘made in Europe’ is an advantage most food products are destined for local consumption and consumers already trust regulations. Increased regulation is not likely to influence product adoption.”","https://techcrunch.com/wp-content/uploads/2021/10/GettyImages-1342327233.jpg?resize=1200,800",2023-10-17 10:58:18
https://techcrunch.com/2023/10/17/scylladb-raises-43m-to-scale-its-nosql-database-platform/,ScyllaDB raises $43M to scale its NoSQL database platform,"Investors have an appetite for databases, it seems.
Today, ScyllaDB, a startup developing database tech for high-throughput, low-latency workloads, announced that it raised $43 million in a funding round led by Eight Roads Ventures with participation from AB Private Credit Investors, AllianceBernstein, TLV partners, Magma Ventures and Qualcomm Ventures.
The new cash will be put toward “accelerating” ScyllaDB’s momentum and expanding the size of its 168-person team, according to co-founder and CEO Dor Laor.
“Today’s disruptors are ingesting an unprecedented amount of data and tapping it to deliver differentiating user experiences that transform markets and displace legacy leaders,” Laor told TechCrunch in an email interview. “Data is being enriched, cleaned, streamed, fed into AI and machine learning pipelines, replicated and cached from multiple sources. That’s why it’s more important than ever to have a database that’s up to the task.”
ScyllaDB is what’s known as a NoSQL database, which — unlike the relational databases once dominant in the enterprise — provides mechanisms for data storage and retrieval that don’t rely on a “tabular relations” model. In a tabular model, a relationship is a connection between two tables of data. But with a NoSQL database, relationships don’t have to follow this schema — offering greater engineering flexibility and, in some cases, improved performance.
NoSQL databases are commonly used for applications like ad serving, AI and machine learning, recommendation and personalization engines, fraud detection and analyzing data from internet of things devices.
According to a 2022 survey by Ventana, almost a quarter (22%) of organizations are using NoSQL databases in production today, while more than one-third (34%) are planning to adopt NoSQL databases within two years or evaluating their potential use. And the NoSQL market is expected to grow to $35.7 billion by 2028, up from $7.3 billion in 2022, the IMARC Group reports.
Now, ScyllaDB’s not the only NoSQL vendor out there — far from it. There’s ArangoDB, Redis Labs and Crate.io to name a few, not to mention bigger players like MongoDB, Amazon’s DynamoDB and Couchbase.
But ScyllaDB claims that its tech offers architectural advantages, like the ability to perform millions of operations per second with “single-digit millisecond” latency. Running across multiple clouds, on a hybrid cloud setup or on-premises, ScyllaDB automatically tunes I/O and CPU performance with workload prioritization, which co-locate workloads under a single server cluster.
Those claims and capabilities were enough to win over customers, evidently. ScyllaDB says its database is now used by over 400 companies including Discord, Epic Games and Palo Alto Networks and that revenue has grown 800% since the company’s founding in December 2012.
“Across industries, R&D teams are increasingly realizing that ScyllaDB’s dramatically different database architecture delivers better performance and horizontal scalability for data-intensive workloads,” Laor said. “ScyllaDB is designed to help fast-growing, fast-moving teams deliver lightning-fast user experiences at extreme scale … ScyllaDB’s unique architecture takes full advantage of modern cloud resources, delivering impressive efficiency and price-performance.”
To date, ScyllaDB has raised $103 million in venture capital.","https://techcrunch.com/wp-content/uploads/2021/04/GettyImages-945420192.jpg?resize=1200,849",2023-10-17 10:00:57
https://techcrunch.com/2023/10/17/figures-humanoid-robot-walks-for-the-camera/,Figure's humanoid robot walks for the camera,"In May of this year, TechCrunch ran a piece titled “Figure’s humanoid robot takes its first steps.” The story was a firsthand account of my visit to the startup’s South Bay offices. The headline was a reference to both the company’s first year of existence and its stated plan to hit a key milestone by its first birthday.
The company later confirmed with me that it had, in fact, managed to get its humanoid robot to walk. I asked for video evidence, which Figure refused to send — until now. Among other things, it’s clear the company wants to get a film crew to capture the bipedal locomotion. I tend to prefer raw laboratory video for stuff like this, but that’s probably one of the many reasons no one is asking me to run marketing for their robotics firm.
Two things jump out at me immediately: First, it’s good to see a non-render from the company. Thus far, their art has been limited to mockups of what the robot could eventually look like. Watching this footage is a reminder that there are many steps along the way to that futuristic bit of product art. Second, you’ve probably noticed that the robot is moving with bent knees, rather than the fully upright motion we see in humans.
Bent knees, on the other hand, are pretty standard in robots — you’ve seen it with Boston Dynamics’ Atlas and Agility’s Digit (though the latter has a reverse bend, similar to an ostrich). Bending gives you better control of balance and other important factors. Ultimately there’s a question of how important it is to hue to a more human-like gait, but obviously this first video of the Figure 01 robot walking is very much early stage. There are plenty of kinks to work out between now and a ship date.
The other thing I will draw your attention to is the hands. Mobile manipulation remains a key problem in this world, and many humanoid systems like Digit and Apptronik’s Apollo have yet to add articulated graspers. Of course, there’s nothing in this video that suggests the grippers are currently functional. On my visit to the company’s HQ, however, they showed me a portion of the office devoted to the development of what looked to be a five-digit, human-style hand.
The video notes that Figure’s headcount at the time of shooting (10/1) was 60. Impressive growth for one year. More updates soon, no doubt.","https://techcrunch.com/wp-content/uploads/2023/10/Screenshot-2023-10-13-at-3.53.07 PM.jpg?resize=1200,725",2023-10-17 10:00:01
https://techcrunch.com/2023/10/17/invesco-raises-swiggy-valuation-to-nearly-8-billion/,Invesco raises Swiggy's valuation to nearly $8 billion,"Conditions appear to be shifting favorably for India’s Swiggy. The food delivery startup — backed by SoftBank, Prosus and Accel — saw its paper valuation slashed by more than a half this year as investors marked their holdings largely in response to the dwindling market conditions. The startup, valued at $10.7 billion in a funding round early 2022, also lost some market share to Zomato, its arch publicly-listed rival, according to Prosus.
Now, not so much.
Invesco, which led Swiggy’s previous round and cut its valuation to under $5.5 billion, marked up the startup’s valuation to $7.85 billion at July’s closure, according to a newly published disclosure.
The U.S. asset manager says it considers the valuation of similar public companies as a factor when reassessing the value of its private investments. Given that shares of Zomato have risen by 33% since the end of July, this could imply that the current $7.85 billion valuation for privately-held Swiggy may be conservative.
Separately, Swiggy, which is eyeing to make an initial public offering next year, appears to be closing in on some of the market share it lost to Zomato this year. Swiggy’s month-on-month volume grew 7% this July and 6% in August, beating Zomato in both months, UBS said in a report this month.","https://techcrunch.com/wp-content/uploads/2023/03/GettyImages-1239421148.jpg?resize=1200,801",2023-10-17 09:24:24
https://techcrunch.com/2023/10/17/stack-overflow-cuts-28-of-its-staff/,Stack Overflow cuts 28% of its staff,"Developer community site Stack Overflow has laid off 28% of its staff, the Prosus-owned company announced Monday.
In a blog post, Stack Overflow’s CEO, Prashanth Chandrasekar indicated that the company is focusing on its path to profitability. While the post didn’t elaborate on the reason behind the job cuts, it mentioned customers’ budgets shifting elsewhere “due to the macroeconomic pressures.”
“This year we took many steps to spend less. Changes have been pursued through the lens of minimizing the impact on the lives of Stackers. Unfortunately, those changes were not enough and we have made the extremely difficult decision to reduce the company’s headcount by approximately 28%,” Chandrasekar said.
While Stack Overflow is primarily a Q&A website for consumers it also has enterprise products like “Stack Overflow for Teams,” which helps organizations maintain a company-wide knowledge base.
The company didn’t specify the number of laid-off employees. However, since it had pushed its headcount to over 500 people last year, more than 100 people are likely to be impacted.
With generative AI gaining popularity for helping coders with different problems, Stack Overflow has seen its traffic drop as compared to last year.
In August, the company said that because of generative AI, it expects “some rises and falls in traditional traffic and engagement over the coming months.”
Earlier this year, Stack Overflow asked AI companies to pay for training data. In January, it barred users from posting answers generated by AI. The company is also trying to bolster its own AI capabilities. In July, it launched OverflowAI with features like generative AI-powered search.
Big Tech is also moving fast to make generative AI-aided products available for coders in a rapid manner. Last month, GitHub expanded access to its Copilot chat to individual users. In May, during its developer conference, Google announced a bunch of AI-centric coding tools including an assistive bot called Codey. The company has also trained its conversational AI tool Bard to help users with code generation and debugging.","https://techcrunch.com/wp-content/uploads/2023/10/3281da8e0c2332be9228c95317c493fe508c5a65-2400x1260-1.webp?resize=1200,630",2023-10-17 07:44:44
https://techcrunch.com/2023/10/17/ambani-jio-financial-launches-lending-and-insurance-businesses/,Ambani's Jio Financial launches lending and insurance businesses,"Jio Financial Services, the Indian conglomerate Reliance Industries-backed financial services firm, has started its lending and insurance businesses and plans to rapidly broaden its offerings as billionaire Mukesh Ambani expands the ever-so-wide tentacles of his oil-to-telecom empire.
The market has been closely paying attention to Reliance’s financial services ambitions for years. But it wasn’t until last year that Ambani, Asia’s richest man, revealed that the firm plans to enter into the sector, which though has grown multiple folds in the past decade remains largely untapped, serving only tens of millions of individuals.
Jio Financial Services, which made public debut in August, said in its annual presentation that it has started to offer personal loan to salaried and self-employed individuals through its MyJio app and 300 stores across India. Its insurance arm has also partnered with 24 insurers to offer a wide-range of coverage across auto, health, and corporate categories, said the firm.
Jio Financial Services has largely remained quiet about precisely what all it plans to do. The firm, whose largest backer remains Reliance Industries, earlier this year partnered with U.S. asset manager BlackRock to launch asset management services in the country.
The financial services is the newest sector for Ambani, who has entered several businesses — including telecom — in the past decade and scaled them to tentpole positions. Reliance also operates the nation’s largest retail chain, which has been valued at $100 billion in recent fund raises from investors including KKR.
As Jio Financial scales its business, it may pose a challenge to a number of players in the industry, including Paytm and Policybazaar. Reliance said it will make use of AI and analytics for its financial services business and operate on a “low cost of servicing.”
Jio Financial Services said it’s taking a direct-to-customer approach with its offerings to drive cost efficiencies and enabling personalized customer interactions. The firm is incorporating “alternate data models for 360-degree customer view and tailored offerings,” and is developing a unified app for the “diverse financial needs of customers.”
In the annual report, Jio Financial Services said it’s also testing a sound box, the fast-omnipresent portable device that alerts merchants when a transaction has completed, the firm said, confirming an August TechCrunch report. The company is “generating substantial data footprint and enhancing our customer engagement across digital channels, and in turn enriching and facilitating other businesses,” it said.
On lending, Jio Financial Services plans to extend loans to businesses and merchants as well as offer loans to facilitate vehicle and home purchases, it said. It also plans to give loans by using shares as collateral. The firm said it has also “relaunched” savings account service and bill payments and plans to launch debit cards.","https://techcrunch.com/wp-content/uploads/2023/08/GettyImages-1243634682.jpg?resize=1200,800",2023-10-17 07:26:12
https://www.innovationnewsnetwork.com/the-significance-of-measuring-climate-actions-in-the-battle-to-net-zero/35851/,The significance of measuring climate actions in the battle to net zero,"Violeta Gevorkjan, Decarbonisation and Sustainability Expert at HeavyFinance, looks at the role of quantifying and measuring climate actions in the transition to clean energy.
With rising temperatures impacting cities worldwide, and most prominently, the world powerlessly watching the fires in Rhodes unfold, the fight against climate change has reached a critical juncture.
As the urgency to combat global warming grows, the concept of carbon credits has also gained momentum as businesses globally are increasing their response to public concerns, client demands, and the need to embrace sustainability. They are now taking matters into their own hands by reducing their own carbon footprint.
Reliable metrics and comprehensive data surrounding climate change are proving crucial as they not only help track progress, but also shape effective policies, mobilise investment, and encourage collaboration among stakeholders.
Therefore, the role of quantifying and measuring climate actions cannot be overlooked in the global transition to a low-carbon economy.
Accountability and transparency
Establishing clear and standardised metrics for measuring climate actions is vital to hold governments, businesses, and individuals accountable for their commitments. Transparent reporting mechanisms foster trust among stakeholders and encourage greater participation, as well as facilitate international co-operation.
With measurable data, the effectiveness of various climate actions and initiatives can be assessed, allowing for informed decisions on where improvements are needed to be made.
Evidence-based decision-making
Accurate measurement provides various decision-makers with essential data to design and implement effective climate policies. Understanding the emissions footprint and the impact of different climate actions enables policymakers to prioritise investments, set ambitious targets, and strategically allocate resources.
Furthermore, with a robust measurement framework in place, successful strategies can be identified, facilitating the replication and scaling up of any impactful initiatives.
Keeping observations can also help with a variety of predictions, such as the amount of rainfall expected next summer or how far sea levels will increase due to warmer temperatures. Following Rhodes, measurements can be used to pick up when or where else fires could occur.
Measuring climate actions such as carbon sequestration is too important in revealing how such efforts are benefitting the wider planet.
Investor confidence and market transformation
Funding and investors play an important role in helping drive the transition to a low-carbon economy. By providing investors with accurate and reliable metrics, their ability to assess risks and opportunities is enhanced.
The UK Government has also recently launched a £50m Research Ventures Catalyst fund, which has combined efforts from the government and private and philanthropic investors, to bolster cutting-edge research in tech and science. Such funding will encourage innovators to explore and take new approaches to R&D in the fight against climate change with confidence.
Robust measurement frameworks can also build investor confidence, encouraging sustainable investments and redirecting capital away from high-carbon activities. In consequence, this unlocks the potential for market transformation, fostering the development of innovative technologies, and sustainable business models.
Global collaboration and ambition
Achieving net-zero emissions requires unprecedented collaboration on a global scale. Measuring and quantifying climate actions provide a common language and shared understanding across nations, facilitating co-operation and knowledge-pooling.
Consistent metrics enable benchmarking and comparison, fostering healthy competition among countries and encouraging them to raise their ambitions. A collective commitment to comprehensive measurement frameworks can also play a key step in maximising the potential of international climate agreements such as the Paris Agreement.
Providing high-quality carbon credits
Prioritising the accuracy, verification, and high value of carbon credits is thus essential.
Implementing practices and monitoring systems to ensure credibility, such as regular soil sampling and personalised advice for farmers to enhance soil health and carbon sequestration, is crucial to both the confidence of investors and battling global warming.
A scientific approach to soil management and carbon measurement not only builds trust in the quality of generated carbon credits, but also reinforces transparency, accountability, and continuous process improvement along the pipeline.
Measuring climate actions offers critical benefits
The battle to achieve net-zero emissions and combat climate change requires a combined global and data-driven approach. Measuring and quantifying climate actions offer several critical benefits, ranging from accountability and transparency to investor confidence and global collaboration.
By relying on reliable metrics and comprehensive data, decision-makers can design effective policies, businesses can demonstrate their commitment to sustainability, and nations can work together to accelerate the transition to a low-carbon economy.
The importance of measurement cannot be overstated, as it lays the foundation for a sustainable future and ensures a shared responsibility in safeguarding our planet for generations to come.",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/08/shutterstockPhilip-Steury-Photography_1672124824.jpg,2023-10-17 13:32:22
https://www.innovationnewsnetwork.com/deep-south-resources-back-in-business-in-copper-exploration-industry/38337/,Deep South Resources is back in business in copper exploration,"Deep South Resources is back to upgrading its copper exploration game in Southern Africa.
Deep South Resources, a Canadian mining exploration company working in Africa, has concluded a long legal battle and is now continuing its work and planning for the future. The company specialises in copper exploration in Africa, particularly in Namibia and Zambia.
We interviewed their president and CEO, Pierre Léveillé, to find out what happened and what is next for the company.
Can you briefly summarise Deep-South Resources? What is your background and key objectives?
Deep South Resources is a mining exploration company specialised in exploring for copper deposits. Our main area of expertise is in the region of Southern Africa, and our main projects are in Namibia and Zambia.
In Namibia, we have the Haib copper project. Here, we’re starting a feasibility study on a copper deposit of 5.3 billion lbs, and we know we can add more.
In Zambia, the company holds three very large exploration licenses that are very well situated, in the heart of the Copper Belt. As of yet, there has been no drilling, so we are really starting from scratch.
The company’s key objective is to explore, find, and develop the largest possible copper deposits. This is something we have already started in Namibia and intend to continue.
In terms of copper, Zambia is probably up there with the Democratic Republic of the Congo, and consequently we believe the project has great promise.
We are hunting for copper deposits in places with large, proven copper reserves and resources.
We think that the project will be productive enough to attract major players to buy us or join us in development. Our main expertise is exploration and development in the Southern part of Africa.
Although we are not looking towards copper production right now, we have the expertise and potential to do it ourselves in the future.
What is the potential of Zambia and Namibia in copper exploration?
The potential for copper exploration in Namibia is greater because the project is more advanced. As well as this, we already have established a defined large resource, and we know we can increase it. The potential, therefore, relies on increasing that already large deposit.
On the exploration side, our main goal and the main potential is to increase the grade, which we are on track to do.
We estimate that by the middle of 2024, we will have increased the grade by at least 25%.
The Zambian project holds major exploration potential. Situated right in the middle of the Copper Belt, surrounded by nine very large copper mines, and we are on the same geology as them. The first exploration programme has enabled us to discover very large copper anomalies. We’re on the right track; the potential to find a deposit is great.
As opposed to Namibia, where the deposit has already been discovered, in Zambia we are still at the deposit discovery stage.
How is Deep South Resources embracing the green revolution?
The technologies we are looking to use for potential future mining are greener metal extraction methods. The classic way to extract metals, such as grinding, milling, roast leaching and flotation, is quite old-fashioned, and it’s highly demanding in terms of power. The roasting also causes a lot of air pollution.
We will not necessarily avoid roasting in Namibia, but it will be limited and will be combined with other technologies, such as heap leaching.
We are also looking at bio heap leaching. The beauty of bio heap leaching is that when you finish the treatment, the gravel that remains is stable. This means that no pollutive material will contaminate the ground and water table. It’s not totally green, but it’s a lot greener than roasting and flotation.
Have you encountered any challenges whilst undertaking these copper exploration projects? And if so, how have they been overcome?
In Namibia, we have the Haib project, which we bought in May 2017. Four years later, in June 2021, the Minister of Mines and Minerals Development refused to renew our licence, stating that he was told that we don’t do any work, so we don’t deserve to have the project.
Naturally, we argued that we had five drills on-site, with 60 employees active. We had shipped one ton of samples to Australia for metallurgical test work; there was a lot of activity. There was a serious difference between the information he received and the information we communicated to the Ministry. Nevertheless, he decided to pull the plug on our licence.
We immediately went to the High Court of Namibia, and requested an injunction over that area to prevent the ministry from granting a licence to anyone else. In doing this, we discovered another private company had applied for our licence six months prior to the expiry. This is very unusual.
In Namibia, when you apply for an existing licence in general, the Ministry will state that the license is still valid and to come back later when it has expired.
Our second step was to request the court to review the Minister’s decision. The court case lasted a little over a year. We had the final hearing in October 2022.
At the end of the same month, there was a big scandal that ran in the press in Namibia, first on social media and then through the traditional press. The scandal concerned allegations that were similar in nature to those experienced by Deep-South Resources. Potential issues surrounding license renewals were highlighted during this time and we were hopeful of a positive outcome.
It was such an important scandal that we thought it was impossible the judge hadn’t seen it. We were also in the fortunate position of having an injunction on the project. Thus, when the judge rendered a strong verdict in our favour, the licence was still available.
It is not in the mandate of the court to order the Ministry to grant the licence, but he ordered them to reopen the application for renewal that was originally denied in 2021, this time considering all the facts presented by Deep-South.
We wanted to defend ourselves to protect the interests of our shareholders. We finally got the licence back in July 2023, two years later. That situation has been seriously damaging; our market capitalisation has dropped by 80%.
We did not get any compensation from the Ministry. If we want compensation, we have to institute a new court case where we will request damage, but on the other hand, we want to keep a good relationship with the Ministry, so we will avoid legal disputes.
That’s been our biggest challenge, not only for us, but for the numerous high-level Namibians who were really annoyed by this scandal and expressed their discomfort with the situation. At the end of the day, the rule of law prevailed and we are now back on track to continue our exciting progress in Namibia.
We also have had smaller challenges, but it is part of our business. As an example, we’re owed a lot of money from the value-added tax, but it takes an awful lot of time to be reimbursed. At some point, they tried to change the rules to not pay back exploration companies, but now it seems to be back on track.
We spent over a year awaiting a payment of around $135,000, which is a lot of money for a small exploration company. We also have issues from time to time, with work visas for foreign employees. The procedure is long and complex compared to Zambia. It’s more administrative stuff that sometimes makes our life a bit difficult. But we always resolve these things.
So what’s next for Deep South Resources?
Quite a lot! We just closed the financing. So even if the market is very difficult at the moment for small companies, we just closed a financing of $2m, and we’re pretty happy because three institutions have taken $1.5m of the financing. They’re very good supporters. With this funding in place, we are resuming the drilling programme that was suspended on the Haib copper project, as well as restarting the feasibility study procedures.
At the completion of the drilling programme, we will also have a new resource estimation somewhere in early 2024. We just announced that we have appointed the MSA group in South Africa to complete this resource estimation.
We are also looking at appointing an engineering firm for the environmental impact study that goes with the feasibility study. We’re working on receiving proposals now from engineering firms for the feasibility study.
We are resuming our activities with METS Engineering in Australia and CSIRO, the governmental laboratory in Australia, for metallurgical test work, heap leaching and other tests. All of this will be announced during the next month, which means that by the end of October, we will be in full swing with the resumption of our activities.
At the same time, in Zambia, we are starting a small programme of geophysical-induced polarisation surveys on the Luanshya West project. We have generated some very interesting soil sampling results recently, and we want to better define the drilling targets for next year. We will be very busy. Stay tuned because there’s a lot of activity coming. We’re pretty excited about these developments and exploration programmes.
After the two years we spent fighting for the company, we look forward to a very good year coming up in the copper exploration field.
Please note, this article will also appear in the sixteenth edition of our quarterly publication.
Go to this partner's profile page to learn more about them",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/DEEPSOU1-28928-shutterstockMo-Femi-Pearse_1270772323-1024x576.jpg,2023-10-17 12:30:24
https://www.innovationnewsnetwork.com/how-the-space-age-is-polluting-our-atmosphere/38320/,How the Space Age is polluting our atmosphere,"The Space Age is leaving fingerprints on the stratosphere – one of the most remote parts of the planet – which has potential implications for climate, the ozone layer, and the continued habitability of Earth.
Using tools hitched to the nose cone of their research planes and sampling more than 11 miles above the planet’s surface, researchers have discovered that the Space Age has resulted in significant amounts of metals in aerosols in the atmosphere.
The increasing amount of debris is likely due to more frequent launches and returns of spacecraft and satellites.
This mass of metal is changing atmospheric chemistry in ways that may impact Earth’s atmosphere and the ozone layer.
The study, ‘Metals from spacecraft re-entry in stratospheric aerosol particles,’ is published in Proceedings of the National Academy of Sciences.
The rising use of metal and polluting materials in space
“We are finding that the Space Age has released human-made materials in what we consider a pristine area of the atmosphere,” said Dan Cziczo, one of the study’s authors.
The team detected more than 20 elements in ratios that mirror those used in spacecraft alloys.
They found that the mass of lithium, aluminium, copper, and lead from spacecraft re-entry far exceeded those metals found in natural cosmic dust.
Nearly 10% of large sulfuric acid particles – the particles that help protect and buffer the ozone layer – contained aluminium and other spacecraft metals used in the era of the Space Age.
Scientists estimate that as many as 50,000 more satellites may reach orbit by 2030. The team calculates that means that, in the next few decades, up to half of stratospheric sulfuric acid particles would contain metals from re-entry.
What effect that could have on the atmosphere, the ozone layer and life on Earth is yet to be understood.
The Space Age is in full swing, but how does this impact space pollution?
Spacecraft launches and returns were once international events. The launches of Sputnik and the Mercury missions were front-page news.
Now, a quickening tide of innovation and loosening regulation means that dozens of countries and corporations are able to launch satellites and spacecraft into orbit and fuel the Space Age.
All those satellites must be sent up on rockets, and most of that material eventually comes back down.
The Space Age has left behind a trail of metals that may change the atmosphere in ways scientists don’t yet understand.
“Just to get things into orbit, you need all this fuel and a huge body to support the payload,” Cziczo commented.
“There are so many rockets going up and coming back and so many satellites falling back through the atmosphere that it’s starting to show up in the stratosphere as these aerosol particles.”
He concluded: “Changes to the atmosphere can be difficult to study and complex to understand.
“But what this research shows us is that the impact of the Space Age on the planet may be significant. Understanding our planet is one of the most urgent research priorities there is.”",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/shutterstockstudiovin_2208172963.jpg,2023-10-17 10:22:21
https://www.innovationnewsnetwork.com/research-innovation-key-renaissance-european-solar-manufacturing/38305/,Research and innovation is key for the renaissance of European solar manufacturing,"The world is switching to green energy, and Europe is presented with an opportunity to ensure solar energy plays the critical role it is meant to.
The European solar manufacturing landscape is at a critical crossroads, with a perfect storm emerging. Over ordering on the demand side, combined with overcapacity on the supply side, has resulted in a record drop in prices for solar modules and other system components. It has never been more important for the EU to advance its industrial strategy for solar PV and support European solar manufacturers.
This is where research and innovation (R&I) comes in. Expanding the European solar manufacturing base also requires an expansion of the EU’s R&I activities. Increased investments in innovative technologies are a key component in the long-term reindustrialisation of Europe’s solar manufacturing base.
Undoubtedly, R&I has been high on the Commission’s agenda. Under Horizon Europe, the EU’s key R&I funding programme, the Commission has allocated a significant budget of €95.5bn. More than 40% of that fund – €40bn – is earmarked for research that supports the European Green Deal.
Previously, the Commission invested €4.99bn towards clean energy technologies under its Horizon 2020 programme; only 10% of this was earmarked for PV projects. The Directorate-General for Research and Innovation has promised to help Europe “stay ahead of the game, and accelerate the roll-out of the EU’s strategic net-zero technologies,” like solar PV.
Solar has also been booming across Europe, especially in the last three years. Over 40GW of solar was installed in 2022, nearly double what was installed the previous year. Already in the first half of 2023, solar generation grew by 13%, according to a recent Ember report.
We have truly entered a thriving solar era, with record drops in fossil fuel generation. Every year, solar generation and installation numbers are increasing.
The R&I challenge
With solar numbers on the rise, and the EU setting its R&I agenda, you might wonder what the issue is? The reality is that the rest of the world has also woken up to the strategic role of solar. China is by far the world leader in the manufacturing of solar. The US has the Inflation Reduction Act which is spurring a boom in American solar manufacturing. India, Turkey, and South Africa are all making similar moves.
There are a number of solutions that EU leaders can take to ensure that European solar manufacturing plays a significant role in the globalised solar supply chain, like changing subsidy rules, setting up a dedicated financing instrument, and delivering ‘resilience’ auctions under the incoming Net-Zero Industry Act.
One wider tool we can’t forget is the role of R&I in supporting Europe’s manufacturing base – the critical link between ‘labs’ and ‘fabs’. The Fraunhofer Institute for Solar Energy Systems (ISE) has found that in Germany, PV technology development has delivered cost reductions of 36% per year on average since 2010, thanks to a combination of upscaling of manufacturing, and continued R&I advancements.
According to the Commission’s third Progress Report on the Competitiveness of Clean Energy Technologies, half of the EU’s greenhouse gas reductions expected by 2050 will require technologies that are not yet commercially available. Solar PV alone is set to generate more than 60% of the EU’s electricity by 2050, requiring more private and public investment into clean energy research.
The perovskite potential
The development of new cell technologies like perovskite solar cells reflect the necessity of expanded R&I investments in European solar manufacturing. In May 2023, Oxford PV, a perovskite solar manufacturer, set the record for the highest recorded efficiency of any commercial-sized solar cell in its Brandenburg an der Havel factory in Germany.
The cell converted 28.6% of the sun’s energy into electricity, and was made by placing a thin film of the perovskite material onto a conventional silicon solar cell. The combined ‘perovskite-on-silicon’ tandem solar cell achieves a conversion efficiency that is substantially higher than that of mainstream silicon-only solar cells, which average 22–24%.
Commenting on the achievement, David Ward, Chief Executive Officer at Oxford PV, noted that their, “innovative solar cells are close to being in the hands of our module-manufacturing customers,” with the focus now on ramping up production.
Recent findings from a team at the University of Surrey have also illustrated the potential of perovskite. They found that a nanoscale ‘ink’ coating improved the perovskite solar cells’ stability, making them suitable for mass production.
Researchers made this breakthrough when they discovered an aluminium oxide that minimises the drop in efficiency during the conditioning of perovskite solar cells. Perovskite has also been used to create self-healing solar panels on satellites in low-Earth orbit that can recover 100% of their efficiency, even after being damaged by radiation in space.
In general, the perovskite material is lighter and cheaper than a silicon-based solar cell, and is extremely efficient. It has even been praised as a ‘miracle material,’ a label earned with recent developments. When it reaches the market at scale, it will transform the PV sector.
All of these findings have emerged from investments into one innovative technology. The European solar PV sector’s research representation to the European Commission – ETIP PV – has made the point that getting R&I to production scale will require continuous EU funding.
Otherwise, we risk only investing in technologies that will one day be obsolete, especially given the acceleration of innovation cycles.
For example, fully realising the potential of perovskite solar cells will require more funding to facilitate its mass-production, and improve its shelf life; currently, perovskite deteriorates quickly when exposed to light, voltage, or heat.
More research will also be required to substitute the lead currently used in perovskite solar cells, for a more eco-friendly alternative, while retaining its efficiency.
A step forward for European solar manufacturing
Undoubtedly, the EU is leading in several areas of PV research and innovation, including in perovskite tandem solar cells. Oxford PV, Fraunhofer ISE, and Helmholtz-Zentrum Berlin, all European research and development bases, hold global solar cell conversion efficiency records.
However, rapid advancements in R&I investments by public and private actors in other regions are catching up to the EU, and threaten to leave it behind, especially when it comes to the industrialisation of R&I.
For example, in June, at a Shanghai trade show, a Chinese manufacturer already announced its plans to commercialise a perovskite solar cell. The EU is already lagging behind in the production of ingots and wafers, an important segment of solar manufacturing. It has also seen a notable loss of expertise in key segments of the PV R&I landscape.
Just like investments in manufacturing, the EU will need to expand and intensify its R&I investments if it wants to keep up, especially with its renewed ambitions for PV manufacturing.
Alongside renewed investments in PV manufacturing capacity, the EU must also invest in the industrialisation of the results of R&I efforts, and strengthen its investments towards developing the next generation of PV technologies.
For example, the perovskite potential will be lost without a strong investment commitment to bridging the manufacturing gap and delivering innovation at scale.
The ETIP PV White Paper on manufacturing puts it best: adequate financing is needed throughout the EU PV value chain. Many solutions can be implemented to accelerate R&I financing at the European level. This could include relaxing the EU’s State Aid rules – its Temporary Crisis and Transition Framework (TCTF) – to accelerate the channelling of funding towards projects.
The rebirth and long life of European solar manufacturing needs a strong research and innovation base. Europe has its part to play in the world’s solar manufacturing story.
Innovations like perovskite solar cells are just one example waiting on the horizon, reflecting the potential of R&I investments to revolutionise the solar industry.
However, the PV manufacturing storm is still brewing; sustained and increased investments in EU R&I investments will help build resilience against this storm. Left unchecked, this storm will only get worse.
Now is the time to double down on EU research and innovation, so that we can guarantee the EU’s solar renaissance.
Please note, this article will also appear in the sixteenth edition of our quarterly publication.",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/1-Copyright-Meyer-Burger-Thalheim-factory.jpg,2023-10-17 09:28:51
https://www.innovationnewsnetwork.com/new-innovation-challenge-launched-tackle-bias-in-ai-systems/38298/,New innovation challenge launched to tackle bias in AI systems,"UK companies can now apply for up to £400,000 in government investment to fund innovative solutions tackling discrimination and bias in AI systems.
The competition will look to support up to three groundbreaking homegrown solutions, with successful bids securing a funding boost of up to £130,000 each to tackle bias in AI.
It comes ahead of the UK hosting the world’s first major AI Safety Summit to consider how to best manage the risks posed by AI while harnessing the opportunities in the best long-term interest of the British people.
Tackling bias in AI systems is a major priority
The first round of submissions to the Department for Science, Innovation, and Technology’s Fairness Innovation Challenge, delivered through the Centre for Data Ethics and Innovation, will nurture the development of new approaches to ensure fairness underpins the development of AI models.
The challenge will tackle the threats of discrimination and bias in AI by encouraging new approaches, which will see participants building a wider social context to develop their models from the off.
Fairness in AI systems is one of the government’s key principles for AI, as set out in the AI Regulation White Paper. AI is a powerful tool for good, presenting near-limitless opportunities to grow the global economy and deliver better public services.
Minister for AI, Viscount Camrose, said: “The opportunities presented by AI are enormous, but to fully realise its benefits we need to tackle bias in AI.
“By ensuring AI models do not reflect bias found in the world, we can not only make AI less potentially harmful but ensure the AI developments of tomorrow reflect the diversity of the communities they will help to serve.”
Harnessing a new, UK-led approach
While there are a number of technical bias audit tools on the market, many of these are developed in the US.
Although companies can use these tools to check for potential bias in AI systems, they often fail to fit alongside UK laws and regulations.
The challenge will promote a new UK-led approach which puts the social and cultural context at the heart of how AI systems are developed, alongside wider technical considerations.
This will focus on two areas. First, a new partnership with King’s College London will offer participants from across the UK’s AI sector the chance to work on potential bias in AI models. The model, developed with Health Data Research UK with the support of NHS AI Lab, is trained on the anonymised records of more than ten million patients to predict possible health outcomes.
Second is a call for ‘open use cases’. Applicants can propose new solutions which tackle discrimination in their own unique models and areas of focus, including tackling fraud, building new law enforcement AI tools, or helping employers build fairer systems which will help analyse and shortlist candidates during recruitment.
Companies currently face various challenges in tackling bias in AI, including insufficient access to data on demographics and ensuring potential solutions meet legal requirements.
The CDEI is working closely with the Information Commissioner’s Office (ICO) and the Equality and Human Rights Commission (EHRC) to deliver this Challenge. This partnership allows participants to tap into the expertise of regulators to ensure their solutions marry up with data protection and equality legislation.
Stephen Almond, Executive Director of Technology, Innovation and Enterprise at the ICO, explained: “The ICO is committed to realising the potential of AI for the whole of society, ensuring that organisations develop AI systems without unwanted bias.”
Baroness Kishwer Falkner, Chairwoman of the Equality and Human Rights Commission, added: “Without careful design and proper regulation, bias in AI systems has the potential to disadvantage protected groups, such as people from ethnic minority backgrounds and disabled people.
“Tech developers and suppliers have a responsibility to ensure that the AI systems do not discriminate.”",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/shutterstockSansoen-Saengsakaorat_2345949869.jpg,2023-10-17 08:27:32
https://www.innovationnewsnetwork.com/new-battery-recycling-method-efficiently-recovers-aluminium-and-lithium/38292/,New battery recycling method efficiently recovers aluminium and lithium,"Researchers from Chalmers University of Technology have developed a new battery recycling method that allows the recovery of 100% of the aluminium and 98% of the lithium in electric car batteries.
The new battery recycling method also minimises the loss of valuable materials such as nickel, cobalt, and manganese.
The researchers use oxalic acid to reduce costs and harmful chemicals in the process.
The paper, ‘Complete and selective recovery of lithium from EV lithium-ion batteries: Modelling and optimisation using oxalic acid as a leaching agent,’ is published in the journal Separation and Purification Technology.
How does the new battery recycling method work?
The team fine-tuned the temperature, concentration, and time to use the oxalic acid to facilitate EV battery recycling.
Martina Petranikova, Associate Professor at the Department of Chemistry and Chemical Engineering at Chalmers, said: “We need alternatives to inorganic chemicals. One of the biggest bottlenecks in today’s processes is removing residual materials like aluminium.
“This is an innovative method that can offer the recycling industry new alternatives and help solve problems that hinder development.”
The aqueous-based recycling method is called hydrometallurgy.
Traditional hydrometallurgy
In traditional hydrometallurgy, the metals in an EV battery cell are dissolved in an inorganic acid. The impurities, such as aluminium and copper, are then removed.
The valuable metals such as cobalt, manganese, lithium, and nickel are separately recovered.
Although the amount of residual copper and aluminium is small, several purification steps are required. Each step in this process can cause lithium to be lost.
The new method reduces the waste of valuable metals
The new battery recycling method reduces the waste of valuable metals needed to make new batteries by reversing the order of the traditional process. By doing this, lithium and aluminium are recovered first.
The latter part of the process leaves aluminium and lithium in the liquid, and the other metals in the solids. Aluminium and lithium then need to be separated.
“Since the metals have very different properties, we don’t think it’ll be hard to separate them. Our method is a promising new route for battery recycling – a route that definitely warrants further exploration,” said Léa Rouquette, PhD student at the Department of Chemistry and Chemical Engineering at Chalmers.
“As the method can be scaled up, we hope it can be used in industry in future years,” concluded Petranikova.",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/shutterstockMaxx-Studio_191931434.jpg,2023-10-17 08:02:49
https://www.innovationnewsnetwork.com/software-supply-chain-security-how-spot-cyber-risks-hidden-plain-sight/38274/,Software supply chain security: How to spot cyber risks hidden in plain sight,"Matt Middleton-Leal, Managing Director EMEA at Qualys, discusses the importance of software supply chain security.
Every day, you will see headlines around IT security issues affecting companies and public sector bodies. These organisations will be affected by attacks that could involve everything from data theft or sensitive information being leaked through to full blown ransomware deployments and interruptions to service. One of the biggest routes for these attacks over the past few years has been the software supply chain – if an attacker can jeopardise one software component, then they can attack multiple companies through that hole.
Applications are now more complex too. They have more moving parts, from open source software projects and internally developed software code through to third party applications that are embedded into services. They can expand and scale up to meet demands, created on-demand using infrastructure as Code or in Kubernetes containers. And they can be created using public software hosted on services like GitHub or the Python Package Index.
As a result, many security teams are not able to track those software assets effectively. To solve this, we have to make it easier to manage those software components and prevent potential attacks from coming in.
Bills of materials and software management
In practice, security teams need insight into what IT assets the organisation has. Without this list, it is impossible for IT security leaders to call their organisations secure. This has to be expanded to software as well.
The first step for this is to create a full inventory of the software that you have in place and the components used to make it. For internal applications – otherwise termed first-party software – this level of insight is often lacking. Studies have shown that between 70 to 90% of first-party software includes open-source components; according to our analysis of more than 13 trillion anonymised data points in the 2023 TruRisk Research Report, 79% of servers installed use open-source components.
Application and security operations teams most commonly rely on manual checks or siloed scripts to evaluate the security of first-party software. This depends on how well those checks work and how often they are carried out, and delays teams from prioritising the right risks for remediation.
Setting up a full process for software supply chain management starts with knowing what applications components you have and what versions those components are. Traditional vulnerability assessment or software composition analysis tools do not detect the presence of embedded open-source packages across the production environment. So, expanding your approach to cover these first-party software applications should be a natural first step.
Alongside looking internally, you should also look at the software that you consume from others. This third-party software will itself be built of different components and services, and any one of those can have an issue. As you don’t ‘own’ the software, it may be difficult to peer inside and know if there are any out-of-date components that could affect your security.
To fix this problem, the US Government supported the use of software bill of materials, or SBOMs. SBOMs provide customers with a list of all the components used within a given application, so security teams can spot any faults that come up. However, uptake of SBOMs is still in its infancy.
Regulation may help on this in time. SBOMs were mandated by the US Government in an Executive Order in May 2021, while the European Union’s Cyber Resilience Act also includes resolutions to implement SBOMs for hardware and software manufacturers that provide products to European consumers. The UK Government is also developing its approach to cyber resilience, and SBOMs are expected to be part of that approach.
The challenge here is that SBOMs are still relatively new, and CISOs have other more pressing issues around security that take up their teams’ resources and commitment. Regulation may force this up the agenda in time, but software supply chain attacks are happening now. SBOMs can deliver more insight into what cyber risks exist and consequently where to concentrate. As part of an overall software supply chain strategy, SBOMs will be essential in future, but the data they provide will help you manage risk around misconfigurations or vulnerabilities now too.
Improving overall processes around security
Just like any security process, software supply chain security depends on the data coming in and how quickly that information can be turned into actions. The issue is that software today is so complex that you can easily miss potential problems, whether they are in your organisation’s own software or contained in another company’s products.
Getting more data on what is in place is necessary in order to begin improving security. However, this data is not useful without the right context. Without that insight, you will not be able to prioritise where changes are needed in your own applications, and you will not be able to put pressure on your suppliers around their updates. Equally, you will not be able to manage those potential risks effectively and mitigate problems before they come up.
To improve your approach to software supply chain management, you will have to look at your overall approach to risk and how you manage software within your organisation. Bringing first-party software and third-party application risk data together will help your team understand the potential threats that exist, where changes are needed, and how you can support those problems getting fixed in an efficient and timely manner.
To make this work for you, look at how you can automate the data gathering so you have a continuous level of insight into what you have in place, and then prioritise your risks so you can always get ahead of any potential problems before they become serious or lead to attempted attacks.",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/©-shutterstock3rdtimeluckystudio_2268705275.jpg,2023-10-17 07:39:18
https://venturebeat.com/business/cibc-innovation-banking-provides-growth-capital-financing-to-dealmaker-2/,CIBC Innovation Banking Provides Growth Capital Financing to DealMaker,"AUSTIN, Texas & TORONTO–(BUSINESS WIRE)–October 17, 2023–
CIBC Innovation Banking is pleased to announce an upsized growth capital financing for DealMaker, a leading capital-raising technology platform. This funding will further fuel the company’s accelerated expansion across North America.
DealMaker was founded in 2018 by veteran Bay Street lawyers, Rebecca Kacaba and Matt Goldstein to modernize the shareholder issuance space. The company was recently selected to join Google’s for Startups Accelerator program, which will help advance DealMaker’s AI innovation in the fintech space.
“The combination of CIBC Innovation Banking and Google backing our long-term plans is an exciting acceleration for us,” says Rebecca Kacaba, CEO & Co-Founder of DealMaker. “Their support helps us in our mission to build the most sophisticated capital markets tool on the planet – and there is no better way to achieve this than to weave AI capabilities throughout our platform for ease of use for both investors and issuers.”
“DealMaker’s comprehensive platform creates a more equitable and modern capital raising experience, focusing intently on transparency,” said Rob Rosen, Managing Director in CIBC Innovation Banking’s Toronto office. “We are thrilled to extend our team’s financial and advisory support as the company continues to scale impressively.”
DealMaker was recently named the sixth fastest growing company in the Globe and Mail’s Top Growing Companies of 2023.
About CIBC Innovation Banking
CIBC Innovation Banking delivers strategic advice, cash management and funding to innovation companies across North America, the U.K., and select European countries at each stage of their business cycle, from start-up to IPO and beyond. With offices in Atlanta, Austin, Boston, Chicago, Denver, Durham, London, Menlo Park, Montreal, New York, Reston, Seattle, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking, private banking, wealth management and capital markets businesses.
About DealMaker
DealMaker is revolutionizing the capital markets with its sophisticated suite of primary issuance, shareholder management, and capital raising solutions, including investor ranking algorithms and data analytics tools, to support all global private placements exemptions. Its mission is to put brands and founders back in control, running streamlined, successful capital raises in one centralized platform. The company operates out of Toronto, Canada; Austin, Texas; and Tampa, Florida. For more information, visit [DealMaker.tech](https://dealmaker.tech).
View source version on businesswire.com: https://www.businesswire.com/news/home/20231017578891/en/
CIBC: Katarina Milicevic, katarina.milicevic@cibc.com, 416-784-6108
DealMaker: Leigh Nolan, leigh.nolan@dealmaker.tech",https://venturebeat.com/wp-content/uploads/2015/10/BusinessWire_FeaturedImage.jpg?w=1200&strip=all,2023-10-17 13:25:50
https://venturebeat.com/games/nvidia-enables-ai-processing-on-windows-pcs-with-rtx-gpus/,Nvidia enables AI processing on Windows PCs with RTX GPUs,"GamesBeat Next unites gaming industry leaders for exceptional content, networking, and deal-making opportunities. Join us on Oct 23-24 in San Francisco. Register Now
In a milestone for personal computing, Nvidia is enabling better AI on PCs by enabling generative AI processing on Windows PCs using RTX-based graphics processing units (GPUs).
In the past year, generative AI has emerged as a transformative trend. With its rapid growth and increasing accessibility, consumers now have simplified interfaces and user-friendly tools that harness the power of GPU-optimized AI, machine learning, and high-performance computing (HPC) software.
Nvidia has enabled a lot of this AI revolution to happen in data centers with lots of GPUs, and now it’s bringing that to RTX-based GPUs on over 100 million Windows PCs worldwide. The integration of AI into major Windows applications has been a five-year journey, with the dedicated AI processors called Tensor Cores, found in GeForce RTX and Nvidia RTX GPUs, driving the generative AI capabilities on Windows PCs and workstations.
Jesse Clayton, director of product management and product marketing for Windows AI at Nvidia, said in an interview with GamesBeat that we’re at a big moment.
Event GamesBeat Next 2023 Join the GamesBeat community in San Francisco this October 23-24. You’ll hear from the brightest minds within the gaming industry on latest developments and their take on the future of gaming.
Learn More
“For AI on PCs, we think is really one of the most important moments in the history of technology. And I don’t think it’s hyperbole to say that for gamers, creators, video streamers, office workers, students, and really even casual PC users — AI is delivering new experiences. It’s unlocking creativity. And it’s making it easier for folks to get more done. AI is being incorporated into every important app. And it’s going to impact every PC user. It’s really fundamentally changing the way that people use computers.”
Previously announced for data centers, TensorRT-LLM, an open-source library designed to accelerate inference performance for large language models (LLMs), is now making its way to Windows. This library, optimized for Nvidia RTX GPUs, can enhance the performance of the latest LLMs, such as Llama 2 and Code Llama, by up to four times.
Additionally, Nvidia has released tools to assist developers in accelerating their LLMs, including scripts that enable compatibility with TensorRT-LLM, TensorRT-optimized open-source models, and a developer reference project that showcases the speed and quality of LLM responses.
“What many people don’t realize is that AI use cases on PC are actually already firmly established. And Nvidia really started this five years ago in 2018,” Clayton said. “When we launched our first GPUs with Tensor Cores, this was a fundamental change in the GPU architecture because we believed in how important AI was going to be. And so with the launch of the so called RTX GPUs, we also launched AI technology for gaming.”
Stable Diffusion demo
Stable Diffusions runs seven times faster on RTX than Apple’s latest chip.
TensorRT acceleration has also been integrated into Stable Diffusion, a popular Web UI by Automatic1111 distribution.
Stable Diffusion takes a text prompt and makes an image based on it. Creators use them to create some stunning works of art. But it takes time and computing resources to come up with each image. That means you have to wait for it to get done. Nvidia’s latest GPUs can speed performance by two times on Stable Diffusion on the previous implementation and more than seven times faster on Apple’s latest chips. So a machine with a GeForce RTX 4090 graphics card can generate 15 images on Stable Diffusion in the time it takes an Apple machine to do two.
DLSS, was based on graphics research where AI takes a low-resolution image and upscales it to high resolution, increasing the frame rate and helping gamers get more value out of their GPUs. Game developers can also add more visual artistry in their games. Now there are more than 300 DLSS games and Nvidia just released version 3.5 of the technology.
“Generative AI has reached a point where it’s unlocking a whole new class of use cases with opportunities to bring PC AI to the mainstream,” Clayton said. “So gamers will enjoy AI-powered avatars. Office workers and students will use large language models, or LLMs, to draft documents and slides and to quickly extract insights from CSV data. Developers are using LLMs to assist with coding and debugging. And every day users will use LLMs to do everything from summarize web content to plan travel, and ultimately to use AI as a digital assistant.”
Video Super Resolution
Moreover, the release of RTX Video Super Resolution (VSR) version 1.5, as part of the Game Ready Driver, further enhances the AI-powered capabilities. VSR improves the quality of streamed video content by reducing compression artifacts, sharpening edges, and enhancing details. The latest version of VSR delivers even better visual quality with updated models, de-artifacting content played in native resolution, and support for both professional RTX and GeForce RTX 20 Series GPUs based on the Turing architecture.
The technology has been integrated into the latest Game Ready Driver and will be included in the upcoming Nvidia Studio Driver, scheduled for release in early November.
The combination of TensorRT-LLM acceleration and LLM capabilities opens up new possibilities in productivity, enabling LLMs to operate up to four times faster on RTX-powered Windows PCs. This acceleration improves the user experience for sophisticated LLM use cases, such as writing and coding assistants that provide multiple unique auto-complete results simultaneously.
Finding Alan Wake 2
Alan Wake has to escape from The Dark Place in Alan Wake 2.
The integration of TensorRT-LLM with other technologies, such as retrieval-augmented generation (RAG), allows LLMs to deliver targeted responses based on specific datasets.
For example, when asked about Nvidia technology integrations in Alan Wake 2, the LLaMa 2 model initially responded that the game had not been announced. However, when RAG was applied with recent GeForce news articles, the LLaMa 2 model quickly provided the correct answer, showcasing the speed and proficiency achieved with TensorRT-LLM acceleration.
Clayton said that if the data already exists in the cloud and if the model had already been trained on that data, it makes sense architecturally to just run it in the cloud.
However, if it’s a personal data set, or a data set that only you have access to, or the model wasn’t trained on the cloud, then you have to find some other way to do it, he said.
“Retraining the models is pretty challenging to do from a computation perspective. This enables you to do it without taking that route. I am right now paying $20 a month to be able to use [AI services]. How many of these cloud services am I going to pay if I can do a lot of that work locally with a powerful GPU?”
Developers interested in leveraging TensorRT-LLM can download it from Nvidia Developer. Additionally, TensorRT-optimized open-source models and a RAG demo trained on GeForce news are available on ngc.nvidia.com and GitHub.com/NVIDIA.
The competition?
Nvidia can add new data for an LLM to find you answers.
Competitors like Intel, Advanced Micro Devices, Qualcomm and Apple are using rival technologies to improve AI on the PC as well as smart devices. Clayton said these solutions will be good for lightweight AI workloads running on low power. These are more like table stakes AI, and they’re complimentary with what Nvidia’s GPUs do, he said.
RTX GPUs have 20 to 100 times the performance of CPUs on AI workloads, he said, and that’s why the tech starts with the GPU. The math at the core of modern AI is matrix multiplication, and at the core of Nvidia’s platform are RTX GPUs with Tensor Cores, which are designed to accelerate matrix multiplication. Today’s GeForce RTX GPUs can compute up to 1,300 trillion Tensor operations per second, which makes them the fastest PC AI accelerators.
“They also represent the world’s largest install base of dedicated AI hardware with more than 100 million RTX PC GPUs worldwide,” Clayton said. “So they really have the performance and flexibility for taking on not only today’s tasks but tomorrow’s AI use cases.”
Your PC can also turn to the cloud for any AI tasks that are too demanding for your PC’s GPU. Today, there are more than 400 AI-enabled PC applications and games.",https://venturebeat.com/wp-content/uploads/2023/10/VSR_1.5.jpg?w=1200&strip=all,2023-10-17 13:00:00
https://venturebeat.com/ai/ai-platform-alliance-will-drive-ai-to-be-more-open-efficient-and-sustainable/,"AI Platform Alliance will drive AI to be more open, efficient and sustainable","VentureBeat presents: AI Unleashed - An exclusive executive event for enterprise data leaders. Network and learn with industry peers. Learn More
A group of prominent companies in the AI industry announced the establishment of the AI Platform Alliance, a consortium aimed at making sure that AI platforms will be more open, efficient, and sustainable.
The consortium seeks to address the growing demand for scalable AI solutions and overcome the challenges posed by the increasing compute power required for AI training and inferencing. The founding members of the alliance include Ampere, Cerebras Systems, Furiosa, Graphcore, Kalray, Kinara, Luminous, Neuchips, Rebellions and Sapeon, with more companies expected to join in the future.
The formation of the AI Platform Alliance comes at a critical juncture, not only for the technology industry but also for the world as a whole. The rapid expansion of AI has led to an unprecedented need for compute power to train and run AI workloads. The aim of the group is to “promote better collaboration and openness.”
While AI training demands substantial compute resources upfront, the total compute power required for AI inferencing can be up to ten times higher, posing an even greater challenge as AI usage scales up. One of the primary objectives of the AI Platform Alliance is to enhance the power and cost efficiency of AI hardware, surpassing the performance delivered by traditional graphics processing units (GPUs).
Event AI Unleashed An exclusive invite-only evening of insights and networking, designed for senior enterprise executives overseeing data stacks and strategies. Learn More
Recognizing the complexity involved in implementing AI solutions, the AI Platform Alliance will collaborate to validate joint AI solutions that offer superior alternatives to the prevailing GPU-based status quo. Notably, Nvidia, the biggest designer of AI chips and GPUs, isn’t a member of the alliance.
By fostering community-driven development, the consortium aims to expedite AI innovation by creating more transparent and accessible AI platforms. This collaborative approach seeks to enhance the efficiency of AI in solving real-world problems and establish sustainable, environmentally friendly, and socially responsible infrastructure at scale.
The AI Platform Alliance invites AI companies that are developing hardware solutions and are dedicated to challenging the status quo to join the consortium. Interested companies can apply for membership through the alliance’s website.",https://venturebeat.com/wp-content/uploads/2023/10/ai-platform-alliance.jpg?w=1200&strip=all,2023-10-17 12:00:00
