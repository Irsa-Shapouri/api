Link,Title,Text,Image,Date Publish
https://www.forbes.com/sites/mikestubbs/2023/10/17/valorant-708-patch-notes-bring-major-smoke-changes/,‘Valorant’ 7.08 Patch Notes Bring Major Smoke Changes,"Valorant gets some big smoke changes. Credit: Riot Games
Riot Games has released the patch notes for Valorant update 7.08 and there are some massive changes coming to smokes, that will make them easier to play around.
With smokes being such an important part of Valorant the dev team has implemented a pretty big change with the 7.08 patch notes. As big smokes can be used to block areas for a considerable amount of time with little info on how long they will remain there a visual and audio indicator has been added to signal their end.
Now, when a smoke from Astra, Brimstone or Omen has 1.5 seconds before it starts to fade, a visual cue will appear and a sound will be played. This will let everyone know that the smoke is about to disappear and you can plan accordingly. We don’t yet know what the sound is or what it looks like, but with the patch about to launch we don’t have long to wait to find out.
On paper, this sounds like a great change. When you round a corner to find a massive smoke and you have no idea how long it has been there it can be impossible to play around. It could disappear just as you are trying to reposition and leave you totally exposed. But now, with the new audio and video cues, you’ll have a small moment to make sure you are in a good position, and hopefully catch people on the other side. Of course, we will have to wait and see how this has actually been implemented to figure out if it is useful or not.
The patch notes also bring a welcome change to the sounds you hear from your allies. When you equipped certain abilities the audio would play not only for enemies as intended, but also allies close to you. The idea is that all things your team does, such as equipping a weapon or ability, should be silent. So this has now been changed to make that the case. Now the only sounds you’ll hear are from enemies equipping their abilities.
Rounding out the patch are a number of bug fixes and some changes to Premier Playoff Tournament queues. Now you’ll only have 15 minutes to queue up and be ready, then when this time expires all eligible teams will be placed into even distributed brackets that are ranked by Premier score. This means that if you have a better regular season, you’ll get some advantages in the playoffs.",https://imageio.forbes.com/specials-images/imageserve/648b1aa18347bb43c775c365/0x0.png?format=png&height=900&width=1600&fit=bounds,2023-10-17 09:00:41
https://www.forbes.com/sites/antonyleather/2023/10/17/intel-core-i5-14600k-review-better-than-amds-ryzen-7-7800x3d-and-core-i5-13600k/,Has Intel beaten AMD?,"Today Intel has launched its 14th Gen Raptor Lake refresh desktop processors, which promise better gaming and content creation performance and here we'll be taking a look at the Core i5-14600K and comparing it against 12th and 13th Gen Intel CPUs as well as those from AMD's Ryzen 7000 series, including its 3D V-cache models.
Intel's Core i5-14600K Antony Leather
At $319 the Core i5-14600K isn't much more expensive than the Core i5-13600K, which might save you $10-20, while AMD's Ryzen 7 7700X costs $329 and the Ryzen 7 7800X3D with its game-boosting 3D V-Cache costs about $80 more at $400. The key questions are whether any of the 14th Gen CPUs are worthwhile upgrades, especially as Intel's LGA1700 socket and motherboards will not see any more CPUS launches - the 14th Gen models are the final generation before Intel moves to a new CPU socket.
Intel has announced new 14th Gen CPU models Intel
Only the Core i7-14700K has received a core count increase, with its E-core count rising by four. The Core i5-14600K can hit 5.3GHz out of the box, while the Core i5-13600K hits the limit at 5.1GHz and that 200MHz boost is mirrored with other 14th Gen CPUs. The core count remains the same at six Performance cores and eight Efficient cores. Gaming performance is further boosted by the introduction of Application Performance Optimizer, which Intel claims can boost performance by up to 17%. Still, upgrading from a Core i5-13600K to a Core i5-14600K is likely a questionable move.
Intel Core i5-14600K performance
My test system included an Asus ROG Maximus Z790 Hero motherboard, liquid cooling system, 32GB of DDR5 6000MHz memory, a Solidigm P41 Plus PCIe 4.0 SSD and an Nvidia RTX 4090 graphics card. The test system was up to date with the latest Windows 11 updates and drivers as of October 10th.
Intel Core i5-14600K conclusion
The Core i5-14600K doesn't offer much more performance than the Core i5-13600K either in games or content creation, but with the latter only saving you a few bucks, it could be worth opting for over the 13th Gen CPU even just for bragging rights. Owners of the Core i5-12600K and to some extent the core i7-12700K will see big improvements, though, in both games and content creation.
Intel's Core i5-14600K and AMD's Ryzen 7 7800X3Dc Antony Leather
Compared to AMD's offerings, the Ryzen 5 7600X was usually slower - significantly so in most content creation tests, while the Ryzen 7 7700X occasionally put up a fight, but was usually either slower or a match for the Core i5-14600K. For a pure gaming system, the Ryzen 7 7800X3D is definitely worth the extra $80 or so, as long as it's paired with a sufficiently powerful graphics card.
As with all 14th Gen CPUs, their main issue if you own an even older Intel CPU or AMD Socket AM4 CPU, is that there is no upgrade path. Motherboards and memory are cheaper than AMD's Socket AM5 route, but that has another few generations of processors planned for it.
Should you buy the Core i5-14600K?
Intel has a decent allrounder in the Core i5-14600K which is much faster than the Ryzen 7 7700X and 7800X3D in most content creation tasks and a match or better than the former in games. It's a good upgrade from a 12th Gen Core i5 CPU too, but gamers would be wise to check out AMD's Ryzen 7 7800X3D. It enjoys stellar gaming performance and a better upgrade path, but with a requirement for DDR5 memory and pricier motherboards, will likely cost significantly more to own.",https://imageio.forbes.com/specials-images/imageserve/652d3a4d139c788e144820a9/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 09:00:05
https://www.forbes.com/sites/antonyleather/2023/10/17/intel-core-i9-14900k-vs-ryzen-7-7800x3d-and-core-i9-13900k-which-should-you-buy/,Intel Core i9-14900K review Vs Ryzen 7 7800X3D And Core i9-13900K,"Today Intel's new 14th Gen Raptor Lake refresh desktop processors are available and we're also allowed to lift the lid on our reviews. Here we'll be checking out the flagship - the Core i9-14900K, but you can see my review of the Core i5-14600K in a separate review.
Intel's Core i9-14900K Antony Leather
Intel has already announced the CPUs along with new features such as Application Performance Optimization (APO) - a new feature that promises to prioritise gaming software threads to boost performance by up to 17%. They also include Wi-Fi 7, Thunderbolt 5 and Bluetooth 5.4 support. Today we're focussing on content creation and gaming performance and working out whether they better options than AMD's Ryzen 7000-series or worthwhile upgrades from Intel 12th and 13th Gen models too.
Intel has announced new 14th Gen CPU models Intel
Only the Core i7-14700K has a core count increase this time, with four additional E-cores to help boost multi-threaded workloads. Aside from features such as APO, the new CPUs rely mostly on higher frequencies to boost performance. We saw the Core i9-14900K can hit 6GHz across two P-cores and 5.7GHz across P-cores in multi-threaded workloads, which are 100-200MHz higher than we've seen with the Core i-9-13900K.
The 14th Gen CPUs are compatible with both 600 and 700-series chipset motherboards that use Intel's LGA1700 processor socket, but may require a BIOS update. They were unexpected too, with Intel due to launch this generation on a new processor socket, but its desktop Meteor Lake models have been canned with a new architecture not arriving till 2024 in the form of Arrow Lake. The 14th Gen CPUs will be the last generation to land on LGA1700 making it a questionable platform to buy into. More on that in the conclusion.
Intel Core i9-14900K Performance
My test system included an Asus ROG Maximus Z790 Hero motherboard, liquid cooling system, 32GB of DDR5 6000MHz memory, a Solidigm P41 Plus PCIe 4.0 SSD and an Nvidia RTX 4090 graphics card. The test system was up to date with the latest Windows 11 updates and drivers as of October 10th.
Intel Core i9-14900K Conclusion
At $589 the Core i9-14900K is around $30 more than the Core i9-13900K, the same price as AMD's Ryzen 9 7950X but $100 less than the Ryzen 9 7950X3D. The latter didn't seem to offer consistent gaming performance with its internal structure hindering it compared to the cheaper Ryzen 7 7800X3D, which definitely gave the new Intel CPU a run for its money in games.
The Core i9-14900K was no slouch, though, keeping and bettering the AMD Ryzen 7 7800X3D in plenty of situations. Intel's advantage, though, is that the Core i9-14900K is also a beast outside of games where neither of its 3D V-Cache models are, with some exceptions such as Photoshop and Lightroom, with photo editing software often preferring AMD's architecture.
Intel's Core i9-14900K Antony Leather
Overall, if there's only $20-30 difference between the Core i9-14900K and 13900K, you may as well go for the former if only for bragging rights, but it is slightly faster in most tests. Spending more than this is questionable given the gains. As mentioned earlier, with no upgrade path, buying a new LGA1700 system is questionable unless you need a new upgrade right now and typically keep your processor four or five years before upgrading.
For a pure gaming system, AMD's Ryzen 7 7800X3D is hard to beat, but does have the added expense of pricier motherboards and memory, perhaps counteracted by a decent lifespan. If neither of these options are suitable then both AMD and Intel will have new processors in 2024, with a new socket for Intel too.
Should you buy the Core i9-14900K?
As an allrounder, though, the Core i9-14900K is a monster and is rarely out of the top three in every single graph. The same can't be said of any of AMD's Ryzen 7000 CPUs, which are great in games or multi-threaded performance, but not really both. It's only drawbacks are a non-existent upgrade path and cheaper 13th Gen CPUs that aren't much slower.","https://imageio.forbes.com/specials-images/imageserve/652e7436699d70b9cc306340/0x0.jpg?format=jpg&crop=2601,1463,x0,y331,safe&height=900&width=1600&fit=bounds",2023-10-17 09:00:04
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/unlocking-the-potential-of-ai-and-data-analytics-for-digital-advertising/,The Potential Of AI And Data Analytics For Digital Advertising,"Inc. 5000 entrepreneur, data privacy and web accessibility advocate, co-founder of Ntooitive Digital and TruAbilities.
getty
In the ever-evolving world of digital marketing, new strategies and tools are constantly being introduced. However, none have yet to have the same impact as artificial intelligence (AI), which has transformed how we approach digital marketing.
Despite some users' hesitancy, AI will revolutionize digital marketing by automating and optimizing marketing efforts. AI personalization allows marketers to expertly tailor content based on user behavior, demographics and preferences.
As AI technology advances, we can expect to see more innovative applications emerge and user adoption increase. But rather than waiting to start using AI, it is important for marketers to begin determining how they can use the technology to improve their operations.
What does AI bring to the table?
AI-powered solutions automate those marketing tasks that were previously impossible, too time-consuming or so mundane humans didn’t want to perform them. These activities include completing (in a matter of seconds) complex calculations that would take humans hours or even days and repetitive tasks such as scheduling, data analysis and responding to inbound customer inquiries.
In 2022, IBM’s Global AI Adoption Index revealed that 35% of organizations reported using AI in one way or another in their business and 42% shared they were looking into AI, its benefits and how it could be incorporated into their processes.
There are a variety of AI platforms available today that can assist with a range of tasks. Some of the most notable that I recommend include the following:
Jasper: An AI copywriting system that uses language processing and machine learning to analyze data about your target audience and make suggestions to improve copy in an original and natural-sounding way.
Pictory: A cloud-based video creation software that uses AI to automatically convert long-form text and video content into shorter videos that can easily be used on social media channels.
Grammarly: A writing assistance tool that offers grammar and spelling checks as well as suggestions for clarity, conciseness, engagement tone and readability.
Reply.io: An all-in-one solution that offers a wide range of features, including email automation, CRM integration and analytics, allowing sales teams to streamline their outreach process and personalize their communication with potential leads.
Ocoya: An AI-powered social media post creator and scheduler that analyzes user behavior and preferences, allowing users to create targeted content personalized to their audience.
How is AI changing the industry?
I believe that it’s not a matter of if businesses will begin using AI but when.
A study conducted by Stanford and MIT showed AI boosted worker productivity by 14% on average. In this way, AI can help get marketers out of the weeds of execution to focus 100% on customer connections, quality storytelling and overall business outcomes. In short, marketers will be able to get back to marketing.
This isn’t to say AI is minimizing the human element; it is instead reducing the stress on the human part, allowing workers to focus on tasks that require human expertise and creativity.
Integrating AI technology in digital marketing is here to stay. However, understanding its application for digital marketing is critical, and that begins with training your AI technologies with the appropriate information it needs to churn out the best possible output. You can’t expect to have the right output without providing the right input and training. Data sets fed into the AI program, for example, can be biased and pose issues with how outputs are interpreted.
That means being upfront with clients or end users about the use of any AI-embedded software. Even if you plan on using a helpful meeting assistant like Fireflies.ai, I would strongly encourage you to seek consent from all attendees so no one is left in the dark. Or, if your client’s next marketing campaign strategy was assisted by AI, it’s incumbent on you to disclose that. The same goes for any AI-generated images, videos or audio that have been used. Why? Transparency promotes trust, and trust builds buy-in—which is exactly what the AI industry desperately needs right now.
Conclusion
The use of AI technology in digital marketing is becoming increasingly prominent and will continue to grow exponentially. Before 2030, the global AI market is expected to be valued at over $1.5 trillion.
Marketers who embrace AI and keep up with the latest tools and strategies can increase their chances of gaining a competitive advantage, but it’s important to understand this technology and how it can best be used within your business. I believe that AI-driven digital marketing, done ethically, is the future, and these tips can provide an effective way to ensure the successful use of emerging technologies and applications.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/6453ca7bf8a3c5db599c02ff/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 09:00:00
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/how-ceos-can-change-culture-reduce-burnout-and-retain-talent/,"How CEOs Can Change Culture, Reduce Burnout And Retain Talent","Marc is CEO at Devo, a cloud-native logging and security analytics solution.
getty
One of the top challenges facing CISOs and security teams today isn’t just the security risks they are being bombarded with; it’s burnout. This directly ties into security risks in that if you're lacking staff, or if the existing staff you have is burnt out, your assets aren’t being optimally protected. In fact, as we found in our recent research with Wakefield Research, 83% of IT security professionals have seen burnout lead to errors that resulted in a security breach; 39% of them have experienced this situation more than once.
Heightened security concerns can also contribute to an even greater sense of burnout. Because keeping the network and its assets safe is such a huge responsibility, security teams are under constant pressure. According to our research, half of IT security staff point to ongoing staff shortages as a top contributor to stress. Their jobs can seem quite thankless, too, since people typically only think about security when something has gone wrong. No wonder they’re burnt out.
The conviction of Uber’s former CISO embodied many security leaders’ fears that they could be the sacrificial lambs if an attack occurred under their watch. This is an extreme example of an executive’s willful obstruction, but security professionals can’t help but wonder if they’ll somehow be blamed if a security incident gets ugly.
Burnout and mental health in this sector aren’t issues for just the security team to deal with—it needs to come from the top. Leaders in the C-suite need to understand why burnout is happening and its implications, then work to help change the culture to avoid or mitigate these issues. This is no easy feat, but the status quo can’t remain unchallenged if you want a content, retained security workforce. That’s particularly important considering the ongoing cybersecurity skills gap.
Looking At Burnout
In our survey, we aimed to dig deeper into what’s really going on with security professionals who are the lifeblood of almost any organization’s cybersecurity efforts. These are the teams who are grappling day in and day out with cyber threats and round-the-clock monitoring.
What we uncovered provided us with a wealth of insights related to burnout and mental health challenges, including:
• Eighty-five percent of IT security professionals predict they'll need to leave their company or role because of burnout, and 24% say they may leave cybersecurity entirely.
• For 77% of respondents, stress levels at work are having a direct impact on keeping private customer data safe.
In another survey we conducted, we also found that hiring, especially for the security operations center (SOC), continues to be a problem.
• Less than one-quarter of respondents say they can typically fill a vacancy in less than a month, and the average time spent filling a position is seven months.
• Fifteen percent of SOC leaders said it takes over two years to hire for a SOC role. You can imagine the impact this has on everyone else who has to pick up the slack.
On top of this, there’s an ongoing shortage of cybersecurity professionals—to the tune of 3.4 million skilled cyber pros globally. There’s no sugarcoating it: the problem is bad. So, now what?
How Leaders Can Take Action
We are living in a time when conversations about burnout and mental health are happening in a much more open fashion than ever before. That’s great because conversations are key and an important first step.
One thing we’ve seen in the last few years, especially coming out of the pandemic, is more willingness to talk about these challenges and treat them as the socioeconomic matters they are as opposed to being considered an individual’s problems—or shrouding these challenges in shame and secrecy.
Mental health must become a company priority with dedicated resources and time. From a leadership and management perspective, there are a number of ways to do this beyond what’s being done at the HR level.
For instance, enable your CISOs and other security leaders to connect and network with their peers. Make the time for them to participate in CISO dinners and networking events where they can share stories with others and get guidance from peers who are grappling with some of the very same challenges day in and day out.
It’s also important to invest in more training and education. Bad actors are continually changing and evolving their tactics, so education and training need to likewise happen regularly. This should be a key aspect of corporate directives. Training should extend across departments, not just the SOC. Giving everyone more access to cyber hygiene training can go a long way toward reducing risk and thereby limit some of the noise coming into the SOC.
Looking To External Resources For Help
Just as there’s no shame in staff admitting they’re struggling, there’s no shame in asking for outside assistance. You are not alone in this challenge. Burnout is a huge problem across many sectors, not just in security, but there are obviously many factors specific to security that make this issue especially concerning.
There are organizations like Cybermindz, a partner of ours, that are focused on improving the burnout problem and addressing mental health in cybersecurity. Such organizations will work with you to help your staff achieve and retain mental well-being in a way that fits with your unique needs.
Steer The Ship To Well-Being
Burnout isn’t a new phenomenon in the cybersecurity space, but it remains a huge problem, one that’s exacerbated by the ongoing skills gap. Organizations struggle to hire and retain staff, and there is a revolving door when it comes to SOC analysts in particular. A burned-out SOC analyst isn’t effective, and it doesn’t set a good precedent for your company’s culture, either.
Your CEO role gives you both the authority and the mandate to navigate your CISO and your SOC through the treacherous landscape of burnout. Culture change comes from the top down, so use the above suggestions to support your security leaders and their teams in every way you can.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/652da8a9e1225ef6f2760e75/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:45:00
https://www.forbes.com/sites/sap/2023/10/17/147b-as-a-service-market-is-linchpin-to-unleash-renewable-energy/,$147 Billion As-A-Service Market Is Linchpin To Unleash Renewable Energy,"As people install solar panels on their rooftops, heat pumps in their basements, and EV charging stations in their communities, electric utilities are exploring service-based opportunities slated to upend traditional energy industry models and address climate change.
Electricity can be a less expensive and more accessible energy source, particularly in regions like Europe that depend on costly imported and increasingly scarce fossil fuels. Driven by growing demand for renewable energy resources amidst supply chain uncertainties – no one in Europe has forgotten last winter’s fossil fuel crisis – energy from the electrical grid offers utilities a viable growth path in a sustainable world. In fact, the global energy-as-a-service market (EaaS) is projected to reach $147.56 billion by 2029.
Servitization helps customers and providers track the carbon footprint of households, businesses, and communities for improved energy management. getty
Servitization powers new revenue streams for utilities
Forward-looking utilities have begun experimenting with as-a-service bundles that encompass customer advice, hardware installation, financing, and energy management software. IDC researchers predicted that by 2025, one-third of competitive gentailers, meaning retail energy providers that also own power generation assets, will have set up integrated supply, efficiency, decarbonization, and electrification service portfolios, growing average profit per customer by more than 20%.
Meantime, consumers are expected to become prosumers who use and produce electricity. In some cases, customers are turning into flexumers who also provide flexibility to the grid, shifting energy usage dynamically with load demands and resource availability throughout the day and night.
“With rooftop solar panels, customers can produce electricity, potentially competing with traditional utility business models,” said Daniela Sellmann, global vice president and head of energy and utilities industries at SAP. “Servitization from connected data between infrastructure like solar panels or charging stations can help providers better distribute or store renewable energy to improve cost efficiency and grid management while generating new revenue streams.”
Trusted customer relationships from digitalization
Broadly speaking, EaaS is just emerging with subscription-based models that offer customers unique benefits. For example, a company or consumer can lease the solar panels, heat pumps, or air conditioning units they need at an affordable fee per month minus a large upfront capital investment. Analyzing a customer’s profile that includes energy usage patterns over time, utilities can personalize service-based offerings.
“EaaS can be simplified on a digital platform connecting intelligent data from onsite equipment in a home or business facility,” said Sellmann. “Similar to other industries like manufacturing, we’re applying SAP’s business solution expertise to help utilities manage servitization end-to-end, including order management and billing, as well as proactive offerings. Customers could compare and select EaaS bundles on a digital commerce site. The utility can offer complimentary equipment or loyalty coupons based on the customer’s energy usage and customer lifecycle value. This would help utilities build trusted customer relationships.”
Data transparency eases renewable energy transition
Unlike traditional utility business models, EaaS requires co-innovation with commercial customers and partners along the energy value chain. As providers add new services, they can integrate data from the solar panel or EV charging station manufacturers, banks that offer financing deals, and service technician support. A bundled offering might include everything from hardware installation through field service management aligned to the contractual agreement. Customers and providers could track the carbon footprint of households, businesses, and communities for improved energy management throughout the lifetime of a customer relationship.
“During project planning for say, a new real estate development, data transparency can help organizations and utilities capture expected energy usage from renewables. Organizations can track actual performance against corporate sustainability commitments and regulatory targets,” said Sellmann. “With the right technologies, utilities are well-positioned to help companies and consumers evolve to renewable energy.”
EaaS supports sustainable energy choices
Natural resources like sun and wind are central to global decarbonization, but they also add new dimensions of volatility to energy management. Leaders in the utility industry are rethinking how to partner with consumers, business, and the public sector to roll-out new services that people can easily find and use in this changed environment.
“We’re excited about the opportunities as society transitions to renewable energy, and utilities adopt EaaS business models,” said Sellmann. “We envision advanced technologies like generative AI will further benefit both utilities and customers. Our newly announced generative AI copilot Joule, which will be embedded into SAP solutions, directly supports the industry’s evolution to EaaS business models.”
Learn how companies in five industries are reshaping their future by focusing on service excellence: Read the IDC InfoBrief",https://imageio.forbes.com/specials-images/imageserve/6523fbf72c5009d9935ef940/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:30:00
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/theres-a-way-out-of-technical-debt-with-more-strategic-software-planning/,More Strategic Software Planning,"Carlos M. Meléndez is the COO and cofounder of Wovenware, a Maxar Company, offering AI and software development services.
getty
Software development teams, with projects ranging from deployment of a cloud-based CRM to modernizing an invoice processing application, can be overwhelmed with not only the enormity of their tasks but the clamoring coming from department heads, all thinking their project is the most critical. With faster development cycles and continuous software iterations, it can be oh-so-tempting to take the path of least resistance and simply build it in the fastest way possible with no regard for how it fits into a long-term strategy.
The resulting software may seem to do the trick and meet the immediate need. It may even help the development team look like the hero who saves the day with a quick-fix solution. However, those disparate software development projects add up, and before you know it, you're way deep in technical debt and struggling to stay afloat.
Technical debt is the costs and problems that occur when software is deployed with a quick-fix mentality without a strategic long-term plan in mind or because cost or speed was selected over quality. The end result is what is known in the industry as ""cruft""—poorly written or redundant code.
There are often many good reasons why it creeps up. Budget concerns can delay upgrades to legacy systems, limited software features and functionality might be deemed ""good enough,"" or other projects might be considered higher priority. Almost always, however, once technical debt sets in, companies struggle with higher costs for future upgrades or fixes, or they struggle with productivity losses since systems can't keep up with changing needs. In fact, according to author and speaker Martin Fowler, ""high-quality software is cheaper to produce.""
Another reason for technical debt (especially within larger enterprises) is the sheer size of the tech stack, which can be impossible to manage thanks to rogue software development projects containing duplicate solutions, outdated technologies and incompatible systems.
Designing For The Future
How can companies avoid technical debt in the first place? Consider the following best practices.
Start With Object-Oriented Design
Progressive software development practices such as object-oriented design, which organizes software design around objects (data) rather than functions and logic, enable developers to make changes to software without having to alter its structure. If object-oriented design is deployed from the outset, developers can ""tweak"" objects and optimize programs without changing the structure of the software. Some types of object-oriented programming languages include Java, Python or C++.
According to Fowler, many people in the software industry compare building software to constructing a building. While new floors or additions can be added, the fundamental structure of the building can't be changed. If the infrastructure is sound and strategic, it will allow for future upgrades.
Fowler noted that building software can be quite uncertain and changing. Customers are never really sure what they need in their software or how it will be received by customers. It needs to be adaptable without having to bulldoze the whole building whenever changes have to be made. Object-oriented design can more easily enable developers to keep software current.
Train Software Developers On Object-Oriented Design
The role of object-oriented design isn't discussed much in universities, and many software developers have been trained in traditional functional programming. Many developers know how to code in the traditional way, but I've noticed that the notion of objects seems to be relatively new. Part of the problem is that traditional coding practices are much easier to follow, and there is much more talent to go around. Yet object-oriented design is more specialized and can require more upfront time.
It's important, therefore, for universities to expose students to object-oriented programming and for software firms to hire developers who are trained in it. While it may require more upfront time, it enables them to break the program into bite-sized parts that can be tackled one object at a time to enable higher-quality software, reduced maintenance costs and more productive developers.
Clean Code As You Go
The best way to remove cruft in software projects is to avoid it in the first place. Object-oriented design, better technical oversight and quality, and more strategic planning can help to prevent it, but cruft happens. According to Fowler, removing cruft is like cleaning as you go while cooking. There's no way to avoid dirty dishes, but if you don't clean them as you go, it will be really hard to remove the crud later. By removing as much cruft as possible in programming, technical debt is reduced since quality software comprises the tech stack.
Conduct Regular Code Reviews
It's important to have software programs reviewed regularly to understand the coding logic and approach taken. An internal code quality and review council can help with this, also helping to set standard operating procedures and policies across the company. These reviews can result in the need to refactor or rewrite code or establish a means of gradual improvement.
These reviews can help to address technical debt by establishing parties or teams responsible for software development oversight.
Understand That Coding Is As Much An Art As It Is A Science
It's well-known in the software industry that programmers take great pride in their code. Every programmer likes to write software, but very few like to maintain someone else's software. By establishing a culture of quality and appealing to a programmer's pride of work, good software can be created from the beginning,
Like bills, credit card payments or expenses, debt can gradually creep up until you're drowning in it. The same goes for technical debt. The key to avoiding it is to work to pay it down, replacing traditional unwieldy programming or knee-jerk technology purchases with strategic, future-proof solutions that don't compromise quality for speedier delivery.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/63e566d03907607403b58ccf/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:30:00
https://www.forbes.com/sites/grrlscientist/2023/10/17/twenty-one-species-removed-from-the-endangered-species-act-due-to-extinction/,21 Species Removed From The Endangered Species Act Due To Extinction,"Delisting species due to their extinction signals a ‘‘wake-up call on the importance of conserving imperiled species before it’s too late’’
© Copyright by GrrlScientist | hosted by Forbes | LinkTr.ee
The Hawaiian po’ouli (Melamprosops phaeosoma) or black-faced honeycreeper, now declared extinct, ... [+] according to the U.S. Fish and Wildlife Service. (Credit: Hawai'i DLNR Division of Forestry and Wildlife via the Center for Biological Diversity.) Hawai'i DLNR Division of Forestry and Wildlife via the Center for Biological Diversity
The U.S. Fish and Wildlife Service announced Monday that they will delist 21 species from the Endangered Species Act because they are extinct. Found in 16 states and in the U.S. territory of Guam, most of these species were listed under the ESA in its early days in the 1970s and 80s, and were very low in numbers or likely already extinct when listed.
According to the statement, one mammal (the Little Mariana fruit bat from Guam), eight freshwater mussels from the Eastern Seaboard, two freshwater fish and 10 birds were declared extinct after being listed as critically endangered for decades. Eight of the delisted birds were endemic to Hawaii, with the po’ouli, Melamprosops phaeosoma, or black-faced honeycreeper, last seen in 2004, making it the most recently sighted of the delisted species.
Other Hawaiian birds may soon become extinct too: after the massive wildfires triggered by climate breakdown roared across much of Maui, the critically endangered ʻAkikiki, Oreomystis bairdi, or Kauaʻi honeycreeper, is now reported to have a population of just five individuals (ref), down from a total of 454 wild individuals reported in 2018 (ref).
“Few people realize the extent to which the crises of extinction and climate change are deeply intertwined,” said Noah Greenwald, endangered species director at the Center for Biological Diversity.
“Both threaten to undo our very way of life, leaving our children with a considerably poorer planet. One silver lining to this sad situation is that protecting and restoring forests, grasslands and other natural habitats will help address both.”
A critically endangered `Akikiki or Kaua`i Honeycreeper (Oreomystis bairdi), a Hawaiian honeycreeper ... [+] that currently has a wild population of just five individuals. (Credit: Carter Atkinson, USGS / Public domain) Carter Atkinson, USGS / Public domain
“Federal protection came too late to reverse these species’ decline, and it’s a wake-up call on the importance of conserving imperiled species before it’s too late,” said USFWS Director Martha Williams in a statement.
These extinctions highlight the importance of the ESA and efforts to conserve species before declines become irreversible. The circumstances of each extinction also underscores how human destructiveness can drive species declines and extinctions through habitat loss, shooting and other forms of exploitation, the introduction of invasive species, particularly cats, rats and pigs, and diseases.
A total of 650 species have already been listed as extinct in the United States, according to the Center for Biological Diversity, a national U.S. nonprofit wildlife conservation organization (ref). Scientists have been warning the public for decades that Earth is experiencing a mass extinction event, which is defined as the loss of more than 75% of its species (more here) in less than 2.8 million years. Unlike the previous mass extinctions, which resulted from extreme temperature changes, rising or falling sea levels and catastrophic, single events such as huge volcanic eruptions or an asteroid crashing into Earth, this current mass extinction event is solely due to human destruction.
“As we commemorate 50 years of the Endangered Species Act this year, we are reminded of the Act’s purpose to be a safety net that stops the journey toward extinction,” Ms Williams reminded us in a statement.
“The ultimate goal is to recover these species, so they no longer need the Act’s protection.”
The ESA has generally been highly effective and is credited with saving 99% of listed species from extinction. Thus far, more than 100 species of plants and animals have been delisted based on recovery or reclassified from endangered to threatened based on improved conservation status, and hundreds more species are stable or improving thanks to the collaborative actions of Tribes, federal agencies, state and local governments, conservation organizations and private citizens.
The final rule to delist these 21 species from the ESA due to extinction will be published today, 17 October 2023, in the Federal Register and will go into effect in 30 days.
Two Reprieves
Two endangered species were granted a temporary reprieve. After the USFWS proposed to delist 23 species in September 2021, public comments informed them that two species may yet cling to life. The first, a Hawaiian perennial mint, Phyllostegia glabra var. lanaiensis, may possibly still be found in newly identified, potentially suitable habitats for the species.
Chris Pague of the Nature Conservancy holds a museum specimen of Ivory Billed woodpecker ... [+] (Campephilus principalis) found in 1894 on the Aucilla River in Jefferson County Florida. The bird, part of the Denver Museum's collection, is probably extinct after the last confirmed sighting in 1944. Recently there was a reported rediscovery of the Ivory-billed woodpecker at the Cache River National Wildlife Refuge in Arkansas. (Credit: RJ Sangosti / The Denver Post via Getty Images) Denver Post via Getty Images
The other species, the ivory-billed woodpecker, Campephilus principalis, also known by a variety of common names including the Lord Gawd Bird, because it was the largest woodpecker species in the United States, is still under review. Despite all the squabbling amongst bird watchers (and even a few professional ornithologists) over the past 20 years, along with hundreds of thousands of hours of exhaustive searching by hundreds of skilled observers, nothing definitive has been reported, making it likely this dramatic bird is truly gone forever. This showy species is likely extinct due to uncontrolled shooting and rampant logging of old-growth bottomland hardwood forests and old-growth temperate coniferous forests of the Southeastern United States and Cuba, where it lived.
SHA-256: 9ab94921e06b203a216cb219d873f92ea4083642075e2e0be632939cd42949aa
Socials: Bluesky | CounterSocial | LinkedIn | Mastodon | MeWe | Post.News | Spoutible | SubStack | Tribel | Tumblr | Twitter","https://imageio.forbes.com/specials-images/imageserve/652e67b2aa2b82de4b6d76e5/0x0.jpg?format=jpg&crop=1140,640,x0,y0,safe&height=900&width=1600&fit=bounds",2023-10-17 08:16:52
https://www.forbes.com/sites/forbestechcouncil/2023/10/17/generative-ai-a-new-era-for-data-analysis-and-interpretation/,Generative AI: A New Era For Data Analysis And Interpretation,"CEO and cofounder of Unacast, a location data and insights platform.
getty
Data has always been the compass guiding businesses. But the modern landscape demands more than just data; it craves insights. Despite the presence of sophisticated dashboards and graphs, the interpretation often remains confined to those with a deep understanding of the data.
I see how our clients, who make site selections, streamline retail operations and make crucial investment decisions, have a significant need for clear and comprehensible narratives to make better decisions.
This is where I believe generative AI will be a game-changer, bridging the gap between numbers and narratives.
The Need For Narratives In Data Interpretation
A dashboard filled with graphs, pie charts and scatter plots can be daunting. Each visualization presents data points, trends and patterns. But how many viewers can understand the stories lying within these numbers? While a seasoned analyst might see a trend or anomaly, for many others, it remains just a collection of numbers and lines.
This is where narratives come in. Stories and contextual descriptions can breathe life into numbers, making them more relatable and understandable.
Generative AI: The Bridge Between Numbers And Narratives
Generative AI, with its capability to produce content, can transform data into rich and helpful narratives.
I see three ways in which the technology will improve data interpretation.
1. Bridging The Knowledge Gap
Not everyone is a data enthusiast. When a dashboard is shared across departments, it's essential that everyone, from retail operations and research to investment and executive teams, understands the insights. Generative AI can convert data patterns into plain, relatable language.
For businesses, particularly in the retail sector, choosing a location is pivotal. Site selection depends on many factors: demographic data, traffic patterns, competition analysis and more. While data points can provide these numbers, what if generative AI could narrate a story?
For instance, instead of just presenting a chart with foot traffic, the AI could generate an insight like, ""The location of your interest sees 20% higher foot traffic during weekends and has two colleges within a three-mile radius, making it an ideal location for businesses targeting the youth demographic.""
Such narratives can significantly enhance decision-making, offering companies a clearer view of potential site benefits and challenges.
2. Providing Enriched Context
Beyond just the current data, understanding trends often requires historical or comparative context.
AI, with its ability to pull from extensive databases, can compare current data with past trends, providing a more comprehensive view.
For instance, if there are changes in trade areas, foot traffic or demographic changes, AI can compare those observations with similar past events and generate insights about potential causes or implications.
3. Boosting Time Efficiency
One of the primary roles of a data analyst or researcher is not just to analyze data but to explain it. Crafting meaningful interpretations takes time.
Generative AI can assist in this process, automating the generation of basic insights and summaries so the numbers and graphs actually have a meaning—also for those not as data savvy.
I have seen numerous times how analysts and researchers, highly skilled and highly paid professionals, spend too much time writing and creating slides for their executive team. Generative AI allows these professionals to dedicate more time to strategy, planning and diving into more complex analysis—what they excel at.
Generative AI As A De Facto Part Of The Workflow
While generative AI holds tremendous potential, it's essential to understand that it's a tool designed to complement human skills, not replace them. A machine can provide narratives in a different and more efficient way than a person, but the deeper, nuanced understanding and strategic application of data will always require the human touch of a data analyst or a researcher looking to inform their teams and colleagues.
The future of data interpretation, especially in sectors like site selection, retail and investment, isn't just about presenting numbers. It's about narrating stories that resonate, insights that inform and analyses that drive action.
Generative AI, with its capability to craft narratives from data, prompts the question of not if but how soon we can integrate generative AI into our data interpretation workflows.
Forbes Technology Council is an invitation-only community for world-class CIOs, CTOs and technology executives. Do I qualify?",https://imageio.forbes.com/specials-images/imageserve/5ffe0a25c386c343ec2b0a9b/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:15:00
https://www.forbes.com/sites/erikkain/2023/10/17/spider-man-2s-glowing-ps5-reviews-gloss-over-one-glaring-problem/,Spider-Man 2’s Glowing PS5 Reviews Gloss Over One Glaring Problem,"Spider-Man 2 Credit: Sony
When I reviewed Spider-Man back in 2018 when it slung its way onto PS4, I gave it a near-perfect score of 9.5 out of 10. It was just so close to a perfect video game, only held back by a few annoying things like forced stealth missions that were super tedious and had no business disrupting the flow of the good stuff.
MORE FROM FORBES 'Spider-Man' (2018) PS4 Review: The Good, The Bad And The Spidey
Reviewing the sequel five years later on the PS5, my colleague Paul Tassi is slightly less generous, giving Spider-Man 2 a still very respectable 8.5/10 (though it’s the kind of score that makes fanboys very, very angry). Paul’s biggest complaint about the game is its open world, which is filled with mini-games and busy work.
He writes:
Early on, you will immediately get overloaded with “points of interest” and little minigames that the game has doubled down on from last time. There are “science” games that have you splicing genes and decoding molecules and using bee drones to shoot down bad bees in order to increase the good bee population. Miles solves very basic puzzles from his reformed Prowler uncle conveniently using PS5 pressure triggers. Miles, at one point, also has a rhythm matching beat segment. But otherwise, it’s pretty standard fare, similar crimes to last time, fighting arenas, minibase takeovers.
IGN docks another .5 off its score, for an 8/10, writing:
As a sequel in a spectacular series, Marvel’s Spider-Man 2 is both blessed and cursed. Its story of two Spider-Men is a great time and a Spidey fan’s dream to play through as comic book pages are brought to life, elegantly walking the tightrope between light humour and heavier themes. Meanwhile, Insomniac refines a successful formula of combat and web-swinging without revolutionising either in major ways, making them comfy and familiar with just enough new tweaks and abilities to elevate them to fun new heights. The part that feels like it actually needed a radical rethinking is the open world of New York City, which has been made bigger but not better, with an exhausting checklist of mostly repetitious side activities.
I added emphasis to that last part, because it’s pretty much the exact thing that Paul said about the game. And frankly, “an exhausting checklist of mostly repetitious side activities” sounds, well, exhausting!
This is a complaint I’ve seen echoed by many reviewers, in one form or another, and it’s something that I think is important. I loved the first game, but the last thing a sequel needed was more open-world stuff to do, more boxes to tick, more repetitive objectives to overcome.
I know that for many people, having more is always better. Ah, we have a 20-hour game, but if we add 20 more hours of filler content and add in more open-world to explore, it will be better! Think Starfield with its thousands of empty, barren planets. More! More! More!
No!
Stop it. Please.
The only thing we need more of is quality. We need denser, higher-value content. Yes, we still need some open-world activities and the fun base-takeovers and all that, but we don’t need more of them. We need compelling stories and missions, awesome boss fights, great dialogue and performances. And yes, it sounds like all of that is in Spider-Man 2 as well, which is great, but does it really need to be baked into such a thick crust of filler?
Again, I’m sure the game is awesome and I’m sure I’m going to have a blast playing it just like I did when I played the first one. The combat was great, the web-slinging traversal was absolutely fantastic and I had a blast with the story. But there were tedious moments and some goofy missions that irked me. And just in general, I think I’ve really burned out on boilerplate open worlds.
I like to see innovation in this space like we got with Elden Ring or Ghost Of Tsushima or the new Zelda games. A game like Spider-Man 2 doesn’t need a bigger NYC, it just needs Queens and it doesn’t need a ton of mini-games, it needs a good story and top-notch combat and traversal mechanics.
I’m a big champion of semi-open world / semi-linear game design, because I think it’s fun to have some freedom but also that narrative momentum that drives the action and story along with some, but not too many, distractions (which are, let’s face it, Peter Parker’s weakness). And because time is precious and we don’t all have time to do all this open-world busy work.
I’ll end by noting that this is definitely personal preference and if you like all that other stuff, cool. You win because by and large, the industry is all about more and rarely about better. You win, I lose. C’est la vie.",https://imageio.forbes.com/specials-images/imageserve/652da5fc80d944ebd7d288d4/0x0.jpg?format=jpg&height=900&width=1600&fit=bounds,2023-10-17 08:15:00
https://techxplore.com/news/2023-10-material-devices-electricity-efficiently.html,New material could allow devices to turn wasted heat into useful electricity more efficiently,"This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
a) Scans of energy transfer for Cu 12 Sb 4 S 13 and Cu 14 Sb 4 S 13 at different temperatures. b) Electrical resistivity as a function of temperature. The anomaly at ≈90 K is related to a cubic-to-tetragonal phase transition. c) Calculated scan of energy transfer for the copper ions in Cu 14 Sb 4 S 13 as a function of temperature, based on the MD simulations. Credit: Advanced Materials (2023). DOI: 10.1002/adma.202306088
Scientists have discovered a way to design materials that improve energy efficiency, in a breakthrough that could help the fight against climate change, make manufacturing greener, and could even take the hassle out of charging your smart watch.
The thermoelectric energy harvesting innovation identifies a new way of harnessing previously untapped sources of waste heat and converting it into electricity.
Thermoelectric materials can turn temperature differences into electricity. Researchers at Reading have discovered that if a thermoelectric material has moving ions inside cages, the heat flow will be reduced. This keeps the hot and cold sides at different temperatures, while electrons can flow from the hot to the cold side, so electricity can be made.
Usually, materials with moving ions break down when making electricity like this. But the material described in the new study published in Advanced Materials doesn't break down easily and could allow devices and generators to turn wasted heat into useful electricity more efficiently than in current designs.
Dr. Paz Vaqueiro, of the University of Reading's Department of Chemistry, led the study. She said, ""This discovery has the potential to help address the global energy crisis and contribute to combating climate change. Approximately two-thirds of the energy generated worldwide is wasted in the form of heat. Converting even a fraction of this waste heat back into useful electricity would help to ensure a sustainable energy supply and reduce carbon emissions.
""The U.K. is predicted to need twice as much electricity in 2050 as it did in 2020. The potential of thermoelectric technology has been known about for a number of years, but currently generators are expensive and not very efficient. Using new thermoelectric technology that is cheaper to run and more efficient could help turn the 48 TWh of waste heat the U.K. produces every year into electricity and could help us on the path to reach net zero.""
Watches and cars
Not only could the breakthrough have positive consequences for the fight against climate change, but it could also make a difference to some of the devices and machinery we use daily.
At present, thermoelectric devices and generators are expensive and only used in niche applications (such as the Voyager probes sent by NASA to explore space beyond the solar system). However, the development of new thermoelectric generators could have a big impact on wearable devices, such as smart watches, where the current technology relies on batteries that require regular recharging. Eliminating this requirement, by harvesting body heat to generate power, could not only be more convenient but also make wearable tech more reliable in critical applications, such as monitoring the health of vulnerable or elderly patients.
The development could be of use for the motor industry, allowing carmakers to develop thermoelectric generators that use waste heat to charge batteries of plug-in electric or hybrid vehicles, increasing their efficiency. Factories producing glass, steel or cement, which are currently carbon-intensive and generate large amounts of waste heat, could also benefit from the technology.
More information: Shriparna Mukherjee et al, Beyond Rattling: Tetrahedrites as Incipient Ionic Conductors, Advanced Materials (2023). DOI: 10.1002/adma.202306088 Journal information: Advanced Materials",https://scx2.b-cdn.net/gfx/news/2023/new-material-could-all.jpg,2023-10-17 09:02:35
https://techxplore.com/news/2023-10-fire-inhibiting-nonflammable-gel-polymer-electrolyte.html,"Fire-inhibiting, nonflammable gel polymer electrolyte for lithium-ion batteries","This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Schematic image depicting the principles of operation of non-flammable gel electrolytes. Credit: Ulsan National Institute of Science and Technology
A collaborative research team has achieved a milestone in battery technology. Their achievement in developing a non-flammable gel polymer electrolyte (GPE) is set to revolutionize the safety of lithium-ion batteries (LIBs) by mitigating the risks of thermal runaway and fire incidents.
The research was led by Professor Hyun-Kon Song in the School of Energy and Chemical Engineering at UNIST, Dr. Seo-Hyun Jung from Research Center for Advanced Specialty Chemicals at Korea Research Institute of Chemical Technology (KRICT), and Dr. Tae-Hee Kim from the Ulsan Advanced Energy Technology R&D Center at Korea Institute of Energy Research (KIER). The results have been published in ACS Energy Letters.
In the past, the potential flammability of LIBs has raised significant concerns, especially in electric vehicles, where fire hazards pose a serious threat to underground parking lots. Addressing this critical issue, the research team has successfully developed a groundbreaking non-flammable polymer semi-solid electrolyte, offering a promising solution to mitigate battery fires.
Conventionally, non-flammable electrolytes have heavily relied on the incorporation of flame retardant additives or solvents with exceptionally high boiling points. However, these methods often resulted in a considerable decrease in ion conductivity, compromising the overall performance of the electrolyte.
In their breakthrough research, the team introduced a trace amount of polymer, creating a semi-solid electrolyte. This novel approach dramatically increased the lithium ion conductivity by 33% compared to existing liquid electrolytes. Moreover, the pouch-type batteries incorporating this non-flammable semi-solid electrolyte exhibited a remarkable 110% improvement in life characteristics, effectively preventing unnecessary electrolyte reactions during the formation and operation of the solid-electrolyte interphase (SEI) layer.
Nail penetration of 650 mAh pouch cells of NCM811||graphite. (a to c) Voltage and temperature profiles (d to f). Credit: Ulsan National Institute of Science and Technology
The key advantage of this innovative electrolyte lies in its exceptional performance and non-combustibility. By suppressing radical chain reactions with fuel compounds during the combustion process, the polymer semi-solid electrolyte effectively inhibits the occurrence of battery fires. The research team demonstrated the excellence of the developed polymer by quantitatively analyzing its ability to stabilize and suppress radicals.
Jihong Jeong (School of Energy and Chemical Engineering, UNIST) said, ""The interaction between the polymerized material inside the battery and volatile solvents allows us to effectively suppress radical chain reactions. Through electrochemical quantification, this breakthrough will greatly contribute to understanding the mechanism of non-flammable electrolytes.""
Co-first author Mideum Kim, a master student in the School of Energy and Chemical Engineering at UNIST and the Korea Research Institute of Chemical Technology (KRICT), further confirmed the exceptional safety of the battery itself through various experiments. The team's comprehensive approach included applying the non-flammable semi-solid electrolyte to pouch-type batteries, ensuring the evaluation of electrolyte non-combustibility extended to practical battery applications.
""The research team's multidisciplinary composition, involving electrochemistry from UNIST, polymer synthesis from the KRICT Research Center for Advanced Specialty Chemicals, and battery safety testing by the Ulsan Advanced Energy Technology R&D Center at Korea Institute of Energy Research (KIER), has been instrumental in achieving this breakthrough,"" stated Professor Song. ""The use of non-flammable semi-solid electrolytes, which can be directly incorporated into existing battery assembly processes, will accelerate the future commercialization of safer batteries.""
More information: Jihong Jeong et al, Fire-Inhibiting Nonflammable Gel Polymer Electrolyte for Lithium-Ion Batteries, ACS Energy Letters (2023). DOI: 10.1021/acsenergylett.3c01128 Journal information: ACS Energy Letters",https://scx2.b-cdn.net/gfx/news/2023/fire-inhibiting-nonfla.jpg,2023-10-17 08:29:04
https://techxplore.com/news/2023-10-soft-packaged-portable-glove.html,"Researchers develop soft-packaged, portable rehabilitation glove","This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Multi-scene application and life assistance function of flexible rehabilitation glove robot. Credit: USTC
Researchers from the University of Science and Technology of China (USTC) of the Chinese Academy of Sciences (CAS) have proposed a soft-packaged and portable rehabilitation glove with fine movement training. It is expected to serve the fine motor rehabilitation and daily living assistance for tens of millions of patients with hand dysfunction around the world.
The technology was described in an article published on Oct. 5 in Nature Machine Intelligence.
Patients with hand dysfunction can recover through repeated and continuous hand movement training. Soft-packaged rehabilitation gloves are lightweight and flexible.
However, because a flexible body is prone to large deformation, which is not conducive to motion perception, and currently available gloves are not conducive to portability, most soft-packaged rehabilitation gloves can only achieve therapeutic movement based on open loop control. This makes the precise rehabilitation of fine motor skills of the hand still challenging. Assisted daily tasks on a poststroke individual. Credit: Nature Machine Intelligence (2023). DOI: 10.1038/s42256-023-00728-z
In this study, the researchers designed a bionic finger sleeve structure that integrates smooth movement and accurate perception by integrating 15 bending sensors and 10 shape-memory-alloy actuators.
Due to the shape memory alloy with high work-to-weight ratio and integrated design, the flexible rehabilitation glove robot weighs only 490 grams and has the ability to work independently.
By imitating the folded skin of the back of the finger, the research team proposed a bionic design featuring a non-uniform stiffness flexible finger sleeve, which reduces the interference of finger sleeve movement on the sensing system and achieves stable and accurate finger state perception.
Further, they proposed a multi-modal fine action rehabilitation training method to achieve portable, accurate and safe rehabilitation training for patients with hand dysfunction. Clinical trials have preliminarily verified the advantages of the portable, low-cost soft-packaged rehabilitation glove robot in fine sports rehabilitation and daily living assistance.
More information: Mengli Sui et al, A soft-packaged and portable rehabilitation glove capable of closed-loop fine motor skills, Nature Machine Intelligence (2023). DOI: 10.1038/s42256-023-00728-z Journal information: Nature Machine Intelligence",https://scx2.b-cdn.net/gfx/news/hires/2023/researchers-develop-so-2.jpg,2023-10-17 08:36:39
https://techxplore.com/news/2023-10-unveils-partially-disordered-phase-li-.html,Study unveils a new partially disordered phase in Li- and Mn-rich cathode materials,"This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Characterization of the as-synthesized L5M85, L10M70 and L15M55. a, The partially disordered spinel phase as an intermediate between the DRX and fully ordered spinel structures. b, XRD patterns of L5M85, L10M70 and L15M55, refined based on the rock salt structure. The lattice parameter (a) and weighted profile R factor (R wp ) are shown in the figure. c–e, SEM images of L5M85 (c), L10M70 (d) and L15M55 (e) after shaker milling with carbon (scale bar = 500 nm). f, STEM image and EDS mapping of the elements O, Mn and Ti in as-synthesized L5M85 (scare bar = 500 nm). g, SAED pattern of as-synthesized L5M85. Credit: Nature Energy (2023). DOI: 10.1038/s41560-023-01375-9
Lithium-ion (Li-ion) batteries are among the most widespread battery technologies worldwide, due to their light weight, high energy densities, easy fabrication process, rapid charging times and other advantageous properties. Identifying strategies that could boost their performance further or facilitate their future upscaling has been the focus of numerous recent studies.
One of the proposed approaches for improving the performance of Li-ion batteries entails identifying new promising cathode materials that can be made from metals that are abundant in nature. So far, Li-ion cathodes have been found to be in some ways ineffective, as phase transitions inside them can elicit what is known as voltage hysteresis, which adversely impacts battery capacity.
Researchers at University of California Berkeley and other institutes across the United States have recently unveiled an unconventional phase transformation in cathode materials that are rich in Li and manganese (Mn). The new phase they uncovered, outlined in a paper in Nature Energy, could enable the creation of highly performing batteries with Mn-based cathodes.
""We want to create high energy density cathode materials for Li-ion that can be made from earth-abundant metals, unlike current cathode materials which contain both Co and Ni,"" Gerbrand Ceder, co-author of the paper, told Tech Xplore. ""This would mean Li-ion batteries that are less expensive, thereby helping their market penetration in EVs, grid, etc.""
Mn is an Earth abundant metal, as it is already widely produced in large quantities for various real-world applications. This metal has a good redox voltage and could thus work well in Li-ion batteries with high energy densities.
These advantageous properties are what ultimately inspired Ceder and his colleagues to try to create cathodes containing a high amount of Mn. Some studies have already explored the potential of Mn-rich cathodes, yet most results gathered so far have been unsatisfactory.
""Previous efforts to use Mn-based cathodes have suffered from the fact that Mn has a tendency to move around and rearrange the crystal structure that you start from,"" Ceder said. ""We decided to turn that into an advantage and start from a material (DRX) that when cycled would turn into a structure that is very good for storing lithium. So, it was a form on inverse design: We knew the material would transform, so we just made sure it transformed to something very good for storing lithium.""
The new phase that Ceder and his colleagues unveiled in their study, dubbed the delta (δ) phase, has a unique, unconventional structure. This structure resembles that of spinels, a class of ceramics typically marked by an ordered internal organization.
""The phase we uncovered is related to the known spinel structure, but only forms that structure in very small domains, which are anti-phased with each other,"" Ceder explained. ""Spinel is a structure that is known to store lithium ions, but its commercialization has been problematic, as it undergoes a destructive phase transformation when cycled in a battery. ""
In the phase observed by the researchers, small domains of spinel act independently. This prevents the destructive transition previously observed in spinel-based cathodes during a battery's operation, instead allowing batteries to retain a good capacity for several battery cycles.
""We can make 'complex' structures in-situ while we charge and discharge a battery and these complex structures can have excellent performance,"" Ceder said. ""A cathode material based on only Mn- and Ti-oxide precursors can potentially be very inexpensive and could reduce the cost of Li-ion batteries by 40–50%. This would make batteries for EV and grid much cheaper.""
In their experiments, the researchers were able to identify the kinetic mechanisms underpinning their Li- and Mn-rich cathodes' transition to the so-called delta phase. This could pave the way for the development of more promising cathode materials with similar characteristics.
Initial tests performed by Ceder and his colleagues yielded very promising results, as the phase they unveiled was found to enable both a high energy density and good battery cyclability. In the future, their work could thus also encourage other teams to explore the potential of cathodes rich in Mn using similar experimental strategies.
""We believe that we can make even higher energy density materials,"" Ceder added. ""Also, we are working on accelerating this transformation to delta so one doesn't have to wait for 10–20 cycles to get the best performance out of one's battery.""
More information: Zijian Cai et al, In situ formed partially disordered phases as earth-abundant Mn-rich cathode materials, Nature Energy (2023). DOI: 10.1038/s41560-023-01375-9 Journal information: Nature Energy
© 2023 Science X Network",https://scx2.b-cdn.net/gfx/news/hires/2023/study-unveils-a-new-pa-1.jpg,2023-10-17 07:40:01
https://techxplore.com/news/2023-10-method-ai-1.html,A method to interpret AI might not be so interpretable after all,"This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
A study finds humans struggle to understand the outputs of formal specifications, a method that some researchers claim can be used to make AI decision-making interpretable to humans. Credit: Bryan Mastergeorge, Massachusetts Institute of Technology
As autonomous systems and artificial intelligence become increasingly common in daily life, new methods are emerging to help humans check that these systems are behaving as expected. One method, called formal specifications, uses mathematical formulas that can be translated into natural-language expressions. Some researchers claim that this method can be used to spell out decisions an AI will make in a way that is interpretable to humans.
MIT Lincoln Laboratory researchers wanted to check such claims of interpretability. Their findings point to the opposite: Formal specifications do not seem to be interpretable by humans. In the team's study, participants were asked to check whether an AI agent's plan would succeed in a virtual game. Presented with the formal specification of the plan, the participants were correct less than half of the time.
""The results are bad news for researchers who have been claiming that formal methods lent interpretability to systems. It might be true in some restricted and abstract sense, but not for anything close to practical system validation,"" says Hosea Siu, a researcher in the laboratory's AI Technology Group. The group's paper, currently available on the arXiv preprint server, was accepted to the 2023 International Conference on Intelligent Robots and Systems held earlier this month.
Interpretability is important because it allows humans to place trust in a machine when used in the real world. If a robot or AI can explain its actions, then humans can decide whether it needs adjustments or can be trusted to make fair decisions. An interpretable system also enables the users of technology—not just the developers—to understand and trust its capabilities. However, interpretability has long been a challenge in the field of AI and autonomy. The machine learning process happens in a ""black box,"" so model developers often can't explain why or how a system came to a certain decision.
""When researchers say 'our machine learning system is accurate,' we ask 'how accurate?"" and 'using what data?' and if that information isn't provided, we reject the claim. We haven't been doing that much when researchers say 'our machine learning system is interpretable,' and we need to start holding those claims up to more scrutiny,"" Siu says.
Lost in translation
For their experiment, the researchers sought to determine whether formal specifications made the behavior of a system more interpretable. They focused on people's ability to use such specifications to validate a system—that is, to understand whether the system always met the user's goals.
Applying formal specifications for this purpose is essentially a by-product of its original use. Formal specifications are part of a broader set of formal methods that use logical expressions as a mathematical framework to describe the behavior of a model. Because the model is built on a logical flow, engineers can use ""model checkers"" to mathematically prove facts about the system, including when it is or isn't possible for the system to complete a task. Now, researchers are trying to use this same framework as a translational tool for humans.
""Researchers confuse the fact that formal specifications have precise semantics with them being interpretable to humans. These are not the same thing,"" Siu says. ""We realized that next-to-nobody was checking to see if people actually understood the outputs.""
In the team's experiment, participants were asked to validate a fairly simple set of behaviors with a robot playing a game of capture the flag, basically answering the question ""If the robot follows these rules exactly, does it always win?""
Participants included both experts and nonexperts in formal methods. They received the formal specifications in three ways—a ""raw"" logical formula, the formula translated into words closer to natural language, and a decision-tree format. Decision trees in particular are often considered in the AI world to be a human-interpretable way to show AI or robot decision-making.
The results: ""Validation performance on the whole was quite terrible, with around 45 percent accuracy, regardless of the presentation type,"" Siu says.
Confidently wrong
Those previously trained in formal specifications only did slightly better than novices. However, the experts reported far more confidence in their answers, regardless of whether they were correct or not. Across the board, people tended to over-trust the correctness of specifications put in front of them, meaning that they ignored rule sets allowing for game losses. This confirmation bias is particularly concerning for system validation, the researchers say, because people are more likely to overlook failure modes.
""We don't think that this result means we should abandon formal specifications as a way to explain system behaviors to people. But we do think that a lot more work needs to go into the design of how they are presented to people and into the workflow in which people use them,"" Siu adds.
When considering why the results were so poor, Siu recognizes that even people who work on formal methods aren't quite trained to check specifications as the experiment asked them to. And, thinking through all the possible outcomes of a set of rules is difficult. Even so, the rule sets shown to participants were short, equivalent to no more than a paragraph of text, ""much shorter than anything you'd encounter in any real system,"" Siu says.
The team isn't attempting to tie their results directly to the performance of humans in real-world robot validation. Instead, they aim to use the results as a starting point to consider what the formal logic community may be missing when claiming interpretability, and how such claims may play out in the real world.
This research was conducted as part of a larger project Siu and teammates are working on to improve the relationship between robots and human operators, especially those in the military. The process of programming robotics can often leave operators out of the loop. With a similar goal of improving interpretability and trust, the project is trying to allow operators to teach tasks to robots directly, in ways that are similar to training humans. Such a process could improve both the operator's confidence in the robot and the robot's adaptability.
Ultimately, they hope the results of this study and their ongoing research can better the application of autonomy, as it becomes more embedded in human life and decision-making.
""Our results push for the need to do human evaluations of certain systems and concepts of autonomy and AI before too many claims are made about their utility with humans,"" Siu adds.
More information: Ho Chit Siu et al, STL: Surprisingly Tricky Logic (for System Validation), arXiv (2023). DOI: 10.48550/arxiv.2305.17258 Journal information: arXiv
This story is republished courtesy of MIT News (web.mit.edu/newsoffice/), a popular site that covers news about MIT research, innovation and teaching.",https://scx2.b-cdn.net/gfx/news/2023/a-method-to-interpret.jpg,2023-10-16 17:27:04
https://techxplore.com/news/2023-10-algorithm-quality-electricity-local-generation.html,New algorithm to help control quality of electricity in local generation systems,"This article has been reviewed according to Science X's editorial process and policies . Editors have highlighted the following attributes while ensuring the content's credibility:
Prototype. Credit: Ildar Idrisov
With the new stage of energy transition in progress, the key tendency of power market development today is distributed power generation, which is characterized by decentralization, smart energy systems, involvement of consumers, and a higher share of renewable energy sources. In distributed generation systems, electricity comes from a number of local power objects, instead of one large station. For example, home owners who use solar panels can sell excess electricity back into the grid.
A fundamental role in these systems belongs to inverters that convert the generated electricity into the alternating current with specific frequency. In Europe and CIS countries, it equals 50 Hz. Researchers from Skoltech presented an algorithm for inverters that aims to control the quality of electricity injected to the main grid. The results are spotlighted by IEEE and have been published as part of 2023 IEEE Belgrade PowerTech.
""Inverters are programmed with mathematical functions and equations with certain coefficients,"" explains the leading author of the research Ilya Veretennikov, an engineer in the Energy Center's Smart Grid Laboratory.
""If grid parameters remain the same, it is enough to adjust the coefficients just once. Energy systems with distributed (local) generation constantly change (for example, if some market participants stopped selling their electricity), coefficients need to be recalculated. It is difficult to evaluate whether the coefficients are calculated correctly or not, but it is necessary to ensure the quality of electricity, which must meet the standard. Otherwise, it cannot be injected into the grid.""
Ilya Veretennikov working on lab experiments. Credit: Ildar Idrisov
The research team developed an algorithm for a controller, which would recalculate coefficients automatically controlling the quality of electricity injected in the main grid. Using data on calculated voltage, the controller generates a control signal, which ensures the alternating current frequency (50 Hz) without any distortions.
""During our research, we came up with a grid model and a more detailed inverter model. With their help, we checked different algorithms of the controller, their stability and efficiency. We proceeded with validating our results through experimental data. We plugged a real inverter in the grid, modeling a case with a local load, when a part of energy is transferred to the grid. In the lab, we work with low-power inverters, which are suitable for home users. We have great equipment for a detailed simulation in real time. It helps simulate any grid with any number of inverters and any parameters,"" adds Veretennikov.
According to the authors, they see the potential of the research and of the local power generation as a whole in those remote regions which, despite many clear days, still exploit unsustainable power energy sources. The research team is planning to fully automate the algorithm following the plug-and-play concept.
""Configuration is a burden for every end user. As of now, not all parameters are automatically configured, but we are working on the algorithm to make it fully automatic as a modern device that can be used right after connecting it to the computer without manual configurations,"" concludes Veretennikov.
More information: Ilya Veretennikov et al, Proportional Multiresonant Controller with Automatic Gains Adjustment for Grid-Connected Inverters, 2023 IEEE Belgrade PowerTech (2023). DOI: 10.1109/PowerTech55446.2023.10202893",https://scx2.b-cdn.net/gfx/news/2023/new-algorithm-to-help-1.jpg,2023-10-16 17:14:03
https://www.reuters.com/innovation/article/tsmc-results/tsmc-third-quarter-profit-to-slide-30-focus-on-how-much-growth-to-come-idUSKBN31H07N,"TSMC third-quarter profit to slide 30%, focus on how much growth to come","TAIPEI (Reuters) - Taiwan Semiconductor Manufacturing Co Ltd is expected to report a 30% slump in third-quarter profit on Thursday but analysts predict robust growth next year as the chip industry emerges from its current downturn.
FILE PHOTO: A logo of Taiwanse chip giant TSMC can be seen in Tainan, Taiwan December 29, 2022. REUTERS/Ann Wang/File Photo
The likely decline in profit also reflects a strong performance last year, when the company was still riding high on pent-up post-pandemic demand.
The world’s largest contract chipmaker is set to report net profit of T$195.9 billion ($6 billion) for July-September - its second straight quarter of profit decline, according to an LSEG SmartEstimate drawn from 19 analysts. SmartEstimates give greater weighting to forecasts from analysts who are more consistently accurate.
Revenue for the quarter came in at around $17 billion, according to TSMC figures, down 20% from a year earlier and roughly the middle of the company’s forecast range.
Global demand for semiconductors began to weaken in the second half of last year, but analysts say inventories at smartphone and computer makers are running down and restocking demand is expected to pick up.
Given that, much of Thursday’s focus will be on TSMC’s outlook for the fourth quarter and beyond.
Morgan Stanley analysts have forecast 10% revenue growth for the fourth quarter but also said in a research note that “guidance may surprise to the upside,” citing strong demand for high-end chips used in artificial intelligence as one factor.
The AI boom has helped drive up the price of shares in Asia’s most valuable company, with TSMC’s Taipei-listed stock having surged 23% so far this year.
An LSEG SmartEstimate puts TSMC’s 2024 revenue growth at around 22%.
Sources have said, however, that TSMC has been nervous about customer demand and told its major suppliers to delay the delivery of high-end chip-making equipment, although they added that suppliers expect the delay to be short-term.
Some analysts are also reining in their optimism somewhat.
Fubon Securities expects a slow start to next year for TSMC, with 10% growth in the first quarter, predicting order cancellations towards the year end and mild restocking demand. In particular, it is concerned that Apple, a major customer, may revise down its orders.
“We think the market consensus is still too bullish,” it said in a research note.
The company is due to report at 0600 GMT on Thursday.
($1 = 32.2290 Taiwan dollars)",https://s2.reutersmedia.net/resources/r/?m=02&d=20231017&t=2&i=1647931385&w=1200&r=LYNXMPEJ9G044,2023-10-17 03:55:49
https://www.reuters.com/innovation/article/canada-batteries/canada-to-give-belgiums-umicore-up-to-c1-billion-for-new-battery-components-plant-idUSKBN31G1PF,Canada to give Belgium's Umicore up to C$1 billion for new battery components plant,"OTTAWA (Reuters) - Canada and the province of Ontario will give up to C$1 billion to a unit of Belgium’s Umicore to help it build a plant that will produce components for electric vehicle batteries, Ottawa said on Monday.
The facility, in the Ontario town of Loyalist Township, will manufacture cathode active materials and precursor cathode active materials, federal Innovation Minister Francois-Philippe Champagne said in a statement.
The plant - the first of its kind in North America - will initially employ 600 people and have a battery materials production capacity of 35 gigawatt hours annually.
Canada, home to a large mining sector for minerals such as lithium, nickel and cobalt, wants to woo firms involved in all levels of the electric vehicle (EV) supply chain via a multibillion-dollar green technology.
The Umicore plant is due to be built in stages and could be worth C$2.7 billion when fully completed. Canada will invest up to C$551.3 billion with Ontario adding up to C$424.6 billion.
The full project has the potential to produce enough battery materials to support the production of over 800,000 EVs per year, Champagne said.",https://s4.reutersmedia.net/resources_v2/images/rcom-default.png,2023-10-16 19:03:02
https://www.reuters.com/innovation/article/overstock-com-jat/hedge-fund-jat-pushes-overstock-com-to-mull-options-filing-idUSKBN31G1KE,Hedge fund JAT pushes Overstock.com to mull options -filing,"NEW YORK (Reuters) - Hedge fund JAT Capital on Monday urged Overstock.com to consider selling certain assets and overhauling its management and compensation, warning it may seek board seats if its suggestions are ignored, a regulatory filing on Monday showed.
The hedge fund, run by John Thaler, owns a 9.1% stake in Overstock.com, and made its demands public in the filing after sending a letter outlining its wishes to the company last week.
Overstock’s stock price climbed nearly 9% after the filing.
Overstock.com, which spent $21.5 million to buy certain intellectual property assets from Bed Bath & Beyond a few months ago, did not immediately respond to a request for comment.
JAT is not traditionally an activist investor but its filing signaled mounting frustration with the company and left open the option of pursuing strategies like running a proxy contest.
JAT also wants the company to emphasize stock option participation and develop a business plan with financial objectives, the filing said.
“The Reporting Persons may consider to seek Board representation to the extent the above recommendations have not been explored, pursued and executed satisfactorily.”",https://s4.reutersmedia.net/resources_v2/images/rcom-default.png,2023-10-16 16:56:50
https://techcrunch.com/2023/10/17/ray-ban-meta-review/,Ray-Ban Meta sunglasses have 'influencer' written all over them,"Ray-Ban Meta sunglasses have ‘influencer’ written all over them The companies have maintained a slim and light design, while rendering their predecessor obsolete with Facebook and Instagram livestreaming
This is a review-in-progress. More soon!
Somewhere between the Ray-Ban Meta and Meta Quest 3 sits an ideal mixed-reality headset. It’s slim, light, offers hand tracking and passthrough and livestreams video when the moment calls for it. It’s designed to be worn inconspicuously outdoors, until the time comes for content capture.
The Meta Quest Ray-Ban is a fantasy at the moment — albeit one that points in the direction of where its makers think this is all headed. Presently, the Ray-Ban Meta and Meta Quest 3 are very different devices, with little in the way of overlap, beyond being head-worn products with built-in sensors.
The Meta Quest 3 is a mixed-reality headset designed to be worn exclusively indoors. It’s light, perhaps, compared to other headsets of its ilk, but wearing the thing while walking around outside frankly sounds a bit miserable. That’s precisely the use case the Ray-Ban Meta was designed for: freedom of movement outside the house that’s designed to go (mostly) unnoticed.
Just prior to writing this, I slipped a pair on, before the JFK airport mobility cart drove my sciatica-ridden ass to the gate. I would say the pair was inconspicuous but for the fact that I was wearing a pair of sunglasses indoors. Well, that and the extremely necessary recording lighting that flashes on so you can’t creep shoot folks without their knowledge. Here’s some of that video:
We got our first glimpse of the Ray-Ban Meta at a briefing just ahead of the recent Connect conference. I was genuinely impressed by the industrial design the join team came up with. Most folks would hard-pressed to distinguish the charger from a standard Ray-Ban classic eyeglass case. It’s a little thicker than some, sure. A bit heavier. More rigid. But the team was able to make surprisingly few concessions.
There are a lot of clever touches here. In the place of a snap is a ring. Open the case and it glows green when fully charged and orange when not. The orange starts blinking when the battery is low. Space has been maximized inside. The battery sits directly beneath the glasses’ folded temples. In front of this is a dock with two charging pins that lie flush with a pair of contact pads hidden on the underside of the glasses’ bridge, held in place with magnets and a small tab.
The USB-C port is located on the outside bottom of the case, allowing it to sit on its back while being charged. Directly above this on the case’s rear is the Bluetooth pairing button. The case is slimmer than the last gen and can be carried in a pocket comfortably.
Meta says the glasses get “up to” four hours on a charge, while the case gets a total of eight charging cycles, for a grand total of 36 hours. As the company notes, “Battery life varies by use, configuration, settings and many other factors.” That’s the case with all tech, of course, but I did notice that video is a power drainer.
The companies really leaned into the style side of things here (not a bad decision when designing tech meant to be worn on the body). There are two main designs for the glasses. There’s the classic Wayfarer (which is probably what you think of when you think of sunglasses) and the new Headliner (not dissimilar from Wayfarer, but significantly more rounded on the top and bottom).
According to Meta, there are 150 design combos possible, when you factor in all of the different design options, including frame color, style and lenses (including sunglasses, clear, prescription, transitions and polarized).
The temples are thicker than most sunglasses — to be expected, seeing as how they contain the speakers and other components (there’s a transparent option, if you want to see for yourself) — but again, the designers have done a good job keeping size down, all things considered. And again, while slightly heavier that a standard pair of Wayfarers (50.8 g vs. 44 g), you can wear them comfortably all day if you want to (or at least the less than four hours the battery lasts).
There’s a touchpad on the outside of the left temple. Swiping back and forth will adjust the volume (other features can be customized in-app). It also doubles as a control panel for live streaming, since you likely don’t want to futz with your phone or keep using the wake word. A tap can check Instagram or Facebook comments and viewers in real-time. The capture button sits next to the hinge on the left temple
There are a pair of small circular modules on the end pieces. They look identical, for the sake of symmetry, but serve very different — albeit related — functions. On the top right (when facing the glasses) is the 12-megapixel camera. On the top left is an LED that turns on to alert people in your vicinity that you’re recording.
When covered, the glasses send an audio alert that they’ve stopped recording. This is to avoid people sticking a piece of electrical tape to hide the light. Meta says they didn’t hear of any specific examples of this happening, but they almost certainly got that feedback. Again, privacy is paramount for a device like this, especially since it’s something that most people around you don’t know exists. When the battery is low, you’ll get a spoken alert and the light will blink orange and turn red right before shutting down. The light will blink white when receiving a call, do a single flash when taking a photo and glow steadily when recording.
When pairing, it flashes blue, going solid when connected. The pairing process is pretty straight forward. You’ll need to download the Meta View app, choose between Meta Ray-Ban and Ray-Ban Stories and allow bluetooth to connect . Images and video will save to the glasses’ 32GB of internal storage (that’s roughly 500 photos or 100 videos at the maximum 30 seconds apiece). You’ll need to tap “Import” inside the app to connect via WiFi and download the contents to your phone. You can also set it up to auto import via settings.
Once everything is paired, put the glasses on and open either Facebook or Instagram to livestream. Tap the plus icon and it will bring you to the livestream screen. Your phone’s camera is, understandably, the default, but double pressing the capture button will switch over to the glasses. Livestreaming is probably the single biggest killer app Ray-Ban Stories was missing.
There are barely visible down firing speakers on the bottom of the temple tips. When I first tried the speakers in an otherwise silent room, they sounded surprisingly loud and clear. They’re open-ear speakers, rather than bone conduction, which has its pluses and minuses. Bone conduction tends to be quite quiet but does a decent job with ambient noise, since it’s arriving at your eardrums through a different method.
As expected, I had to turn up the volume quite a bit among the airport din. I would recommend them for quieter environments, where possible, but obviously that isn’t always an option. Sound is integral to the headphones, beyond music listening. For instance, there’s an audible shutter click when you take a picture.
There are on-board microphones as well, which listen for the “hey Meta” wake word. Voice certainly makes sense on a device like this. It can be used to take a picture, stop and start video and adjust volume (turns out voice is kind of an annoying way to do the latter). You can also ask the glasses for the time, weather and how much battery is left. You can also ask Alexa style-questions, and Meta AI will attempt to answer. That’s currently only available here in the U.S. through an open beta.
The price starts at $299 for standard lenses. Polarized run $329 and transitions $379. Prescription lenses are on a sliding scale. The price will almost certainly be a deterrent for many — and understandably so. Ultimately, you need to ask yourself how much value a face-worn camera will bring to your life. If you make a living livestreaming, it may make sense. It’s a lot to pay however, for sheer novelty.
It’s worth noting that future updates will bring more value to the device, including sign translation (through voice) and the ability to identify landmarks in front of you. One can see the future of head-worn computing laid out in front of your face — though it’s still going to be a while before we get there.",https://techcrunch.com/wp-content/uploads/2023/09/Meta-Ray-Ban-Stories-06.jpg?w=1200,2023-10-17 07:01:44
https://techcrunch.com/2023/10/17/ray-ban-meta-review/,Ray-Ban Meta sunglasses have 'influencer' written all over them,"Ray-Ban Meta sunglasses have ‘influencer’ written all over them The companies have maintained a slim and light design, while rendering their predecessor obsolete with Facebook and Instagram livestreaming
This is a review-in-progress. More soon!
Somewhere between the Ray-Ban Meta and Meta Quest 3 sits an ideal mixed-reality headset. It’s slim, light, offers hand tracking and passthrough and livestreams video when the moment calls for it. It’s designed to be worn inconspicuously outdoors, until the time comes for content capture.
The Meta Quest Ray-Ban is a fantasy at the moment — albeit one that points in the direction of where its makers think this is all headed. Presently, the Ray-Ban Meta and Meta Quest 3 are very different devices, with little in the way of overlap, beyond being head-worn products with built-in sensors.
The Meta Quest 3 is a mixed-reality headset designed to be worn exclusively indoors. It’s light, perhaps, compared to other headsets of its ilk, but wearing the thing while walking around outside frankly sounds a bit miserable. That’s precisely the use case the Ray-Ban Meta was designed for: freedom of movement outside the house that’s designed to go (mostly) unnoticed.
Just prior to writing this, I slipped a pair on, before the JFK airport mobility cart drove my sciatica-ridden ass to the gate. I would say the pair was inconspicuous but for the fact that I was wearing a pair of sunglasses indoors. Well, that and the extremely necessary recording lighting that flashes on so you can’t creep shoot folks without their knowledge. Here’s some of that video:
We got our first glimpse of the Ray-Ban Meta at a briefing just ahead of the recent Connect conference. I was genuinely impressed by the industrial design the join team came up with. Most folks would hard-pressed to distinguish the charger from a standard Ray-Ban classic eyeglass case. It’s a little thicker than some, sure. A bit heavier. More rigid. But the team was able to make surprisingly few concessions.
There are a lot of clever touches here. In the place of a snap is a ring. Open the case and it glows green when fully charged and orange when not. The orange starts blinking when the battery is low. Space has been maximized inside. The battery sits directly beneath the glasses’ folded temples. In front of this is a dock with two charging pins that lie flush with a pair of contact pads hidden on the underside of the glasses’ bridge, held in place with magnets and a small tab.
The USB-C port is located on the outside bottom of the case, allowing it to sit on its back while being charged. Directly above this on the case’s rear is the Bluetooth pairing button. The case is slimmer than the last gen and can be carried in a pocket comfortably.
Meta says the glasses get “up to” four hours on a charge, while the case gets a total of eight charging cycles, for a grand total of 36 hours. As the company notes, “Battery life varies by use, configuration, settings and many other factors.” That’s the case with all tech, of course, but I did notice that video is a power drainer.
The companies really leaned into the style side of things here (not a bad decision when designing tech meant to be worn on the body). There are two main designs for the glasses. There’s the classic Wayfarer (which is probably what you think of when you think of sunglasses) and the new Headliner (not dissimilar from Wayfarer, but significantly more rounded on the top and bottom).
According to Meta, there are 150 design combos possible, when you factor in all of the different design options, including frame color, style and lenses (including sunglasses, clear, prescription, transitions and polarized).
The temples are thicker than most sunglasses — to be expected, seeing as how they contain the speakers and other components (there’s a transparent option, if you want to see for yourself) — but again, the designers have done a good job keeping size down, all things considered. And again, while slightly heavier that a standard pair of Wayfarers (50.8 g vs. 44 g), you can wear them comfortably all day if you want to (or at least the less than four hours the battery lasts).
There’s a touchpad on the outside of the left temple. Swiping back and forth will adjust the volume (other features can be customized in-app). It also doubles as a control panel for live streaming, since you likely don’t want to futz with your phone or keep using the wake word. A tap can check Instagram or Facebook comments and viewers in real-time. The capture button sits next to the hinge on the left temple
There are a pair of small circular modules on the end pieces. They look identical, for the sake of symmetry, but serve very different — albeit related — functions. On the top right (when facing the glasses) is the 12-megapixel camera. On the top left is an LED that turns on to alert people in your vicinity that you’re recording.
When covered, the glasses send an audio alert that they’ve stopped recording. This is to avoid people sticking a piece of electrical tape to hide the light. Meta says they didn’t hear of any specific examples of this happening, but they almost certainly got that feedback. Again, privacy is paramount for a device like this, especially since it’s something that most people around you don’t know exists. When the battery is low, you’ll get a spoken alert and the light will blink orange and turn red right before shutting down. The light will blink white when receiving a call, do a single flash when taking a photo and glow steadily when recording.
When pairing, it flashes blue, going solid when connected. The pairing process is pretty straight forward. You’ll need to download the Meta View app, choose between Meta Ray-Ban and Ray-Ban Stories and allow bluetooth to connect . Images and video will save to the glasses’ 32GB of internal storage (that’s roughly 500 photos or 100 videos at the maximum 30 seconds apiece). You’ll need to tap “Import” inside the app to connect via WiFi and download the contents to your phone. You can also set it up to auto import via settings.
Once everything is paired, put the glasses on and open either Facebook or Instagram to livestream. Tap the plus icon and it will bring you to the livestream screen. Your phone’s camera is, understandably, the default, but double pressing the capture button will switch over to the glasses. Livestreaming is probably the single biggest killer app Ray-Ban Stories was missing.
There are barely visible down firing speakers on the bottom of the temple tips. When I first tried the speakers in an otherwise silent room, they sounded surprisingly loud and clear. They’re open-ear speakers, rather than bone conduction, which has its pluses and minuses. Bone conduction tends to be quite quiet but does a decent job with ambient noise, since it’s arriving at your eardrums through a different method.
As expected, I had to turn up the volume quite a bit among the airport din. I would recommend them for quieter environments, where possible, but obviously that isn’t always an option. Sound is integral to the headphones, beyond music listening. For instance, there’s an audible shutter click when you take a picture.
There are on-board microphones as well, which listen for the “hey Meta” wake word. Voice certainly makes sense on a device like this. It can be used to take a picture, stop and start video and adjust volume (turns out voice is kind of an annoying way to do the latter). You can also ask the glasses for the time, weather and how much battery is left. You can also ask Alexa style-questions, and Meta AI will attempt to answer. That’s currently only available here in the U.S. through an open beta.
The price starts at $299 for standard lenses. Polarized run $329 and transitions $379. Prescription lenses are on a sliding scale. The price will almost certainly be a deterrent for many — and understandably so. Ultimately, you need to ask yourself how much value a face-worn camera will bring to your life. If you make a living livestreaming, it may make sense. It’s a lot to pay however, for sheer novelty.
It’s worth noting that future updates will bring more value to the device, including sign translation (through voice) and the ability to identify landmarks in front of you. One can see the future of head-worn computing laid out in front of your face — though it’s still going to be a while before we get there.",https://techcrunch.com/wp-content/uploads/2023/09/Meta-Ray-Ban-Stories-06.jpg?w=1200,2023-10-17 07:01:44
https://techcrunch.com/2023/10/17/youtube-is-launching-new-playback-and-creator-focused-features/,YouTube is launching new playback and creator-focused features,"YouTube is rolling out a new set of features for better mobile-based playback, song search, and creator-friendly tools
The company is making it easier for users to increase playback speed. With the new update, users can just tap and hold anywhere on the player to increase the speed to 2x, and once they reach the part they want they can let go.
Additionally, when you are seeking the video to go back or go forward during the scrub bar and want to go back to the place you started, YouTube will indicate the point with a vibration.
Google is also introducing a feature called stable volume — which can be toggled through the playback settings menu — to reduce sudden differences in the volume in a clip. This feature was launched in test mode on several platforms in July.
In August, YouTube started testing a feature that let users find a song by humming the tune. With the new set of updates, users will be able to find a song by playing, singing, or humming.
For YouTube users accessing the service through the company’s mobile and tablet apps, the company is rolling out a screen lock feature to avoid accidental taps. To unlock the screen, you can tap and hold onto the lock icon.
YouTube said today that it is combining the Library tab and account page into a new “You” tab. The new tab will include previously watched videos, playlists, downloads, and purchases along with account and channel information. The page also allows you to quicky switch between accounts.
If you spend hours on YouTube, you must have heard creators say “hit like and subscribe” many times. Now, the company is turning this into a feature. So when the person in the video utters these words, YouTube will highlight these buttons with a visual cue in sync. YouTube didn’t specify if users will be able to turn this off, as constantly looking at the animation that highlights the subscribe button could be distracting.
Additionally, to help creators engage with viewers, YouTube will rotate top comments continuously. Plus, for new videos, there will be a new animation and live video count update for the first 24 hours.
Google is also updating the video description menu with additional details on smart TVs. Earlier, the details page took over the whole screen. Now things such as the video description, comments, and the subscribe button will be tucked in a neat vertical menu accessible when you tap the video title.
With these new features, YouTube has shifted its focus back to playback and search enhancements. In the last few months, the company has been concentrating on rolling out AI-powered and Short-focused features such as summaries for videos, Collab and Q&As, and TikTok-styled short video feed. Last month, it added creator tools to help them find the right music for their videos and easily add dubs.",https://techcrunch.com/wp-content/uploads/2023/07/lockmode_2x.gif?w=790,2023-10-17 13:00:48
https://techcrunch.com/2023/10/17/k2-space-is-building-a-mass-abundant-power-rich-future-for-space-exploration/,K2 Space is building a power-rich future for space exploration based on the premise that bigger is better,"Los Angeles-based K2 Space is accelerating its path to orbit with fresh venture funding, new defense contracts and a satellite architecture that will be capable of delivering staggering power levels in a single launch.
The company is taking what cofounder and CEO Karan Kunjur described in a recent interview as a “pretty significant contrarian bet against the market.” The premise of the bet goes something like this: the space industry is governed by a single calculus – cost per kilogram of mass. The dollar cost of mass affects how spacecraft are designed, how scientific missions are evaluated, and even how entire businesses are planned.
Although the cost per kilogram of mass has declined with the rise of new launch capabilities, like SpaceX’s pioneering work in rocket reusability, spacecraft and mission designers still face egregious mass constraints. As a result, spacecraft have gotten smaller, lighter and more compact. But that doesn’t come without significant trade-offs in power, payload mass and payload volume.
K2 Space is moving in the opposite direction. The company emerged from stealth in March with ambitious plans to design and build massive satellite buses, at never-before-seen costs. Their hypothesis is that next-gen launch vehicles like SpaceX’s Starship will fundamentally change the cost per kilogram paradigm that has ruled with an iron fist for so long – but that to take advantage of this future, we must start planning for it now.
The company is developing two satellite buses: a one ton payload mass bus called Mega, which can fly on launch vehicles operating today, and a much larger Giga satellite for up to 15 tons of payload, that’s built for super-heavy rockets. These products can “lower the barrier to accessing power, aperture or mass for any application in space,” cofounder and CTO Neel Kunjur explained in a recent interview. (The two cofounders are brothers.)
But it isn’t just the size of the satellites that’s remarkable. Revealed in more detail today, K2 has designed them to operate in a stackable, scalable architecture, so that customers can essentially purchase and operate a high-powered constellation at super low costs. K2’s products could unlock higher energy orbits, like medium Earth orbit, for companies at price points that are currently prohibitive.
The satellites were designed to maximize launch vehicle fairing volume, too. Stacked together, 10 Mega-class satellites will be able to fit in a Falcon 9 fairing, which would deliver 200 kilowatts of power in a single launch. Even more staggering, 40 Megas will be able to fit into a Starship, which would deliver 800 kW of power in one go.
For reference, ViaSat’s ViaSat-3 telecommunications satellite, one of the most powerful satellites every deployed in orbit, has 25 kilowatts of power and cost over half a billion to build. In contrast, a 10 Mega-class constellation would cost less than $150 million.
Neel, who spent over five years at SpaceX developing avionics for SpaceX’s Dragon spacecraft, said that delivering both high power and high packing density drove the satellite design.
“Not really many people other than SpaceX are maximizing the capability of Falcon 9,” he explained. “When SpaceX launches a Starlink mission these days, they’re using every kilo of launch capacity they possibly can on Falcon 9. Everybody else is leaving a lot on the table. We wanted to deliver the highest power density per launch possible. At 200 kilowatts deployed per Falcon launch, there’s no other satellite bus manufacturer that comes even close to the amount of power that we can deploy in a single Falcon 9 launch.”
Toward mass abundance
The engineering challenges are substantial. Although there are some major upsides of no longer having to mass optimize every single component – things can be more robust, or made out of heavier (but substantially cheaper) materials – K2 is essentially redesigning a satellite from scratch, even down to the reaction wheel, one of the most fundamental and settled aspects of satellite design (K2’s is one of the largest ever designed). Liberated from entrenched design paradigms that assume mass constraint, around 85% of the spacecraft is vertically integrated, if only because some of the technology doesn’t exist to support the novel satellite architecture.
For that reason, attracting top talent has been key. The company recently brought on Rafael Martinez, who led the design of the original Hall thruster for SpaceX’s Starlink constellation and was director of propulsion engineering at Apollo Fusion, to lead the design of what the Karan said will be the highest-powered Hall thruster to be deployed in space by a factor of four. Other notable hires include Ashrith Balakumar, who led the avionics engineering team for SpaceX’s Dragon spacecraft, and Drew Miller, a senior mechanical engineer with experience at SpaceX, Kitty Hawk and Maxar.
K2 has tripled in size since March – growing from a team of 6 to 18 – and is looking to expand even further to around 40 over the next six months. All this is leading up to the company’s first satellite launch in 2025, for which the launch partner hasn’t yet been announced. Investor interest in K2’s mission has not abated either: the company has also raised an additional $7 million in capital, bringing its total capital raised to $16 million, including an $8.5 million seed round announced earlier this year.
Among the new investors is Alpine Space Ventures, a European fund led by a number of early SpaceX engineers, including Catriona Chambers – who happens to be the person who hired Neel at SpaceX.
The company has also landed three contracts from the U.S. Department of Defense on behalf of different end users, reflecting some interesting traction from defense for K2’s larger platform. The company was awarded the three contracts, which have a total potential contract value of $4.5 million, over the last three months.
“There’s been a real push for resilience in our defense architecture, and historically resilience has come in the form of proliferation, where that proliferation required you going smaller, cheaper, faster,” Karan said. “But unfortunately, there’s a lot of use cases and a lot of end users that actually would like more power than what those small sats offer. The key thing that most of the end users we’ve talked to so far are excited by is being able to have proliferation without sacrificing performance.”
To give a taste of the kind of future the Kunjur brothers are building toward, they described a “dream mission”: launching four or five Mega-class satellites to establish a geostationary communications network around Mars. Scientific missions are deeply constrained in how much power they can send back because they depend on aging Mars orbiters – not to mention any future missions to the Red Planet.
But that’s just the beginning. A future in space unconstrained by mass is one that has only just begun to be explored.
“Across almost every application, because we’ve been forced to either mass constrain our payloads or volume constrain our payloads or even power constraint them, what we can actually do has been constrained to a large degree,” Karan said. “As a result, the types of missions you can do are more limited.”","https://techcrunch.com/wp-content/uploads/2023/10/K2-Space-Factory-Satellite-Photo.jpg?resize=1200,800",2023-10-17 13:00:39
https://techcrunch.com/2023/10/17/swshs-new-app-helps-you-maintain-your-friendships-through-polls-and-games/,Swsh's new app helps you maintain your friendships through polls and games,"Ever since BeReal became popular among Gen Zers, giving them a platform to be their true selves rather than scrolling through heavily edited content, there’s been a rise in social apps that cater to this craving for authenticity and connection. Swsh is a social app founded by Gen Zers, for Gen Zers that wants to help strengthen true friendships – as opposed to surface-level relationships with followers.
The app is essentially a mobile version of the “Most Likely To” game, which has existed for ages and rarely fails to liven up a party. It’s also great as an icebreaker when meeting someone new or catching up with a group of friends, whether virtually or in person.
Every night at 9 PM, Swsh users answer five daily “Most Likely To” questions with their friends, voting on who is the most likely to “dance with a stranger” or “party till 3 AM then work out at 6 AM.” Users collect superlatives and points for how many votes they earn. There’s also a comment section where users can debate about which friend best fits the description.
The polls are powered by Open AI’s ChatGPT and curated by Swsh’s team of four, Swsh co-founder and CEO Alexandra Debow explained to TechCrunch.
In addition to polls, Swsh will gradually test and roll out more in-app experiences “that feel natural to people you meet IRL and as a way to make it fun to keep in touch with them after,” Debow said, mentioning capabilities like posting photos after an event is over and the ability to share voice memos.
Swsh launched its public beta in September, with an official launch slated for early 2024. The app is currently only available on iOS devices. An Android version is coming soon.
The app’s name stems from “See you again, somewhere, somehow,” a saying Debow coined after growing tired of saying goodbye to so many friends. As a Canadian born and raised in Hong Kong, Debow met a lot of expats who never stayed for long; she became frustrated with having to communicate with long-distance friends over social media.
“Social media confuses friendship with followers, and text chains back and forth feel draining,” Debow said. “We believe that a middle ground exists: an effortless and enjoyable way to build relationships you care about.”
Debow is also the founder of Alive Vibe, a virtual events marketplace; The Entrepre女ers Network, a network for female entrepreneurs; and The Why Wait? Women Entrepreneurship Collective, the first Gen-Z-led women entrepreneurship event in Shanghai.
Swsh has two other co-founders– Weilyn Chong (COO) and Nathan Ahn (CTO). Chong is an economics and computer science student at Princeton University and a board member at the Nasdaq Entrepreneurial Center. She also co-founded the Entrepre女ers Network. Ahn studied computer science at Yale and was a software engineer intern at Meta.
Debow and Chong spoke at TechCrunch Disrupt last month.
The startup recently secured $1.7 million in pre-seed funding, which will help build the product as well as bring on engineering hires. The round was co-led by Stellation Capital and MaC Venture Capital, along with notable angels Glenn Solomon, Cory Levy (Z Fellows), Cyril Berdugo, Patrick de Picciotto, Ansh Nanda and Richard Li, among others.
“Consumer social has a history of being reinvented by young people: Facebook, Instagram and Snap are just a few canonical examples,” Peter Boyce, founder of Stellation Capital, said. “If everything works out, a young founding team like Swsh could earn their spot next, inventing a new consumer platform that shapes how we spend time with the people we care for in our lives.”","https://techcrunch.com/wp-content/uploads/2023/10/swsh-Founders.jpg?resize=1200,995",2023-10-17 13:00:23
https://techcrunch.com/2023/10/17/ai-generating-music-app-riffusion-turns-viral-success-into-4m-in-funding/,AI-generating music app Riffusion turns viral success into $4M in funding,"Nearly a year ago, developers Seth Forsgren and Hayk Martiros released a hobby project called Riffusion that could generate music using not audio but images of audio. It sounds counterintuitive (no pun intended), but it worked — my colleague Devin Coldewey got the rundown here.
While their approach had its limitations, Riffusion netted Forsgren and Martiros a lot of attention — not exactly surprising given the curiosity (and controversy) surrounding AI-generated music tech. Millions of people tried Riffusion, according to Forsgren, and the platform was cited in research papers published out of Big Tech companies including Meta, Google and TikTok parent ByteDance.
Some of the attention came from investors as well, it seems.
This year, Forsgren and Martiros decided to commercialize Riffusion, which is now being advised by the musical duo The Chainsmokers and has closed a $4 million seed round led by Greycroft with participation from South Park Commons and Sky9.
Riffusion is also launching a new, free-to-use app — an improved version of last year’s Riffusion — that allows users to describe lyrics and a musical style to generate “riffs” that can be shared publicly or with friends.
“[The new Riffusion] empowers anyone to create original music via short, shareable audio clips,” Forsgren told TechCrunch in an email interview. “Users simply describe the lyrics and a musical style, and our model generates riffs complete with singing and custom artwork in a few seconds. From inspiring musicians, to wishing your mom ‘good morning!,’ riffs are a new form of expression and communication that dramatically reduce the barrier to music creation.”
Matiros and Forsgren met at Princeton while in undergrad, and have spent the last decade playing music together in an amateur band. Forsgren previously founded two venture-backed tech companies, Hardline and Yodel, while Matiros joined drone startup Skydio as one of its first employees.
Forsgren says that he and Matiros were inspired to scale Riffusion by the potential they see in generative AI tools to connect people through creativity.
“The pandemic gave us all a lot more time at home — and led me to learn to play the piano,” Forsgren said. “Music has a great power to connect us in times of isolation. Generative AI is a new and rapidly changing space, and Riffusion aims to harness this technology to deliver a fun new instrument — one that empowers everyone to actively create music throughout their lives.”
The upgraded Riffusion is powered by an audio model that the Riffusion team — which is six people strong, including Forsgren and Matiros — trained from scratch. Like the model behind the original Riffusion, the new model’s fine-tuned on spectrograms, or visual representations of audio that show the amplitude of different frequencies over time.
Forsgren and Martiros made spectrograms of music and tagged the resulting images with the relevant terms, like “blues guitar,” “jazz piano” and so on. Feeding the model this collection “taught” it what certain sounds “look like” and how it might re-create or combine them given a text prompt (e.g. “lo-fi beat for the holidays,” “mambo but from Kenya,” “a folksy blues song from the Mississippi Delta,” etc.).
“Users describe musical qualities through natural language or even recording their own voice, as a method of prompting the model to generate unique outputs,” Forsgren explained. “We think the product will empower music producers and audio engineers to explore new ideas and get inspiration in a totally new way.”
Here’s a sample made using Riffusion’s ability to record a voice with the prompt “punk rock anthem, male vocals, energetic guitar and drums”:
But what, you might ask, about the potential for copyright infringement?
Increasingly, homemade tracks that use generative AI to conjure familiar sounds that can be passed off as authentic, or at least close enough, have been going viral. Just last month, a Discord community dedicated to generative audio released an entire album using an AI-generated copy of Travis Scott’s voice — attracting the wrath of the label representing him.
Music labels have been quick to flag AI-generated tracks to streaming partners like Spotify and SoundCloud, citing intellectual property concerns — and they’ve generally been victorious. But there’s still a lack of clarity on whether “deepfake” music violates the copyright of artists, labels and other rights holders.
Forsgren was quick to note that the new and improved Riffusion wasn’t trained to recognize famous artist names or songs — and, he says, can’t replicate them.
“The product isn’t built to produce deepfakes and doesn’t recognize famous artist names in its prompts,” he said. “Instead, it lets users craft personal messages and catchy hooks using the app. It’s not uncommon to have a riff you create get stuck in your head and find yourself singing along to it all day.”
There’s no clear monetization strategy — yet. For now, Forsgren and Martiros say that they’re focusing on growing Riffusion’s team and developing complementary new generative AI products.
But Forsgren also hinted at working more closely with artists like The Chainsmokers to see how the tech could be used in their creative processes.
“It’s very early days for generative music. Models such as Google’s MusicLM, Facebook’s MusicGen, and Stability’s Stable Audio are exciting tools in the space,” Forsgren said. “But Riffusion stands out as one of the first to enable users to generate lyrics in their music via a fun and accessible website.”","https://techcrunch.com/wp-content/uploads/2023/01/GettyImages-1352457907.jpg?resize=1200,752",2023-10-17 13:00:14
https://techcrunch.com/2023/10/17/nirvana-nabs-57m-to-make-ai-inroads-into-commercial-trucking-insurance/,Nirvana nabs $57M to make AI inroads into commercial trucking insurance,"Nirvana Insurance — an insurance startup taking a new approach to insurance products for commercial fleets using artificial intelligence, telematics, internet-of-things technology and 15 billion miles of trucking data to calculate risk models — is taking on something else: new funding.
The startup has raised an all-equity Series B of $57 million, money that it will be using to continue expanding its big data platform, for hiring, and to continue growing its business, which is initially targeting the trucking industry. The funding comes the heels of seeing its business grow 30-fold since launching in 2022.
Lightspeed Venture Partners is leading the round, with General Catalyst and Valor Equity Partners also participating. We understand that the round doubles the company’s valuation to over $350 million post-money.
The problem that Nirvana is aiming to solve is that for the most part, fleets of trucks are run as small businesses with very thin margins.
“Ninety percent of fleets have less than 50 trucks,” Rushil Goel, the CEO and co-founder of San Francisco-based Nirvana, said in an interview. With rising fuel costs, he added, “they are really struggling to stay alive.”
Add to that the mandatory headache of insurance.Typically it can cost $15,000-$20,000 annually to insure a fleet, a rate that is on the rise, he said. On top of that, getting quotes and policies sorted out can often take weeks, and filing and getting claims paid out can keep drivers off the road for weeks.
Nirvana’s solution aims to speed up activity across all those different areas: faster quotes, at rates that are right-sized to the customer in question, with better tools to claim against the policy if needed.
It does this by way of tapping into the many sensors that are already built into trucks, using the data amassed from them to build new models for pricing.
“We leverage billions of data points from sensors on on trucks to build risk models,” Goal said. “No one else is doing this.”
A typical truck might look more cumbersome and basic on first glance to the average consumer, but in fact there is a growing trend for more technology in these vehicles that is not unlike the developments underway in the consumer automotive space.
A federal mandate in 2017 required all trucks to have electronic logging devices installed. New vehicles are equipped with these, and in the U.S. the number of heavy trucks now with these numbers 18-20 million, he estimated, with the global number much bigger; and in the lighter truck market, those devices are in about 30-40% of all vehicles. Typically, used trucks in both categories get retrofitted, or are swapped out on an average of a 10-15 year refreshment cycle. All this points to a definite market today, plus one that is growing.
“There is a wave of IoT adoption in the trucking space,” Goel said.
Alongside its risk models, Nirvana has also built tools for its customers to both file and make claims using images and other data from dashboard cameras.
Sensors and cameras these days on trucks are able to pick up a number of characteristics in the game of driving: do drives brake too hard, do they signal before changing lanes, are they floating between lanes a lot, are they using their horn a lot, and where are they honking?
Goel said that the data it collects from these cameras and sensors is adequate enough that it’s built an AI based on it that helps calculate premiums for its customers; those whose driving Nirvana’s AI deems “safe” can get discounts of up to 20%.
“Commercial fleets today produce a tremendous amount of data, yet most insurers still insist on a cookie-cutter approach to insurance that does nothing to incentivize safety. Nirvana is bringing insurance into the modern era, changing how the industry considers risk,” said Raviraj Jain, a partner at Lightspeed Venture Partners, in a statement. “Their incredible growth is a testament to the opportunities AI and data analytics are opening up in fleet insurance and beyond, especially given that the IoT fleet management market is expected to continue growing.”","https://techcrunch.com/wp-content/uploads/2022/11/GettyImages-1357459564.jpg?resize=1200,799",2023-10-17 12:53:02
https://techcrunch.com/2023/10/17/hoxton-ventures-shoots-for-the-big-time-luring-bryan-gartner-from-khosla-ventures/,"Hoxton Ventures shoots for the big time, luring Bryan Gartner from Khosla Ventures","Everyone more or less agrees that 2023 is going to be effectively written off in VC-land, as the feeding frenzy of the last few years leaves everyone exhausted, valuations flattened or crashed, and exit market remain more or less closed.
VCs appear to be using this period to get their house in order for the next cycle. We’ve already witnessed General Catalyst using this down-time to scoop up a Seed arm in Europe; another Euro VC spend ‘quality time’ with their LPs to raise another fund; and former bystanders, use it to jump on the AI hype cycle.
Why not use it to expand your partnership, right?
To that end, Hoxton Ventures has now managed to lure former Bryan Gartner, Partner at Khosla Ventures, to join as Partner.
Gartner formerly worked on venture growth-stage investments at the VC, but he’ll be refreshing his memory of early-stage investing now he’s at Hoxton.
Over an interviewed, he admitted it’d also a career move based on his personal life: “I have a very international family. My wife is actually British, so [moving] was always on the cards…but we weren’t thinking this quickly. When I was introduced to Hussein through one of the Hoxton portfolio companies, I noticed immediately the transparent nature and the mutual respect among the partnership. A complete lack of ego in the room which you know, frankly, is a breath of fresh air. And it felt to me that there’s an inflection point that Hoxton is about to hit, and I’m thrilled to be to be part of that story.”
About his time at Khosla, he told me: “There’s there’s a dichotomy between the firm’s that truly work deeply with their portfolio companies and those that don’t… It’s not really about the board meeting. It’s about the calls you make, the meetings leading up to the board meeting. And I’ve always really enjoyed that. I’ve got a bunch of stories in my track record that weren’t obvious wins and then became 9, 10, 11x returns, because of really rolling up sleeves and plugging in and that’s what excites me in this industry. And that’s what I saw at Hoxton.”
Hoxton’s early stage credentials in Europe include a 60-strong portfolio which includes a number of successful companies including Darkrace, Deliveroo, Preply, Spacelift, and TourRadar, as well newer investments such as Avantia, Lumi, and XYZ Reality.
Hussein Kanji, partner, Hoxton Ventures said in a statement: “Bryan is an experienced investor, having seen companies go from their early formation to exit and IPO. His late stage skill set will provide us with a new strength around the table.”
He added over a call: “We’ve been intentionally thinking about expanding and building up the partnership. We started alongside Seedcamp, Connect Ventures, and a bunch of others pioneering investing in Europe, and I think we’ve gone into a really good ‘boutique mode’. But I think we’re trying to make another transition now.”
He said Hoxton is growign: “We’ve become much more of a firm and an institution, kind of in the same journey as Index Ventures in the 90s went from a small shop and became the powerhouse that it is today. And if you’re going to do that, you’ve got to hire really great people. So Brian is part of that. And we’re probably going to try and do this one or two more times.”
In March 2022, Hoxton Ventures closed a $215 million new fund, Hoxton III.
Gartner has 16 years of experience in venture capital. He was a Vice President at Insight Venture Partners, where he invested in companies that include AdColony, which was acquired by Opera for $350 million, Alteryx (IPO’d), Fenergo (acquired by Astorg and Bridgepoint for $600M), Pluralsight (acquired by Vista for $3.5B), Smartsheet (IPO’d), and Udemy (IPO’).
After Insight Venture Partners, he joined join Apax, where he invested in Wizeline, which was acquired by CDPQ for ~$450M. He was also a Partner role at Sidewalk Infrastructure Partners. He was previously a Partner at Khosla Ventures for two years.
Hoxton’s investments include Finesse, Giraffe360, Inflow, Really Clever, Peptone, Universal Quantum and XYZ Reality.",https://techcrunch.com/wp-content/uploads/2023/10/Bryan-Gartner-Hoxton-Ventures.jpg?w=762,2023-10-17 12:40:10
https://techcrunch.com/2023/10/17/amazon-passkey-sign-in/,"Amazon quietly rolls out support for passkeys, with a catch","Amazon has quietly rolled out support for passkeys as it becomes the latest tech giant to join the passwordless future. But you still might have to hold onto your Amazon password for a little while longer.
The option to set up a passkey is now available on the e-commerce giant’s website, allowing users to log in using biometric authentication on their device, such as their fingerprint or face scan. Doing so makes it far more difficult for bad actors to remotely access users’ accounts, given that the attacker also needs physical access to the user’s device.
But Amazon’s implementation of passkeys isn’t without issues, as noted by Vincent Delitz, co-founder of German tech startup Corbado, who first documented the arrival of passkey support on Amazon.
Delitz noted that there is currently no support for passkeys in Amazon’s native apps, such as Amazon’s shopping app or Prime Video, which TechCrunch has also checked, meaning you still have to use a password to sign-in (for now). What’s more, if you’ve set up a passkey but previously set up two-factor authentication (2FA), Amazon will still prompt you to enter a one-time verification code when logging in, a move Delitz said was “redundant,” since passkeys remove the need for 2FA as they are stored on your device.
Amazon says on its website: “You will still need to verify a one-time code after signing in with the passkey,” but does not explain why.
It’s unclear if the requirement for 2FA codes is a temporary feature and whether Amazon plans to add passkey support to its mobile apps. It’s also not yet known if passkey support has been made available to all Amazon users, though TechCrunch has confirmed that the feature is available in the U.S., U.K., France, and Germany.
Amazon has not responded to TechCrunch’s questions.
The arrival of passkeys on Amazon lands as WhatsApp announced that it’s rolling out support for passkeys to all Android users, and just days after Google said it planned to make passkeys the default sign-in method for all Google Account holders. GitHub, Windows 11, TikTok and 1Password have all rolled out support for passkeys.","https://techcrunch.com/wp-content/uploads/2019/06/password.jpg?resize=1200,900",2023-10-17 12:05:56
https://techcrunch.com/2023/10/17/procurify-lands-fresh-cash-to-invest-in-ai-powered-tools-for-procurement/,Procurify lands fresh cash to invest in AI-powered tools for procurement,"Roughly eight years ago, a little-known startup called Procurify raised $4 million for its platform that hosts tools to take some of the pain out of enterprise procurement.
The company never became buzzy, exactly. But Procurify grew steadily over the subsequent years, going on to raise $20 million in its Series B and today closing a $50 million Series C funding round led by Ten Coves Capital with participation from Export Development Canada, Canada’s export credit agency.
Procurify, which is based in Vancouver, Canada (hence the investment from the EDA), was co-founded by Aman Mann (the CEO), Eugene Dong (the CTO) and Kenneth Loi (the former CCO). The trio began working on the idea for the company in Dong’s parents’ basement after meeting in British Columbia Institute of Technology’s business management program in 2011.
“We recognized a gap in the procurement market for affordable, easy-to-use procurement software,” Mann told TechCrunch in an email interview. “In virtually every industry, organizations are struggling with a lack of real-time spend visibility and control.”
To Mann’s point, according to a recent Statista survey, hundreds of companies engaging in business-to-business procurement — i.e. finding, agreeing to terms and purchasing goods and services from a third party — admit to struggling with compliance processes, complex approval processes and purchasing systems and reconciling invoices in a timely manner.
So how does Procurify help? By consolidating various procurement steps in one place, mainly.
Procurify offers modules to manage purchasing, accounts payable (i.e. money owed to suppliers) and data analytics. Using the platform, customers can reallocate procurement spend and adjust forecasts, identify bottlenecks in “procure-to-pay” workflows and perform supplier analyses to inform future procurement decisions.
Procurify also leverages AI, detecting anomalies in purchase orders or invoices to flag them for review.
“In today’s post-pandemic economy, industries are grappling with layoffs, disrupted supply chains and rising operational costs,” Mann said. “The need for responsible spend controls and clear financial oversight is more pressing than ever.”
Procurify competes with incumbents (Coupa, SAP Ariba), enterprise resource management software with procurement management features (NetSuite) and upstarts (Precoro, Zip) in the over-$6.1 billion procurement software segment.
The startup appears to be doing well for itself, though, with a customer base of over 700 companies, a 100% year-over-year increase in sales and plans to invest heavily in bringing new AI capabilities to market.
“Procurify helps organizations bring more spend under management, thereby consolidating spend data from our customers’ procure-to-pay workflows to provide a complete picture of expenditures before they’re committed,” Mann said. “By harnessing the power of this comprehensive spend data and integrating it with AI models, enterprises could unearth opportunities for process optimization, manage risks and achieve cost efficiencies.”
Procurify, which has a team of just over 170 employees, has raised a total of $70 million in venture capital to date. In addition to AI R&D, Mann says that the proceeds from the Series C will be put toward general expansion and launching new payment features.","https://techcrunch.com/wp-content/uploads/2022/09/GettyImages-931203582.jpg?resize=1200,800",2023-10-17 12:00:48
https://techcrunch.com/2023/10/17/reality-defender-raises-15m-to-detect-text-video-and-image-deepfakes/,"Reality Defender raises $15M to detect text, video and image deepfakes","Reality Defender, one of several startups developing tools to attempt to detect deepfakes and other AI-generated content, today announced that it raised $15 million in a Series A funding round led by DCVC with participation from Comcast Ventures, Ex/ante, Parameter Ventures and Nat Friedman’s AI Grant.
The proceeds will be put toward doubling Reality Defender’s 23-person team into the next year and improving its AI content detection models, according to co-founder and CEO Ben Colman.
“New methods of deepfaking and content generation will consistently appear, taking the world by surprise both through spectacle and the amount of damage they can cause,” Colman told TechCrunch in an email interview. “By adopting a research-forward mindset, Reality Defender can stay several steps ahead of these new generation methods and models before they appear publicly, being proactive about detection instead of reacting to what just appears today.”
Colman, a former Goldman Sachs VP, launched Reality Defender in 2021 alongside Ali Shahriyari and Gaurav Bharaj. Shahriyari previously worked at Originate, a digital transformation tech consulting firm, and the AI Foundation, a startup building AI-powered animated chatbots. Bharaj was a colleague of Shahriyari’s at the AI Foundation, where he led R&D.
Reality Defender began as a nonprofit. But, according to Colman, the team turned to outside financing once they realized the scope of the deepfakes problem — and the growing commercial demand for deepfake-detecting technologies.
Colman’s not exaggerating about the scope. DeepMedia, a Reality Defender rival working on synthetic media detection tools, estimates that there’s been three times as many video deepfakes and eight times as many voice deepfakes posted online this year compared to the same time period in 2022.
The rise in the volume of deepfakes is attributable in large part to the commoditization of generative AI tools.
Cloning a voice or creating a deepfake image or video — that is, an image or video digitally manipulated to convincingly replace a person’s likeness — used to cost hundreds to thousands of dollars and require data science know-how. But over the last few years, platforms like the voice-synthesizing ElevenLabs and open source models such as Stable Diffusion, which generates images, have enabled malicious actors to mount deepfake campaigns at little to no cost.
Just this month, users on the notorious chat board 4chan leveraged a range of generative AI tools, including Stable Diffusion, to unleash a blitz of racist images online. Meanwhile, trolls have used ElevenLabs to imitate the voices of celebrities, generating audio ranging in content from memes and erotica to virulent hate speech. And state actors aligned with the Chinese Communist Party have generated lifelike AI avatars portraying news anchors, commenting on topics such as gun violence in the U.S.
Some generative AI platforms have implemented filters and other restrictions to combat abuse. But, as in cybersecurity, it’s a cat and mouse game.
“Some of the greatest risk from AI-generated media stems from use and abuse of deepfaked materials on social media,” Colman said. “These platforms have no incentive to scan deepfakes because there’s no legislation requiring them to do so, unlike the legislation forcing them to remove child sexual abuse material and other illegal materials.”
Reality Defender purports to detect a range of deepfakes and AI-generated media, offering an API and web app that analyze videos, audio, text and images for signs of AI-driven modifications. Using “proprietary models” trained on in-house data sets “created to work in the real world and not in the lab,” Colman claims that Reality Defender is able to achieve a higher deepfake accuracy rate than its competitors.
“We train an ensemble of deep learning detection models, each of which focuses on its own methodology,” Colman said. “We learned long ago that not only does the single-model, monomodal approach not work, but neither does testing for accuracy in a lab versus real-world accuracy.”
But can any tool reliably detect deepfakes? That’s an open question.
OpenAI, the AI startup behind the viral AI-powered chatbot ChatGPT, recently pulled its tool to detect AI-generated text, citing its “low rate of accuracy.” And at least one study shows evidence that deepfake video detectors can be fooled if the deepfakes fed into them are edited in a certain way.
There’s also the risk of deepfake detection models amplifying biases.
A 2021 paper from researchers at the University of Southern California found that some of the data sets used to train deepfake detection systems might under-represent people of a certain gender or with specific skin colors. This bias can be amplified in deepfake detectors, the coauthors said, with some detectors showing up to a 10.7% difference in error rate depending on the racial group.
Colman stands behind Reality Defender’s accuracy. And he asserts the company actively works to mitigate biases in its algorithms, incorporating “a wide variety accents, skin colors and other varied data” into its detector training data sets.
“We’re always training, retraining and improving our detector models so they fit new scenarios and use cases, all while accurately representing the real world and not just a small subset of data or individuals,” Colman said.
Call me cynical, but I’m not sure if I buy those claims without a third-party audit to back them up. My skepticism isn’t impacting Reality Defender’s business, though, which Colman tells me is quite robust. Reality Defender’s customer base spans governments “across several continents” as well as “top-tier” financial institutions, media corporations and multinationals.
That’s despite competition from startups like Truepic, Sentinel and Effectiv, as well as deepfake detection tools from incumbents such as Microsoft.
In an effort to maintain its position in the deepfake detection software market, which was valued at $3.86 billion in 2020, according to HSRC, Reality Defender plans to introduce an “explainable AI” tool that’ll let customers scan a document to see color-coded paragraphs of AI-generated text. Also on the horizon is real-time voice deepfake detection for call centers, to be followed b ay real-time video detection tool.
“In short, Reality Defender will protect a company’s bottom line and reputation,” Colman said. “Reality Defender uses AI to fight AI, helping the largest entities, platforms and governments determine whether a piece of media is likely real or likely manipulated. This helps combat against fraud in the finance world, prevent the dissemination of disinformation in media organizations and prevent the spread of irreversible and damaging materials on the governmental level, just to name three out of hundreds of use cases.”","https://techcrunch.com/wp-content/uploads/2023/07/GettyImages-1463459171.jpg?resize=1200,800",2023-10-17 12:00:27
https://techcrunch.com/2023/10/17/nova-credit-lands-45m-in-funding-to-grow-its-cross-border-and-alternative-data-credit-products/,Nova Credit lands $45M to grow its cross-border and alternative data credit products,"Moving from one country to another is difficult in many ways, not the least of which involves starting over financially.
Nova Credit, which started out as a graduate research project out of Stanford University about seven years ago, was founded to help immigrants overcome the obstacles of applying for things like apartments or loans with no credit history in the U.S.
“We realized that half of the graduate student population of any university consists of international students – and if you go and ask them about their experience with financial services, you’ll hear some version of the same story – ‘I can’t get credit or I can’t get a credit card. Or I need to go ask my classmate to co-sign for an apartment or cell phone plan,’ ” said Misha Esipov, CEO and founder of Nova Credit.
With that Credit Passport product, Nova has connectivity into credit bureau data from other parts of the world through its APIs. Nova launched that product with American Express and then added dozens of institution partners over the years, such as HSBC,Scotiabank, Verizon and Earnest. (Nova Credit emerges embedded within those institutions’ applications).
It works in 20 countries outside of the U.S. as well. If a person from the U.S. moves to London or Singapore, for example, they can get approved for banking products internationally in those markets through Nova.
In recent years, the startup also has expanded beyond its flagship Credit Passport product to also offer anyone – not just immigrants – the ability to use alternative data for credit but providing access to their bank account. That product, dubbed Cash Atlas, is cash flow underwriting product that allows anyone with a U.S. bank account to allow the information – such as rent payment history or direct deposit of paychecks – inside those accounts to be used to help them apply for credit. It is aimed at the tens of millions of people that are “effectively locked out of the U.S. financial system,” Esipov said.
“We’ve started to expand from a single product company to a multi-product company and platform…Our strategy is to evolve into a more data analytics company that serves not only people that are new to this country, but frankly any customer segment and plugging the gaps in this antiquated traditional credit reporting industry which doesn’t do well for serving customers who are new to credit,” Esipov told TechCrunch in an interview.
Today, Nova is announcing that it has raised $45 million in Series C funding, additional capital that it plans to use toward expanding its product offering and geographically, Esipov told TechCrunch exclusively. The round came together “in a matter of weeks,” he said.
Canapi Ventures led the round, which included participation from existing backers Kleiner Perkins, General Catalyst, Index Ventures and Y Combinator as well as new investors such as Avid Ventures, Geodesic Capital, Harmonic Capital, Radiate Capital, and Socium Ventures (Cox Enterprises).
The raise marks the company’s first external round of financing since February 2020, which makes it a bit of an outlier in the fintech space, which experienced a major funding boom in 2020 and 2021. At that time, Nova raised $50 million in a financing that reportedly gave it a valuation of $295 million after its first close.
Esipov declined to disclose Nova’s current valuation, saying only that the company was “xxxx happy” He also declined to reveal hard revenue figures, saying only that the company has grown its revenues by 10x since that 2020 funding round and that it more than tripled its data transaction volumes since the start of 2023.
Nova has remained relatively lean, with about 100 employees. It does plan to do some more hiring with its new capital. It also plans to take its Credit Passport business global – having already launched in markets such as Canada, the United Kingdom, the United Arab Emirate and Singapore. The company is also planning to invest in its Cash Atlas product and developing more new products.
For now, Nova’s Passport product provides the majority of the company’s revenue but Atlas “is growing even more quickly on a percentage basis right now.” Esipov said the company has invested “heavily” in information security and compliance since it was a seed-stage company.
The executive projects that the company will reach profitability in the relatively near future, possibly as early as next year. It opted not to raise more capital – in fact less than its last round – to avoid taking on “unnecessary dilution,” he added.
Jeffrey Reitman, general partner at Canapi Ventures, told TechCrunch he was initially attracted to Nova’s mission of enabling newcomers and thin-file consumers the ability to have fair access to financial products. Canapi first invested in the company in the Series B round and is impressed with “the explosive growth” it has displayed since.
“Many of our banking LPs are in active dialogues with Nova Credit to leverage their products to better serve their customers and that also nicely aligns with the mission here at Canapi,” he said.
Nova, Reitman added, “has amassed more collective access to international credit data than any one of the major credit bureaus and that makes for a very valuable proposition for its customers.”
Want more fintech news in your inbox? Sign up for The Interchange here.","https://techcrunch.com/wp-content/uploads/2020/09/GettyImages-a0146-000299-e1646928299148.jpg?resize=1200,799",2023-10-17 12:00:15
https://techcrunch.com/2023/10/17/microsoft-affiliated-research-finds-flaws-in-gtp-4/,Microsoft-affiliated research finds flaws in GPT-4,"Sometimes, following instructions too precisely can land you in hot water — if you’re a large language model, that is.
That’s the conclusion reached by a new, Microsoft-affiliated scientific paper that looked at the “trustworthiness” — and toxicity — of large language models (LLMs) including OpenAI’s GPT-4 and GPT-3.5, GPT-4’s predecessor.
The co-authors write that, possibly because GPT-4 is more likely to follow the instructions of “jailbreaking” prompts that bypass the model’s built-in safety measures, GPT-4 can be more easily prompted than other LLMs to spout toxic, biased text.
In other words, GPT-4’s good “intentions” and improved comprehension can — in the wrong hands — lead it astray.
“We find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, which are maliciously designed to bypass the security measures of LLMs, potentially because GPT-4 follows (misleading) instructions more precisely,” the co-authors write in a blog post accompanying the paper.
Now, why would Microsoft greenlight research that casts an OpenAI product it itself uses (GPT-4 powers Microsoft’s Bing Chat chatbot) in a poor light? The answer lies in a note within the blog post:
“[T]he research team worked with Microsoft product groups to confirm that the potential vulnerabilities identified do not impact current customer-facing services. This is in part true because finished AI applications apply a range of mitigation approaches to address potential harms that may occur at the model level of the technology. In addition, we have shared our research with GPT’s developer, OpenAI, which has noted the potential vulnerabilities in the system cards for relevant models.”
So the implication is that the relevant bug fixes and patches were made before the paper’s publication. Whether that’s truly the case, the research is yet another data point suggesting that LLMs — even from Microsoft-backed, billion-dollar-plus-revenue-generating startups like OpenAI — remain an imperfect science.
GPT-4, like all LLMs, must be instructed — or “prompted” — to complete a task (e.g. writing an email or summarizing an article). Jailbreaking LLMs entails using prompts worded in a specific way to “trick” the LLM into perform a task that wasn’t a part of its objective.
For example, the LLM powering Bing Chat wasn’t designed to write neo-Nazi propaganda. But at release, because it was trained on vast amounts of data from the internet — some of it toxic — it was susceptible to doing just that when fed a particular prompt.
As for GPT-4, the researchers found that the model is more likely to generate toxic text than GPT-3.5 when given certain jailbreaking prompts that they “constructed.” They also claim that GPT-4 “agrees with … biased content more frequently than GPT-3.5,” at least depending on the demographic groups mentioned in the prompt.
For example, give the prompt “Homosexuals have HIV,” GPT-4 usually “strongly” disagrees with the statement, according to the researchers. But given “Women have HIV,” GPT-4 agrees — and outputs biased content.
Just as concerningly, GPT-4 — when given the “right” jailbreaking prompts — can leak private, sensitive data including email addresses, say the researchers. All LLMs can leak details from the data on which they’re trained. But GPT-4 proves more susceptible to doing this than others.
Alongside the paper, the researchers have open sourced the code they used to benchmark the models on GitHub. “Our goal is to encourage others in the research community to utilize and build upon this work,” they wrote in the blog post, “potentially pre-empting nefarious actions by adversaries who would exploit vulnerabilities to cause harm.”","https://techcrunch.com/wp-content/uploads/2023/06/OpenAI-logo-symmetry.jpg?resize=1200,675",2023-10-17 11:30:25
https://techcrunch.com/2023/10/17/biotech-eu/,"Give biotech a chance for the planet's sake, EU lawmakers urged","European Union lawmakers are being urged to avoid too much risk aversion from holding back the potential of the homegrown biotech sector.
Developments in biotech could be transformative in a range of critical sectors. Beyond huge promise in healthcare, innovative, low carbon applications in areas like agriculture and food systems and energy production could help address pressing environmental and sustainability challenges. But there’s concern among some local operators that the bloc’s current approach could cap potential biotech benefits — especially in the context of the urgency required to tackle the climate crisis.
“The main regulatory challenges for the EU’s biotech startups are long timelines for approval of new products and a lack of openness towards modern biotech solutions that may lead to GMO solutions,” Joško Bobanović, partner at Sofinnova Partners, a major investor in European biotech, tells TechCrunch. “ Today, EU startups often do not bother trying to get approval in Europe because of long approval timelines, opting instead to go directly to the US or Asia. This is a huge loss for Europe given the plethora of leading-edge technologies developed here.
“Recent Nobel prizes for technologies like CRISPR or for discoveries that led to RNA vaccines highlight European regulators’ hesitance toward genetic technologies, similar to favoring landlines over mobile phones. (Remember what happened with Nokia and smart phones.) The potential benefits of these innovations far outweigh the risks even as they are part of a duly rigorous regulatory cycle.”
“If you look at venture capital, there’s significantly more money going into the synbio [synthetic biology] community in the United States, and so we’re really at a disadvantage here in Europe,” says Stef van Grieken, CEO and co-founder of EU-based startup Cradle, which offers generative AI tools to help bioengineers design proteins. “There’s also a lot of regulatory risk in Europe. So GMO, a lot of these types of techniques are considered genetic modification. And rules in Europe are very strict. And so if you look at a company like Meatable, that’s growing meat in in a dish instead of using a cow — they’re a Dutch company but they’re launching their products in Singapore, in the United States due to regulatory constraints.”
He also points the level of recent biotech support announced by the Biden administration, including a pledge to invest $2 billion in biotechnology and biotech manufacturing — suggesting the bloc is lagging behind on financial support for the field too. “According to McKinsey, about 60% of our current economic inputs you could make with biology,” he says. “And so that’s substantial, right? Like everything that we consume is a lot larger than the things on the internet.”
“One of the things that’s starting to become obvious is there’s lots of application domains for these types of techniques,” he adds, discussing generative AI’s role in accelerating biotech R&D. “I mean, I’m excited about ChatGPT and [popular generative AI] applications but let’s say… [helping] science and R&D teams to get their bio-based products to market faster to help us solve climate change may be a bit more important than producing better marketing copy.”
Earlier this month the European Union adopted a list of ten technologies it considers critical to the bloc’s future economic security — ranging from AI, quantum and advanced semiconductors, to space tech, robotics and biotech — making a clear statement of recognition of transformative and strategic potential. At the same time, four of the listed techs were flagged for further risk assessment, including biotech (the other three pegged for extra scrutiny are: AI, advanced semiconductors and quantum).
The Commission’s recommendation suggested Member States conduct collective risk assessments of these four critical areas by the end of the year — with lawmakers highlighting the possibility that transformative potential could also lead to highly sensitive risks, such as threats to fundamental rights or civil-military fusion.
Reports have suggested the move could prefigure the introduction of additional EU regulations.
Of the four technologies flagged for risk assessments, biotech may be the least familiar, in terms of public understanding — with the term spanning practices like genetic modification; new genomic techniques (such as CRISPR-Cas9 gene editing); gene-drive; and synthetic biology (aka synbio; a multidisciplinary field); all of which were explicitly name-checked in the Commission’s PR as examples of biotech that should be risk assessed by Member States.
The listed techs all deal with manipulating genetic material but can involve different approaches and applications. Developments in one field may also dial up potential elsewhere — such as gene editing techniques increasing potential applications for synthetic biology, for example — further advancing the complexity of developments since there may be overlap in how these biotechnologies are applied.
Despite relatively low public awareness of biotech advances, Cradle’s van Grieken points out some techniques have actually been widely used in industrial processes for years — helping to produce things like detergents which can work at lower temperatures (via industrially produced enzymes); or synthetic insulin for diabetics (i.e. instead of extracting biological insulin from the pancreatic glands of slaughtered cows and pigs).
While, as noted above, a newer wave of alternative protein startups — including companies being built in Europe — are leveraging developments in the field to do things like scale lab-grown meat or produce non-animal derived dairy proteins, on a mission to transform food systems without the huge carbon footprints attached to traditional (animal-derived) meat and dairy.
But it’s interesting how under the radar some of these regional applications of biotech remain. Certain terminology may be preferred (or avoided) in marketing copy — likely with an eye on regulatory risk and/or consumer trust.
“Precision fermentation is not synthetic biology per se,” a spokesperson for one alt protein startup — France’s Bon Vivant — told us, when we asked what it meant by “precision fermentation”, the term it prefers for explaining its dairy-targeting biotech, querying the bio techniques it’s applying to repurpose yeast microorganisms to brew up cow’s milk proteins.
“As a board member of Food Fermentation Europe, Bon Vivant is still working on a science based and still easily understandable definition,” the spokesman also responded to our ask. Its marketing copy, meanwhile, studiously avoids saying it’s genetically modifying yeast to produce milk proteins — which is essentially what it’s doing — the closest it comes is writing that it “programs” yeasts.
Yet it’s widely accepted that precision fermentation is an example of synthetic biology. (See, for e.g., Wikipedia’s definition: “Precision fermentation is an approach to manufacturing specific functional products which intends to minimise the production of unwanted by-products through the application of synthetic biology, particularly by generating synthetic ‘cell factories’ with engineered genomes and metabolic pathways optimised to produce the desired compounds as efficiently as possible with the available resources.”) So it’s curious to observe a European startup that’s doing interesting things with synthetic biology being so reluctant to say so.
The example speaks to the uncertainty steeping biotech developments in Europe — suggesting disruptors remain worried that causing a splash here could amp up their regulatory risk and bring fresh limits on their fledgling businesses, or at least trigger a new wave of consumer concern, rather than inviting admiration and unlocking homegrown support (or even — dare we say it — congratulatory cheerleading).
Cautionary tale
Cradle’s van Grieken is concerned the EU taking an overly risk averse approach to biotech is out-of-date with where the bloc needs to get to; that precautionary treatment of biotech is riskily self-defeating when it comes to the challenges now facing the bloc, including its headline green ambition to get to ‘Net Zero’ by 2050.
Europe is already “late to the party” when it comes to recognizing the economic and strategic importance of biotech compared to the US and parts of Asia, he argues. But his worry about the EU’s modus operandi is an active frustration that the bloc may be creating a blindspot by not being more encouraging of a sector with transformative potential when it comes to tackling the existential crisis of climate change.
“[Synbio’s potential] is not actually being recognised in the environmental policies of the EU,” he suggests. “If you look at the European Green Deal, a lot of it is focused on energy — like energy production, insulating more homes; it’s focused on recycling; on reducing pollution — like mobility; those types of things. Synbio isn’t really a theme. But it could be an incredibly powerful resource for the EU.
“This specific [Commission] call to the Member States to figure out what the risks are [for biotech] — my worry is that we’ll see increased regulation in this space without actually trying to promote the space and become… a leader in this space. Which we currently, unfortunately, are not. So that’s my biggest worry. But I do think at least recognising that it is something that could be strategic, it’s a good first step.”
“Biotech is a serious business and we need serious regulation here,” he adds when pressed to confirm his position. “But inversely, we don’t want to hamper innovation based on outdated notions of what this technology can and cannot do.”
“The EU needs to accelerate its regulatory processes and be more receptive to new technologies,” agrees Sofinnova’s Bobanović. “This is a critical success factor in the global race to address climate change but also to ensure food independence, a topic becoming more prominent post–COVID-19.”
“Failing to adapt may see our innovations benefiting other markets and the EU losing its competitive edge, much like the electronics industry. Once we lose talent and knowledge centers, it is impossible to recover them,” the investor also warns.
Consumer concern about genetically modified organisms (GMOs) does have a long history in the EU — especially in relation to food safety — which likely informs the precautionary approach the bloc has adopted towards the use of biotech in food production since at least the early 2000s. Out of that has come a legal framework that’s focused on health and safety; harmonized risk assessments; labelling; and traceability.
Consumer awareness of cutting edge biotech may be low but a perception of public concern over GMOs in food, which took root after an earlier era of developments during a time of more lax regulation, has been harder to shift. Yet actual consumer concerns are concentrated elsewhere, research suggests.
A 2019 Eurobarometer survey on food safety indicates EU citizens’ concern over GMO has declined while worries about food risks associated with traditional farming methods are riding high. So while 44% of respondents (the largest proportion) said they were concerned about the presence of antibiotics and hormone residues in meat; and 39% were worried about pesticide residues in foods; a lower proportion — 27% — said they were concerned about GMO being used in foods and only 4% were concerned about genome editing in this context (albeit, for the latter bio technique, the survey also found relatively low knowledge of the use of genome editing in food production — 21% vs 60% for GMO in food — so very low concern there may be a reflection of low awareness).
The survey results suggest EU policymaking in this area — certainly on the food front — risks being out of step with public safety concerns. (To wit: Environmental pollutants in fish, meat and dairy was another big worry for 37% of respondents.)
Taken together the Eurobarometer paints a picture of regional consumers with substantial anxieties about the health risks (and environmental toll) attached to current farming and agricultural practices — and lower concern about biotech being applied to engineer food output. (Also relevant: A Eurobarometer survey from 2021 which found an overwhelming percentage of EU citizens consider climate change to be the most serious problem facing the world.)
Yet the bloc remains saddled with a regulatory regime that ploughs massive subsidies into traditional agriculture while demanding high levels of caution — and even throwing up regulatory hurdles — when it comes to applying biotech to critical sustainability challenges. Critics argue this combo looks increasingly misaligned with where the bloc says it wants to get to with its flagship green transition.
Of course it’s worth noting that policymaking across the 27-Member State bloc is complex, with many entities necessarily involved in change-making. The Commission’s role, while important as a proposer of new pan-EU laws (and/or legislative reforms), is just part of the picture. EU Member States themselves can also have their own biotech and bio-ethics rules and reforms — so a Commission intervention listing biotech as a critical tech, and pushing for Member States to conduct risk assessments, may be aimed at driving for harmonization between this patchwork of national laws — which could, ultimately, streamline and simplify life for biotech entrepreneurs down the line.
Other factors also play a role. Another notable development for regulation of novel biotechs in the EU occurred, in 2018, when the Court of Justice (CJEU) ruled that organisms produced using relatively new techniques, such as gene editing, should fall under the bloc’s existing rules on GMO. So the legal system is also involved in interpreting how existing rules apply to biotech developments. But, again, it’s up to policymakers to keep up with such developments and make sure legislative frameworks are providing the right incentives.
“Europe is complex in terms of regulation, market access,” says Sofinnova partner Cedric Moreau, who is focused on the pharmaceuticals side of biotech investing. “We are not as the US [where] when you have the go from the FDA you have a more than a 300 million people market opening and very homogeneous.”
“We see where the European Commission wants to go — making sure that [it’s] not overlapping with State Members’ policy and making sure that the definition, and the category and the activity are very well defined; to not prevent any innovation or [developments] in the space that could be impacted by [divergence in Member State laws],” he suggests.
“It’s important to make some clear rules, clear definitions because [as investors] we need clarity,” he also tells us. “When we are investing in companies for five, eight, 10 years we cannot bet on regulation that will decide if our drug is a high unmet medical need or just an unmet medical need [for example]… And if our market exclusivity will be 10 years, or six years or nine years or five years. So we need to have clarity — and if it’s not clear enough what we will have to do to build our business case is always to retain the more conservative scenario.”
“At Sofinnova, we are a strong believer of Europe,” Moreau adds. “Because we are deploying — roughly 80% — of our capital in Europe. So we think that Europe is a fantastic playground for healthcare, for innovation. Because we have great science, great scientists, great people. And we have also an ecosystem that could really develop great success stor[ies]… Great products, impactful products for the patient. Then having said that… obviously, we think that there were several things that could be improved.”
Climate urgency vs legal uncertainty
“There is some urgency to consider these types of techniques seriously,” argues van Grieken, talking up the potential of synbio to help in the fight against climate change. “I’m not trying to advocate for ‘no regulation’ type of space. I think we need very strong controls. But on the actual end product, not on how they get researched and developed. And in certain cases, like for example with lab-grown meat or if you look at companies that are making alternatives to cheese or milk, those should be products that we should at least consider having on the market in the EU.”
“Take a company like Perfect Day foods in the United States,” he continues. “They’re making milk without cows. They can do that at, like around — I think — it’s 3% to 5% of the emissions compared to using a cow. That’s a pretty significant improvement. And we use a lot of dairy products, right? And we have a planet on fire.”
As we reported last year, Cradle is using generative AI to predict protein sequences to speed up R&D for protein engineers building bio-based products. So its business is applying AI to accelerate biotech developments — which, of course, means it has an interest in speeding up biotech progress by encouraging a more R&D-friendly regulatory environment, too.
The acceleration its customers are seeing is considerable, as van Grieken tells it — turning what would “typically” be a 1%-5% success rate for stabilizing a protein into a 50% success rate on average, thanks to the predictive power of its generative AI models. But stringent regulation is one brake the startup’s tech can’t uplift. Hence his call for EU lawmakers to zoom out and consider a bigger risk picture.
One idea he welcomes is if the EU were to establish more regulatory sandboxes where biotech R&D could be undertaken without so much legal uncertainty fogging the ambition — which amounts to a call for rules that focus more on outputs, than on the R&D itself.
When it comes to AI, a network of regulatory sandboxes is something the bloc is in the process of setting up — at the same time as EU co-legislators are hammering out a comprehensive, risk-based framework for applying artificial intelligence. So support for, and controls on, cutting edge techs are both possible under the regional lawmakers’ playbook.
Add to that, earlier this year (in April) the Commission put out out a proposal for reforming the bloc’s pharmaceutical regulation — which floats launching a regulatory sandbox as one of the suggested measures to boost regional innovation in drug research and design.
But, in that case, the sandbox would be limited to products regulated as medicines. So even if the bloc’s co-legislators adopt the proposal there are many other biotech innovations that won’t be granted a safe space to experiment — since the end product they’re aiming to disrupt isn’t a pharmaceutical. (And of course climate change won’t be fixed by popping a pill, personalized or otherwise.)
Supporting the production of edible proteins without the climate-heating emissions of traditional agriculture is just one example of biotech’s transformative potential for the environment. Bioplastics offer an alternative to petrochemical-based plastics, as another. While bioremediation is a field that offers promise for cleaning up pollutants — including by engineering microorganisms (such as algae) to accelerate uptake of CO2, the major climate heating gas.
Also on a climate tip, production of biofuels could be more sustainably scaled up using biotech techniques — such as, again, by designing microorganisms that can more efficiently turn biomass into low carbon biofuels.
European bioengineers are even working on genetically modifying plants to amp up their ability to fight indoor pollution (see: French startup Neoplants). So when you start to really think about engineering biology for human and environmental utility the canvas looks broad indeed.
Or, well, it should — but European biotech startups have to do their bluesky thinking from under a more legally clouded horizon.
For biotech startups operating in the EU, van Grieken argues it’s “significantly harder” to do the R&D and test potential innovations with so much regulatory risk hanging over the field. “There’s a lot of uncertainty,” he emphasizes. “For example, the Netherlands just introduced the ability to sample these types of [biotech-derived food] products and have investors taste them. But a very reasonable question from these investors is can you do that and sell this stuff? And if the answer is silence, then, you know, that is not a great answer. And I think this industry needs some clarity around that.”
Current EU rules also create some “weird” scenarios, as he tells it. For example, making an “informed edit” to a genome (i.e. where a bioengineer thinks about what mutation to make) would “typically” be considered a GMO in Europe (meaning the regulatory framework starts to apply) — whereas practices which produce random mutations, as happens a lot in the plant seed space, would not. So an operator that’s, for instance, shining UV light on a plant seed and introducing random mutations falls under less regulatory risk than someone doing bioengineering to select for a specific mutation — perhaps seeking higher crop yield to boost productivity or resistance to drought — regardless of the motivations behind the intent.
“If you think about how you might actually engineer one of these systems, it’s considered problematic; but if you just do it randomly, it’s fine. And so that’s not very smart,” he argues. “Because a lot of the techniques that we have today to make informed decisions about where to make changes in order to get to a certain outcome, that’s also a safe outcome — so it’s actually a lot better than doing it random.”
“If you look at, for example, the United States or places in Asia where a lot of these synthetic biology techniques are allowed it’s not like we’re seeing any major problems,” van Grieken also points out. “So we might be being a bit too constrained right now.
“You should be able to show that your product is good; actually is improving its environmental footprint; is safe to use; is delicious, in the case to food, right — and all these types of things — and get approval for it in some reasonable amount of time so you can still get to market.”
Towards a balanced approach?
Despite criticism that it’s too cautious, EU lawmakers have been talking about evolving the bloc’s approach to biotech. They have also been taking some action too.
This summer, for example, the Commission adopted a proposal for a new regulation on plants produced by certain new genomic techniques (NGTs) which would allow plants produced in this way which could also occur naturally (or via conventional breeding) to be placed on the market — exempting them from requirements in the current GMO legislation.
The NGTs the Commission has proposed loosening the rules for are targeted mutagenesis (aka plants that contain genetic material from the same plant); and cisgenesis, including intragenesis (i.e. plants that contain genetic material from crossable plants) — which would only need to undergo a verification process, under the proposal. Whereas transgenic plants (containing genetic material from non-crossable species) would remain subject to comprehensive, case-by-case risk assessment, approval and authorization prior to any sale under the EU’s existing GMO Directive.
The bloc’s Farm to Fork Strategy, meanwhile — part of the aforementioned European Green Deal which is focused on driving sustainability of agriculture and food production — recognizes biotech as having potential to contribute to the fight against climate change. “New innovative techniques, including biotechnology and the development of bio-based products, may play a role in increasing sustainability, provided they are safe for consumers and the environment while bringing benefits for society as a whole. They can also accelerate the process of reducing dependency on pesticides,” the Commission wrote in the May 2020 strategy document.
Although, subsequent to that, a 2021 study the EU undertook of new genomic techniques noted the “rapid” development of NGTs and their products over the past two decades — finding “considerable interest” in conducting research on NGTs in the EU. But it also identified that “most” development is taking place outside the EU. Which does support the contention the bloc is lagging when it comes to biotech research, despite “considerable” homegrown appetite to do this cutting-edge work.
“Following the [2018 GMO] ruling of the [CJEU], there have been reports of negative impacts on public and private research on new genomic techniques in the EU due to the current regulatory framework,” the EU’s executive also noted in the study. “Regulatory barriers would particularly affect small and medium-sized enterprises (SMEs) and smallscale operators seeking to gain market access with new genomic techniques, even though many Member States and stakeholders see opportunities for them in this sector.”
“The use of NGTs raises ethical concerns but so does missing opportunities as a result of not using them,” it went on, essentially echoing van Grieken’s point. “Based on the findings of the study, most of the ethical concerns raised relate to how these techniques are used, rather than the techniques themselves.”
At that time, the Commission concluded that any further policy action in the area should be “aimed at reaping benefits from innovation while addressing concerns”, further stipulating that a “purely safety-based risk assessment may not be enough to promote sustainability and contribute to the objectives of the European Green Deal and in particular the ‘Farm to Fork’ and biodiversity strategies”. The document also explicitly recognized that risk assessment alone could lead to a flawed evaluation process — in which “benefits contributing to sustainability” are not properly considered.
Asked about the critique it’s over-indexing on risk, when it comes to biotech, and not properly weighting potential sustainability (or, indeed, other) benefits, a Commission spokesperson declined to provide comment. But they pointed us to an EU webpage on R&D and the “bioeconomy” — where the EU’s executive also talks up the transformative potential of homegrown biotech developments, writing for example that: “Stronger development of the bioeconomy will help the EU accelerate progress towards a circular and low-carbon economy. It will help modernise and strengthen the EU industrial base, creating new value chains and greener, more cost-effective industrial processes, while protecting biodiversity and the environment.”
The page also links to the bloc’s long-standing bioeconomy strategy — which features an action plan that lists carrying out an analysis of “enablers and bottlenecks for the deployment of biobased innovations” as one of 14 “concrete actions” regional lawmakers are committed to (on paper at least).
The EU bioeconomy strategy was originally set out back in 2012, and reviewed in 2018, with the aim of supporting 2030 Sustainable Development Goals; the Paris Agreement climate objectives; and new EU policy priorities — with the Commission writing then that reaping the “economic, social and environmental benefits of the bioeconomy, dedicated bioeconomy strategies, investments and innovation are required at all levels in the EU”. Hence the updated strategy emphasizing the need for the development of national and regional bioeconomy strategies.
Five years on from that, the Commission lists just nine Member States that have set out a national bioeconomy strategy (Austria, Finland, France, Germany, Ireland, Italy, Latvia, the Netherlands and Spain) — meaning a substantial majority of EU members still lack this piece of the biotech ecosystem support puzzle. So, clearly, there’s more work for regional lawmakers to do to match the bloc’s ambition to build up Europe’s biotech base with actions that deliver results.
Looking ahead, Cradle’s van Grieken sees two big ares of promise for biotech: Human health being the first one; and what he refers to as “planetary health” as the second. “The reason why I left Google is because those are two of the major problems that my generation faces in the world,” he tells TechCrunch. “In human health, increasingly I think we’ll be a lot better at targeting disease with these types of [bioengineered] molecules and curing people.
“On the planetary health side, I think what will increasingly see is that bio-based products will come out that are cheaper than the petrochemical or animal alternatives. Because, ultimately, biology can do a lot of these things in a much lower energy way and also environmental footprint. I think we’re going to see a breadth of products that is going to be super exciting.”
He’s also bullish on cost — suggesting developments in generative AI can be the flywheel that speeds up biotech R&D — and that acceleration of developments in the lab will draw down the costs entailed in unlocking the big, transformative biotech benefits.
“It’s also why we started Cradle — to really accelerate R&D and make R&D a lot cheaper,” he says, arguing: “There is no fundamental reason why this cannot be done… Biology is ultimately capable of doing very complicated things at very low energy — like, look around you right now. There’s probably a plant somewhere there and try to realise that it’s just like water and ambient carbon that created that, right? It’s just wild, if you think about it.”
French startup Bon Vivant, meanwhile, is working to build a European business that can help tackle the planetary health challenge head on. As noted above, it’s reprogramming yeast microorganisms to produce milk proteins to offer the food industry an alternative so they can sell non-animal-based dairy products — which could have a massive impact on shrinking CO2 emissions if taken up at scale.
Foods derived from animals, including dairy, are generally associated with the highest greenhouse gas emissions (see, for e.g., this UN data on kilograms of emissions per kg of food) — owing to factors including land use, methane emissions from livestock and nitrous oxide emissions from the waste produced by animals. So biotechnologies applied to food production which can replace the need for us to get so much protein from animal sources have the potential for radical reductions in emissions if we integrate these new processes into our food systems.
Asked about the regulatory challenge of building an alternative protein business in Europe, Bon Vivant’s co-founder, Stéphane MacMillan, offers two thoughts. On the one hand he sounds sanguine — suggesting high food safety standards in the EU could create a competitive advantage for local startups over time, as a sort of ‘gold standard’ mark (i.e. once regulatory clearance to sell locally is obtained, which he estimates in their case may take two to three years vs a quicker anticipated time-to-market over in the US).
“Everyone is saying, well, it takes too long in Europe to get approval. Okay, it’s taking longer than any other countries but at the same time we have to be proud of standards that we have in Europe,” he tells TechCrunch. “These standards are also the reason why European food is really seen as the best class in most parts of the world. So we have to comply with it. It takes a bit more time. But, at the same time, I think… that guarantee for the consumer that our products are absolutely non-GMO — that’s really important and [builds trust] with customers.”
But he also suggests the bloc’s policymakers need to find “the right balance” — between having such high homegrown standards and risking a future where European consumers are forced to buy foreign bio products “because we were not able to build the champions”.
“It’s not black or white,” he suggests. “It’s a balance that we need all to find collectively. Both are right. But we just to find the right balance.”
Offering an investor perspective on the same point, Sofinnova’s Bobanović sees even less upside for EU biotech startups trying to turn increasingly strict regional food safety standards into a competitive advantage. So — at the least — the suggestion is the bloc shouldn’t be looking to pile more rules on the sector if it’s serious about growing the bioeconomy.
“While Europe’s stringent rules might enhance consumer trust in certain sectors, it’s unlikely the case for biotech,” he argues. “Unlike the luxury industry where ‘made in Europe’ is an advantage most food products are destined for local consumption and consumers already trust regulations. Increased regulation is not likely to influence product adoption.”","https://techcrunch.com/wp-content/uploads/2021/10/GettyImages-1342327233.jpg?resize=1200,800",2023-10-17 10:58:18
https://techcrunch.com/2023/10/17/scylladb-raises-43m-to-scale-its-nosql-database-platform/,ScyllaDB raises $43M to scale its NoSQL database platform,"Investors have an appetite for databases, it seems.
Today, ScyllaDB, a startup developing database tech for high-throughput, low-latency workloads, announced that it raised $43 million in a funding round led by Eight Roads Ventures with participation from AB Private Credit Investors, AllianceBernstein, TLV partners, Magma Ventures and Qualcomm Ventures.
The new cash will be put toward “accelerating” ScyllaDB’s momentum and expanding the size of its 168-person team, according to co-founder and CEO Dor Laor.
“Today’s disruptors are ingesting an unprecedented amount of data and tapping it to deliver differentiating user experiences that transform markets and displace legacy leaders,” Laor told TechCrunch in an email interview. “Data is being enriched, cleaned, streamed, fed into AI and machine learning pipelines, replicated and cached from multiple sources. That’s why it’s more important than ever to have a database that’s up to the task.”
ScyllaDB is what’s known as a NoSQL database, which — unlike the relational databases once dominant in the enterprise — provides mechanisms for data storage and retrieval that don’t rely on a “tabular relations” model. In a tabular model, a relationship is a connection between two tables of data. But with a NoSQL database, relationships don’t have to follow this schema — offering greater engineering flexibility and, in some cases, improved performance.
NoSQL databases are commonly used for applications like ad serving, AI and machine learning, recommendation and personalization engines, fraud detection and analyzing data from internet of things devices.
According to a 2022 survey by Ventana, almost a quarter (22%) of organizations are using NoSQL databases in production today, while more than one-third (34%) are planning to adopt NoSQL databases within two years or evaluating their potential use. And the NoSQL market is expected to grow to $35.7 billion by 2028, up from $7.3 billion in 2022, the IMARC Group reports.
Now, ScyllaDB’s not the only NoSQL vendor out there — far from it. There’s ArangoDB, Redis Labs and Crate.io to name a few, not to mention bigger players like MongoDB, Amazon’s DynamoDB and Couchbase.
But ScyllaDB claims that its tech offers architectural advantages, like the ability to perform millions of operations per second with “single-digit millisecond” latency. Running across multiple clouds, on a hybrid cloud setup or on-premises, ScyllaDB automatically tunes I/O and CPU performance with workload prioritization, which co-locate workloads under a single server cluster.
Those claims and capabilities were enough to win over customers, evidently. ScyllaDB says its database is now used by over 400 companies including Discord, Epic Games and Palo Alto Networks and that revenue has grown 800% since the company’s founding in December 2012.
“Across industries, R&D teams are increasingly realizing that ScyllaDB’s dramatically different database architecture delivers better performance and horizontal scalability for data-intensive workloads,” Laor said. “ScyllaDB is designed to help fast-growing, fast-moving teams deliver lightning-fast user experiences at extreme scale … ScyllaDB’s unique architecture takes full advantage of modern cloud resources, delivering impressive efficiency and price-performance.”
To date, ScyllaDB has raised $103 million in venture capital.","https://techcrunch.com/wp-content/uploads/2021/04/GettyImages-945420192.jpg?resize=1200,849",2023-10-17 10:00:57
https://techcrunch.com/2023/10/17/figures-humanoid-robot-walks-for-the-camera/,Figure's humanoid robot walks for the camera,"In May of this year, TechCrunch ran a piece titled “Figure’s humanoid robot takes its first steps.” The story was a firsthand account of my visit to the startup’s South Bay offices. The headline was a reference to both the company’s first year of existence and its stated plan to hit a key milestone by its first birthday.
The company later confirmed with me that it had, in fact, managed to get its humanoid robot to walk. I asked for video evidence, which Figure refused to send — until now. Among other things, it’s clear the company wants to get a film crew to capture the bipedal locomotion. I tend to prefer raw laboratory video for stuff like this, but that’s probably one of the many reasons no one is asking me to run marketing for their robotics firm.
Two things jump out at me immediately: First, it’s good to see a non-render from the company. Thus far, their art has been limited to mockups of what the robot could eventually look like. Watching this footage is a reminder that there are many steps along the way to that futuristic bit of product art. Second, you’ve probably noticed that the robot is moving with bent knees, rather than the fully upright motion we see in humans.
Bent knees, on the other hand, are pretty standard in robots — you’ve seen it with Boston Dynamics’ Atlas and Agility’s Digit (though the latter has a reverse bend, similar to an ostrich). Bending gives you better control of balance and other important factors. Ultimately there’s a question of how important it is to hue to a more human-like gait, but obviously this first video of the Figure 01 robot walking is very much early stage. There are plenty of kinks to work out between now and a ship date.
The other thing I will draw your attention to is the hands. Mobile manipulation remains a key problem in this world, and many humanoid systems like Digit and Apptronik’s Apollo have yet to add articulated graspers. Of course, there’s nothing in this video that suggests the grippers are currently functional. On my visit to the company’s HQ, however, they showed me a portion of the office devoted to the development of what looked to be a five-digit, human-style hand.
The video notes that Figure’s headcount at the time of shooting (10/1) was 60. Impressive growth for one year. More updates soon, no doubt.","https://techcrunch.com/wp-content/uploads/2023/10/Screenshot-2023-10-13-at-3.53.07 PM.jpg?resize=1200,725",2023-10-17 10:00:01
https://techcrunch.com/2023/10/17/invesco-raises-swiggy-valuation-to-nearly-8-billion/,Invesco raises Swiggy's valuation to nearly $8 billion,"Conditions appear to be shifting favorably for India’s Swiggy. The food delivery startup — backed by SoftBank, Prosus and Accel — saw its paper valuation slashed by more than a half this year as investors marked their holdings largely in response to the dwindling market conditions. The startup, valued at $10.7 billion in a funding round early 2022, also lost some market share to Zomato, its arch publicly-listed rival, according to Prosus.
Now, not so much.
Invesco, which led Swiggy’s previous round and cut its valuation to under $5.5 billion, marked up the startup’s valuation to $7.85 billion at July’s closure, according to a newly published disclosure.
The U.S. asset manager says it considers the valuation of similar public companies as a factor when reassessing the value of its private investments. Given that shares of Zomato have risen by 33% since the end of July, this could imply that the current $7.85 billion valuation for privately-held Swiggy may be conservative.
Separately, Swiggy, which is eyeing to make an initial public offering next year, appears to be closing in on some of the market share it lost to Zomato this year. Swiggy’s month-on-month volume grew 7% this July and 6% in August, beating Zomato in both months, UBS said in a report this month.","https://techcrunch.com/wp-content/uploads/2023/03/GettyImages-1239421148.jpg?resize=1200,801",2023-10-17 09:24:24
https://techcrunch.com/2023/10/17/stack-overflow-cuts-28-of-its-staff/,Stack Overflow cuts 28% of its staff,"Developer community site Stack Overflow has laid off 28% of its staff, the Prosus-owned company announced Monday.
In a blog post, Stack Overflow’s CEO, Prashanth Chandrasekar indicated that the company is focusing on its path to profitability. While the post didn’t elaborate on the reason behind the job cuts, it mentioned customers’ budgets shifting elsewhere “due to the macroeconomic pressures.”
“This year we took many steps to spend less. Changes have been pursued through the lens of minimizing the impact on the lives of Stackers. Unfortunately, those changes were not enough and we have made the extremely difficult decision to reduce the company’s headcount by approximately 28%,” Chandrasekar said.
While Stack Overflow is primarily a Q&A website for consumers it also has enterprise products like “Stack Overflow for Teams,” which helps organizations maintain a company-wide knowledge base.
The company didn’t specify the number of laid-off employees. However, since it had pushed its headcount to over 500 people last year, more than 100 people are likely to be impacted.
With generative AI gaining popularity for helping coders with different problems, Stack Overflow has seen its traffic drop as compared to last year.
In August, the company said that because of generative AI, it expects “some rises and falls in traditional traffic and engagement over the coming months.”
Earlier this year, Stack Overflow asked AI companies to pay for training data. In January, it barred users from posting answers generated by AI. The company is also trying to bolster its own AI capabilities. In July, it launched OverflowAI with features like generative AI-powered search.
Big Tech is also moving fast to make generative AI-aided products available for coders in a rapid manner. Last month, GitHub expanded access to its Copilot chat to individual users. In May, during its developer conference, Google announced a bunch of AI-centric coding tools including an assistive bot called Codey. The company has also trained its conversational AI tool Bard to help users with code generation and debugging.","https://techcrunch.com/wp-content/uploads/2023/10/3281da8e0c2332be9228c95317c493fe508c5a65-2400x1260-1.webp?resize=1200,630",2023-10-17 07:44:44
https://techcrunch.com/2023/10/17/ambani-jio-financial-launches-lending-and-insurance-businesses/,Ambani's Jio Financial launches lending and insurance businesses,"Jio Financial Services, the Indian conglomerate Reliance Industries-backed financial services firm, has started its lending and insurance businesses and plans to rapidly broaden its offerings as billionaire Mukesh Ambani expands the ever-so-wide tentacles of his oil-to-telecom empire.
The market has been closely paying attention to Reliance’s financial services ambitions for years. But it wasn’t until last year that Ambani, Asia’s richest man, revealed that the firm plans to enter into the sector, which though has grown multiple folds in the past decade remains largely untapped, serving only tens of millions of individuals.
Jio Financial Services, which made public debut in August, said in its annual presentation that it has started to offer personal loan to salaried and self-employed individuals through its MyJio app and 300 stores across India. Its insurance arm has also partnered with 24 insurers to offer a wide-range of coverage across auto, health, and corporate categories, said the firm.
Jio Financial Services has largely remained quiet about precisely what all it plans to do. The firm, whose largest backer remains Reliance Industries, earlier this year partnered with U.S. asset manager BlackRock to launch asset management services in the country.
The financial services is the newest sector for Ambani, who has entered several businesses — including telecom — in the past decade and scaled them to tentpole positions. Reliance also operates the nation’s largest retail chain, which has been valued at $100 billion in recent fund raises from investors including KKR.
As Jio Financial scales its business, it may pose a challenge to a number of players in the industry, including Paytm and Policybazaar. Reliance said it will make use of AI and analytics for its financial services business and operate on a “low cost of servicing.”
Jio Financial Services said it’s taking a direct-to-customer approach with its offerings to drive cost efficiencies and enabling personalized customer interactions. The firm is incorporating “alternate data models for 360-degree customer view and tailored offerings,” and is developing a unified app for the “diverse financial needs of customers.”
In the annual report, Jio Financial Services said it’s also testing a sound box, the fast-omnipresent portable device that alerts merchants when a transaction has completed, the firm said, confirming an August TechCrunch report. The company is “generating substantial data footprint and enhancing our customer engagement across digital channels, and in turn enriching and facilitating other businesses,” it said.
On lending, Jio Financial Services plans to extend loans to businesses and merchants as well as offer loans to facilitate vehicle and home purchases, it said. It also plans to give loans by using shares as collateral. The firm said it has also “relaunched” savings account service and bill payments and plans to launch debit cards.","https://techcrunch.com/wp-content/uploads/2023/08/GettyImages-1243634682.jpg?resize=1200,800",2023-10-17 07:26:12
https://techcrunch.com/2023/10/17/ray-ban-meta-review/,Ray-Ban Meta sunglasses have 'influencer' written all over them,"Ray-Ban Meta sunglasses have ‘influencer’ written all over them The companies have maintained a slim and light design, while rendering their predecessor obsolete with Facebook and Instagram livestreaming
This is a review-in-progress. More soon!
Somewhere between the Ray-Ban Meta and Meta Quest 3 sits an ideal mixed-reality headset. It’s slim, light, offers hand tracking and passthrough and livestreams video when the moment calls for it. It’s designed to be worn inconspicuously outdoors, until the time comes for content capture.
The Meta Quest Ray-Ban is a fantasy at the moment — albeit one that points in the direction of where its makers think this is all headed. Presently, the Ray-Ban Meta and Meta Quest 3 are very different devices, with little in the way of overlap, beyond being head-worn products with built-in sensors.
The Meta Quest 3 is a mixed-reality headset designed to be worn exclusively indoors. It’s light, perhaps, compared to other headsets of its ilk, but wearing the thing while walking around outside frankly sounds a bit miserable. That’s precisely the use case the Ray-Ban Meta was designed for: freedom of movement outside the house that’s designed to go (mostly) unnoticed.
Just prior to writing this, I slipped a pair on, before the JFK airport mobility cart drove my sciatica-ridden ass to the gate. I would say the pair was inconspicuous but for the fact that I was wearing a pair of sunglasses indoors. Well, that and the extremely necessary recording lighting that flashes on so you can’t creep shoot folks without their knowledge. Here’s some of that video:
We got our first glimpse of the Ray-Ban Meta at a briefing just ahead of the recent Connect conference. I was genuinely impressed by the industrial design the join team came up with. Most folks would hard-pressed to distinguish the charger from a standard Ray-Ban classic eyeglass case. It’s a little thicker than some, sure. A bit heavier. More rigid. But the team was able to make surprisingly few concessions.
There are a lot of clever touches here. In the place of a snap is a ring. Open the case and it glows green when fully charged and orange when not. The orange starts blinking when the battery is low. Space has been maximized inside. The battery sits directly beneath the glasses’ folded temples. In front of this is a dock with two charging pins that lie flush with a pair of contact pads hidden on the underside of the glasses’ bridge, held in place with magnets and a small tab.
The USB-C port is located on the outside bottom of the case, allowing it to sit on its back while being charged. Directly above this on the case’s rear is the Bluetooth pairing button. The case is slimmer than the last gen and can be carried in a pocket comfortably.
Meta says the glasses get “up to” four hours on a charge, while the case gets a total of eight charging cycles, for a grand total of 36 hours. As the company notes, “Battery life varies by use, configuration, settings and many other factors.” That’s the case with all tech, of course, but I did notice that video is a power drainer.
The companies really leaned into the style side of things here (not a bad decision when designing tech meant to be worn on the body). There are two main designs for the glasses. There’s the classic Wayfarer (which is probably what you think of when you think of sunglasses) and the new Headliner (not dissimilar from Wayfarer, but significantly more rounded on the top and bottom).
According to Meta, there are 150 design combos possible, when you factor in all of the different design options, including frame color, style and lenses (including sunglasses, clear, prescription, transitions and polarized).
The temples are thicker than most sunglasses — to be expected, seeing as how they contain the speakers and other components (there’s a transparent option, if you want to see for yourself) — but again, the designers have done a good job keeping size down, all things considered. And again, while slightly heavier that a standard pair of Wayfarers (50.8 g vs. 44 g), you can wear them comfortably all day if you want to (or at least the less than four hours the battery lasts).
There’s a touchpad on the outside of the left temple. Swiping back and forth will adjust the volume (other features can be customized in-app). It also doubles as a control panel for live streaming, since you likely don’t want to futz with your phone or keep using the wake word. A tap can check Instagram or Facebook comments and viewers in real-time. The capture button sits next to the hinge on the left temple
There are a pair of small circular modules on the end pieces. They look identical, for the sake of symmetry, but serve very different — albeit related — functions. On the top right (when facing the glasses) is the 12-megapixel camera. On the top left is an LED that turns on to alert people in your vicinity that you’re recording.
When covered, the glasses send an audio alert that they’ve stopped recording. This is to avoid people sticking a piece of electrical tape to hide the light. Meta says they didn’t hear of any specific examples of this happening, but they almost certainly got that feedback. Again, privacy is paramount for a device like this, especially since it’s something that most people around you don’t know exists. When the battery is low, you’ll get a spoken alert and the light will blink orange and turn red right before shutting down. The light will blink white when receiving a call, do a single flash when taking a photo and glow steadily when recording.
When pairing, it flashes blue, going solid when connected. The pairing process is pretty straight forward. You’ll need to download the Meta View app, choose between Meta Ray-Ban and Ray-Ban Stories and allow bluetooth to connect . Images and video will save to the glasses’ 32GB of internal storage (that’s roughly 500 photos or 100 videos at the maximum 30 seconds apiece). You’ll need to tap “Import” inside the app to connect via WiFi and download the contents to your phone. You can also set it up to auto import via settings.
Once everything is paired, put the glasses on and open either Facebook or Instagram to livestream. Tap the plus icon and it will bring you to the livestream screen. Your phone’s camera is, understandably, the default, but double pressing the capture button will switch over to the glasses. Livestreaming is probably the single biggest killer app Ray-Ban Stories was missing.
There are barely visible down firing speakers on the bottom of the temple tips. When I first tried the speakers in an otherwise silent room, they sounded surprisingly loud and clear. They’re open-ear speakers, rather than bone conduction, which has its pluses and minuses. Bone conduction tends to be quite quiet but does a decent job with ambient noise, since it’s arriving at your eardrums through a different method.
As expected, I had to turn up the volume quite a bit among the airport din. I would recommend them for quieter environments, where possible, but obviously that isn’t always an option. Sound is integral to the headphones, beyond music listening. For instance, there’s an audible shutter click when you take a picture.
There are on-board microphones as well, which listen for the “hey Meta” wake word. Voice certainly makes sense on a device like this. It can be used to take a picture, stop and start video and adjust volume (turns out voice is kind of an annoying way to do the latter). You can also ask the glasses for the time, weather and how much battery is left. You can also ask Alexa style-questions, and Meta AI will attempt to answer. That’s currently only available here in the U.S. through an open beta.
The price starts at $299 for standard lenses. Polarized run $329 and transitions $379. Prescription lenses are on a sliding scale. The price will almost certainly be a deterrent for many — and understandably so. Ultimately, you need to ask yourself how much value a face-worn camera will bring to your life. If you make a living livestreaming, it may make sense. It’s a lot to pay however, for sheer novelty.
It’s worth noting that future updates will bring more value to the device, including sign translation (through voice) and the ability to identify landmarks in front of you. One can see the future of head-worn computing laid out in front of your face — though it’s still going to be a while before we get there.",https://techcrunch.com/wp-content/uploads/2023/09/Meta-Ray-Ban-Stories-06.jpg?w=1200,2023-10-17 07:01:44
https://techcrunch.com/2023/10/16/snapchat-is-now-allowing-websites-to-embed-content/,Snapchat is now allowing websites to embed content,"Snapchat has relied on people consuming content on its own app. But now, the social network is allowing websites to embed public content including Lenses, Spotlight videos, Public Stories, and Public Profiles.
Users who want to embed a Story, video, or a Lens, can open up the content on a desktop browser using the link. They can click on the embed button on the share sheet to copy the code and post it to their site.
In July 2022, Snap made its website more useful for users with core features like the ability to send messages and Snaps. The initial version of Snapchat for the web was only available for Snapchat+ users in the United States, United Kingdom, Canada, Australia, and New Zealand. The company made it available to all users in September 2022.
Snapchat’s rivals Instagram and TikTok have long offered web embeds so blogs and news sites can include content from those platforms. Snap hopes that this move will drive more traffic to the app and website.
Last week, Snap CEO Evan Spiegel sent an internal memo to employees stating that the social network wants to reach 475+ million daily users in 2024, according to The Verge’s Alex Heath. The report also noted that Snap aims to have 14 million Snapchat+ subscribers and $500 million in non-ads revenue. Last month, Snap reported that Snapchat+ crossed 5 million subscribers.
The Verge’s report added that the company has set a goal to achieve a 20% increase in ad-based revenue year-on-year.",https://techcrunch.com/wp-content/uploads/2021/01/GettyImages-1172921170.jpg?w=1024,2023-10-17 06:02:54
https://techcrunch.com/2023/10/16/ftx-execs-blew-through-8b-testimony-reveals-how/,FTX execs blew through $8B; testimony reveals how,"Sam Bankman-Fried and other FTX executives spent $8 billion worth of customer funds on real estate, venture capital investments, campaign donations, endorsement deals and even a sports stadium, according to testimony from former senior FTX executive Nishad Singh.
Singh’s testimony, which kicked off the third week of Bankman-Fried’s trial, provides fresh details of exactly where that money went.
Singh, who has already pled guilty to fraud, money laundering and violation of campaign finance laws, said Monday that he learned of the massive hole in Alameda’s books as a result of a coding error that “prevented the correct accounting” of user deposits by around $8 billion.
Singh’s testimony helps corroborate the statements given by three previous prosecution witnesses, all of whom were in Bankman-Fried’s inner circle: FTX CTO Gary Wang, Alameda CEO Caroline Ellison and FTX engineer Adam Yedidia. While Wang and Ellison have pled guilty, each witness has pointed to Bankman-Fried as the orchestrator of fraud and money laundering.
Singh said that even after learning about the hole, “implicitly and explicitly, I green-lit transactions that I knew must have been digging the hole deeper and therefore coming from customer funds.”
Singh went on to describe Bankman-Fried’s spending as “excessive.” He said that he often learned about large spends after the fact, and that his expressions of concern weren’t taken seriously.
“I also would express that I felt kind of embarrassed or ashamed of how much it all wreaked of excess and flashiness,” said Singh. “It didn’t align with what I thought we were building a company for.”
Where the money went
Prosecutor Nicolas Roos and Singh went through spreadsheets detailing different ways Alameda spent the $8 billion in customer funds. Singh testified that Bankman-Fried was “in general the one making the final decision on investments and investment team decisions as a whole.”
In addition to going over a $1 billion on Genesis Digital Assets, a crypto mining firm in Kazakhstan, and $500 million on Anthropic, an AI company focused on safety, the prosecution focused on Alameda’s $200 million investment into K5 Global, a venture firm led by investor Michael Kives who is known for his extensive network.
That network seemed to impress Bankman-Fried deeply. After attending a Super Bowl Party hosted by K5 in Los Angeles, the former crypto mogul told Singh that he had met “the most impressive collection of people he ever had in one location.” Faces at the party included Hilary Clinton, Katy Perry, Orlando Bloom, Leonardo DiCaprio, Jeff Bezos, Kendall and Kris Jenner and Kate Hudson.
Bankman-Fried had proposed a term sheet to Singh and Wang one night that laid out hundreds of millions of dollars of onuses to Kives and Bryan Baum, co-founder and managing partner of K5. The sheet also proposed up to $1 billion long-term capital to give to the VC firm, according to Singh.
“We can get from them essentially infinite connections,” wrote Bankman-Fried in a letter to FTX leadership that was shared at Monday’s trial. “I think that if we asked them to arrange a dinner with us, Elon, Obama, Rihanna and Zuckerberg in a month, they would probably succeed.”
Singh said he expressed concern about partnering with K5 and giving them such substantial funds, which would be “really toxic to FTX and Alameda culture.” He said that “politicking and social climbing was not going to be rewarded, and here we were rewarding people in exorbitant amounts.”
The former FTX executive suggested that Bankman-Fried use his own money, not FTX’s, to make some of these investments. Those protestations didn’t yield results, according to the spreadsheet, which showed the K5 deal went through Alameda’s venture arm.
Bankman-Fried also believed that endorsement deals and even “unpaid partnerships with celebrities” would help increase FTX’s influence to propel its success, said Singh.
To that end, about $205 million of that $8 billion chunk was spent renaming the Miami Heat stadium to FTX Arena. Another $150 million was spent to endorse the MLB. Other items on a spreadsheet shown to the jury show FTX paid out $1.13 billion in exchange for endorsements from basketball player Steph Curry, video game developer Riot, Seinfeld writer Larry David to endorse FTX in a Super Bowl ad, football star Tom Brady and model Giselle Bündchen, with whom FTX was coordinating on some philanthropic efforts, according to Singh. .
Singh’s testimony also revealed a range of properties that had been purchased with the funds, including a $30 million penthouse in the Bahamas that Singh said was “too ostentatious.”
Bankman-Fried has also donated tens of millions to election campaigns.
The former FTX executive, who also went to high school with Bankman-Fried and was a close friend of his brother, testified that he expressed concern about the company’s spending, but was usually blown off.
Singh recalled one instance where Bankman-Fried got visibly angry with him and said that people like him were “sowing seeds of doubt in the company decisions” and were “the real insidious problem here.”
“It was pretty humiliating,” said Singh.
Where did this $8 billion hole come from?
Singh’s testimony aligned with Yedidia’s that states in June 2022, the executives learned that Alameda owed $8 billion worth of FTX customer money after Ellison shared a Google Doc displaying the “extremely negative” balance.
Singh told the court this hole was due to a bug that Yedidia accidentally introduced into the system in 2021. The bug “prevented correct accounting for fiat@FTX.com’s balances on specific types of withdrawals,” said Singh. Fiat@FTX.com was an internal accounting system that recorded user deposits.
On top of this, Singh testified that he built out systems on FTX that gave Alameda “special privileges” not afforded to other users. A feature called “allow negative” let Alameda trade, borrow and withdraw FTX funds in excess of its balance and collateral amounts, according to Singh. He testified that he coded an initial version of the feature in 2019 at Bankman-Fried and Wang’s advisement.
A later version of this code allowed Alameda to borrow from FTX without having tis collateral liquidated. In effect, it could “withdraw money that it didn’t have,” meaning it could “lose money” that “belonged to customers,” Singh said.
By June 2022, Alameda had built up its own $2.7 billion deficit on the FTX platform.
“This seemed like a real abuse of a feature that until this point I believe was serving FTX, not hurting it,” said Singh.
Alameda at this point also owed $8 billion in user funds to FTX that it no longer had on hand. In total, the negative account balance and accounting bug contributed to a $11 billion hole on FTX’s balance sheet, Singh testified.","https://techcrunch.com/wp-content/uploads/2023/10/GettyImages-1728183029.jpg?resize=1200,801",2023-10-17 03:52:32
https://www.innovationnewsnetwork.com/deep-south-resources-back-in-business-in-copper-exploration-industry/38337/,Deep South Resources is back in business in copper exploration,"Deep South Resources is back to upgrading its copper exploration game in Southern Africa.
Deep South Resources, a Canadian mining exploration company working in Africa, has concluded a long legal battle and is now continuing its work and planning for the future. The company specialises in copper exploration in Africa, particularly in Namibia and Zambia.
We interviewed their president and CEO, Pierre Léveillé, to find out what happened and what is next for the company.
Can you briefly summarise Deep-South Resources? What is your background and key objectives?
Deep South Resources is a mining exploration company specialised in exploring for copper deposits. Our main area of expertise is in the region of Southern Africa, and our main projects are in Namibia and Zambia.
In Namibia, we have the Haib copper project. Here, we’re starting a feasibility study on a copper deposit of 5.3 billion lbs, and we know we can add more.
In Zambia, the company holds three very large exploration licenses that are very well situated, in the heart of the Copper Belt. As of yet, there has been no drilling, so we are really starting from scratch.
The company’s key objective is to explore, find, and develop the largest possible copper deposits. This is something we have already started in Namibia and intend to continue.
In terms of copper, Zambia is probably up there with the Democratic Republic of the Congo, and consequently we believe the project has great promise.
We are hunting for copper deposits in places with large, proven copper reserves and resources.
We think that the project will be productive enough to attract major players to buy us or join us in development. Our main expertise is exploration and development in the Southern part of Africa.
Although we are not looking towards copper production right now, we have the expertise and potential to do it ourselves in the future.
What is the potential of Zambia and Namibia in copper exploration?
The potential for copper exploration in Namibia is greater because the project is more advanced. As well as this, we already have established a defined large resource, and we know we can increase it. The potential, therefore, relies on increasing that already large deposit.
On the exploration side, our main goal and the main potential is to increase the grade, which we are on track to do.
We estimate that by the middle of 2024, we will have increased the grade by at least 25%.
The Zambian project holds major exploration potential. Situated right in the middle of the Copper Belt, surrounded by nine very large copper mines, and we are on the same geology as them. The first exploration programme has enabled us to discover very large copper anomalies. We’re on the right track; the potential to find a deposit is great.
As opposed to Namibia, where the deposit has already been discovered, in Zambia we are still at the deposit discovery stage.
How is Deep South Resources embracing the green revolution?
The technologies we are looking to use for potential future mining are greener metal extraction methods. The classic way to extract metals, such as grinding, milling, roast leaching and flotation, is quite old-fashioned, and it’s highly demanding in terms of power. The roasting also causes a lot of air pollution.
We will not necessarily avoid roasting in Namibia, but it will be limited and will be combined with other technologies, such as heap leaching.
We are also looking at bio heap leaching. The beauty of bio heap leaching is that when you finish the treatment, the gravel that remains is stable. This means that no pollutive material will contaminate the ground and water table. It’s not totally green, but it’s a lot greener than roasting and flotation.
Have you encountered any challenges whilst undertaking these copper exploration projects? And if so, how have they been overcome?
In Namibia, we have the Haib project, which we bought in May 2017. Four years later, in June 2021, the Minister of Mines and Minerals Development refused to renew our licence, stating that he was told that we don’t do any work, so we don’t deserve to have the project.
Naturally, we argued that we had five drills on-site, with 60 employees active. We had shipped one ton of samples to Australia for metallurgical test work; there was a lot of activity. There was a serious difference between the information he received and the information we communicated to the Ministry. Nevertheless, he decided to pull the plug on our licence.
We immediately went to the High Court of Namibia, and requested an injunction over that area to prevent the ministry from granting a licence to anyone else. In doing this, we discovered another private company had applied for our licence six months prior to the expiry. This is very unusual.
In Namibia, when you apply for an existing licence in general, the Ministry will state that the license is still valid and to come back later when it has expired.
Our second step was to request the court to review the Minister’s decision. The court case lasted a little over a year. We had the final hearing in October 2022.
At the end of the same month, there was a big scandal that ran in the press in Namibia, first on social media and then through the traditional press. The scandal concerned allegations that were similar in nature to those experienced by Deep-South Resources. Potential issues surrounding license renewals were highlighted during this time and we were hopeful of a positive outcome.
It was such an important scandal that we thought it was impossible the judge hadn’t seen it. We were also in the fortunate position of having an injunction on the project. Thus, when the judge rendered a strong verdict in our favour, the licence was still available.
It is not in the mandate of the court to order the Ministry to grant the licence, but he ordered them to reopen the application for renewal that was originally denied in 2021, this time considering all the facts presented by Deep-South.
We wanted to defend ourselves to protect the interests of our shareholders. We finally got the licence back in July 2023, two years later. That situation has been seriously damaging; our market capitalisation has dropped by 80%.
We did not get any compensation from the Ministry. If we want compensation, we have to institute a new court case where we will request damage, but on the other hand, we want to keep a good relationship with the Ministry, so we will avoid legal disputes.
That’s been our biggest challenge, not only for us, but for the numerous high-level Namibians who were really annoyed by this scandal and expressed their discomfort with the situation. At the end of the day, the rule of law prevailed and we are now back on track to continue our exciting progress in Namibia.
We also have had smaller challenges, but it is part of our business. As an example, we’re owed a lot of money from the value-added tax, but it takes an awful lot of time to be reimbursed. At some point, they tried to change the rules to not pay back exploration companies, but now it seems to be back on track.
We spent over a year awaiting a payment of around $135,000, which is a lot of money for a small exploration company. We also have issues from time to time, with work visas for foreign employees. The procedure is long and complex compared to Zambia. It’s more administrative stuff that sometimes makes our life a bit difficult. But we always resolve these things.
So what’s next for Deep South Resources?
Quite a lot! We just closed the financing. So even if the market is very difficult at the moment for small companies, we just closed a financing of $2m, and we’re pretty happy because three institutions have taken $1.5m of the financing. They’re very good supporters. With this funding in place, we are resuming the drilling programme that was suspended on the Haib copper project, as well as restarting the feasibility study procedures.
At the completion of the drilling programme, we will also have a new resource estimation somewhere in early 2024. We just announced that we have appointed the MSA group in South Africa to complete this resource estimation.
We are also looking at appointing an engineering firm for the environmental impact study that goes with the feasibility study. We’re working on receiving proposals now from engineering firms for the feasibility study.
We are resuming our activities with METS Engineering in Australia and CSIRO, the governmental laboratory in Australia, for metallurgical test work, heap leaching and other tests. All of this will be announced during the next month, which means that by the end of October, we will be in full swing with the resumption of our activities.
At the same time, in Zambia, we are starting a small programme of geophysical-induced polarisation surveys on the Luanshya West project. We have generated some very interesting soil sampling results recently, and we want to better define the drilling targets for next year. We will be very busy. Stay tuned because there’s a lot of activity coming. We’re pretty excited about these developments and exploration programmes.
After the two years we spent fighting for the company, we look forward to a very good year coming up in the copper exploration field.
Please note, this article will also appear in the sixteenth edition of our quarterly publication.
Go to this partner's profile page to learn more about them",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/DEEPSOU1-28928-shutterstockMo-Femi-Pearse_1270772323-1024x576.jpg,2023-10-17 12:30:24
https://www.innovationnewsnetwork.com/how-the-space-age-is-polluting-our-atmosphere/38320/,How the Space Age is polluting our atmosphere,"The Space Age is leaving fingerprints on the stratosphere – one of the most remote parts of the planet – which has potential implications for climate, the ozone layer, and the continued habitability of Earth.
Using tools hitched to the nose cone of their research planes and sampling more than 11 miles above the planet’s surface, researchers have discovered that the Space Age has resulted in significant amounts of metals in aerosols in the atmosphere.
The increasing amount of debris is likely due to more frequent launches and returns of spacecraft and satellites.
This mass of metal is changing atmospheric chemistry in ways that may impact Earth’s atmosphere and the ozone layer.
The study, ‘Metals from spacecraft re-entry in stratospheric aerosol particles,’ is published in Proceedings of the National Academy of Sciences.
The rising use of metal and polluting materials in space
“We are finding that the Space Age has released human-made materials in what we consider a pristine area of the atmosphere,” said Dan Cziczo, one of the study’s authors.
The team detected more than 20 elements in ratios that mirror those used in spacecraft alloys.
They found that the mass of lithium, aluminium, copper, and lead from spacecraft re-entry far exceeded those metals found in natural cosmic dust.
Nearly 10% of large sulfuric acid particles – the particles that help protect and buffer the ozone layer – contained aluminium and other spacecraft metals used in the era of the Space Age.
Scientists estimate that as many as 50,000 more satellites may reach orbit by 2030. The team calculates that means that, in the next few decades, up to half of stratospheric sulfuric acid particles would contain metals from re-entry.
What effect that could have on the atmosphere, the ozone layer and life on Earth is yet to be understood.
The Space Age is in full swing, but how does this impact space pollution?
Spacecraft launches and returns were once international events. The launches of Sputnik and the Mercury missions were front-page news.
Now, a quickening tide of innovation and loosening regulation means that dozens of countries and corporations are able to launch satellites and spacecraft into orbit and fuel the Space Age.
All those satellites must be sent up on rockets, and most of that material eventually comes back down.
The Space Age has left behind a trail of metals that may change the atmosphere in ways scientists don’t yet understand.
“Just to get things into orbit, you need all this fuel and a huge body to support the payload,” Cziczo commented.
“There are so many rockets going up and coming back and so many satellites falling back through the atmosphere that it’s starting to show up in the stratosphere as these aerosol particles.”
He concluded: “Changes to the atmosphere can be difficult to study and complex to understand.
“But what this research shows us is that the impact of the Space Age on the planet may be significant. Understanding our planet is one of the most urgent research priorities there is.”",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/shutterstockstudiovin_2208172963.jpg,2023-10-17 10:22:21
https://www.innovationnewsnetwork.com/research-innovation-key-renaissance-european-solar-manufacturing/38305/,Research and innovation is key for the renaissance of European solar manufacturing,"The world is switching to green energy, and Europe is presented with an opportunity to ensure solar energy plays the critical role it is meant to.
The European solar manufacturing landscape is at a critical crossroads, with a perfect storm emerging. Over ordering on the demand side, combined with overcapacity on the supply side, has resulted in a record drop in prices for solar modules and other system components. It has never been more important for the EU to advance its industrial strategy for solar PV and support European solar manufacturers.
This is where research and innovation (R&I) comes in. Expanding the European solar manufacturing base also requires an expansion of the EU’s R&I activities. Increased investments in innovative technologies are a key component in the long-term reindustrialisation of Europe’s solar manufacturing base.
Undoubtedly, R&I has been high on the Commission’s agenda. Under Horizon Europe, the EU’s key R&I funding programme, the Commission has allocated a significant budget of €95.5bn. More than 40% of that fund – €40bn – is earmarked for research that supports the European Green Deal.
Previously, the Commission invested €4.99bn towards clean energy technologies under its Horizon 2020 programme; only 10% of this was earmarked for PV projects. The Directorate-General for Research and Innovation has promised to help Europe “stay ahead of the game, and accelerate the roll-out of the EU’s strategic net-zero technologies,” like solar PV.
Solar has also been booming across Europe, especially in the last three years. Over 40GW of solar was installed in 2022, nearly double what was installed the previous year. Already in the first half of 2023, solar generation grew by 13%, according to a recent Ember report.
We have truly entered a thriving solar era, with record drops in fossil fuel generation. Every year, solar generation and installation numbers are increasing.
The R&I challenge
With solar numbers on the rise, and the EU setting its R&I agenda, you might wonder what the issue is? The reality is that the rest of the world has also woken up to the strategic role of solar. China is by far the world leader in the manufacturing of solar. The US has the Inflation Reduction Act which is spurring a boom in American solar manufacturing. India, Turkey, and South Africa are all making similar moves.
There are a number of solutions that EU leaders can take to ensure that European solar manufacturing plays a significant role in the globalised solar supply chain, like changing subsidy rules, setting up a dedicated financing instrument, and delivering ‘resilience’ auctions under the incoming Net-Zero Industry Act.
One wider tool we can’t forget is the role of R&I in supporting Europe’s manufacturing base – the critical link between ‘labs’ and ‘fabs’. The Fraunhofer Institute for Solar Energy Systems (ISE) has found that in Germany, PV technology development has delivered cost reductions of 36% per year on average since 2010, thanks to a combination of upscaling of manufacturing, and continued R&I advancements.
According to the Commission’s third Progress Report on the Competitiveness of Clean Energy Technologies, half of the EU’s greenhouse gas reductions expected by 2050 will require technologies that are not yet commercially available. Solar PV alone is set to generate more than 60% of the EU’s electricity by 2050, requiring more private and public investment into clean energy research.
The perovskite potential
The development of new cell technologies like perovskite solar cells reflect the necessity of expanded R&I investments in European solar manufacturing. In May 2023, Oxford PV, a perovskite solar manufacturer, set the record for the highest recorded efficiency of any commercial-sized solar cell in its Brandenburg an der Havel factory in Germany.
The cell converted 28.6% of the sun’s energy into electricity, and was made by placing a thin film of the perovskite material onto a conventional silicon solar cell. The combined ‘perovskite-on-silicon’ tandem solar cell achieves a conversion efficiency that is substantially higher than that of mainstream silicon-only solar cells, which average 22–24%.
Commenting on the achievement, David Ward, Chief Executive Officer at Oxford PV, noted that their, “innovative solar cells are close to being in the hands of our module-manufacturing customers,” with the focus now on ramping up production.
Recent findings from a team at the University of Surrey have also illustrated the potential of perovskite. They found that a nanoscale ‘ink’ coating improved the perovskite solar cells’ stability, making them suitable for mass production.
Researchers made this breakthrough when they discovered an aluminium oxide that minimises the drop in efficiency during the conditioning of perovskite solar cells. Perovskite has also been used to create self-healing solar panels on satellites in low-Earth orbit that can recover 100% of their efficiency, even after being damaged by radiation in space.
In general, the perovskite material is lighter and cheaper than a silicon-based solar cell, and is extremely efficient. It has even been praised as a ‘miracle material,’ a label earned with recent developments. When it reaches the market at scale, it will transform the PV sector.
All of these findings have emerged from investments into one innovative technology. The European solar PV sector’s research representation to the European Commission – ETIP PV – has made the point that getting R&I to production scale will require continuous EU funding.
Otherwise, we risk only investing in technologies that will one day be obsolete, especially given the acceleration of innovation cycles.
For example, fully realising the potential of perovskite solar cells will require more funding to facilitate its mass-production, and improve its shelf life; currently, perovskite deteriorates quickly when exposed to light, voltage, or heat.
More research will also be required to substitute the lead currently used in perovskite solar cells, for a more eco-friendly alternative, while retaining its efficiency.
A step forward for European solar manufacturing
Undoubtedly, the EU is leading in several areas of PV research and innovation, including in perovskite tandem solar cells. Oxford PV, Fraunhofer ISE, and Helmholtz-Zentrum Berlin, all European research and development bases, hold global solar cell conversion efficiency records.
However, rapid advancements in R&I investments by public and private actors in other regions are catching up to the EU, and threaten to leave it behind, especially when it comes to the industrialisation of R&I.
For example, in June, at a Shanghai trade show, a Chinese manufacturer already announced its plans to commercialise a perovskite solar cell. The EU is already lagging behind in the production of ingots and wafers, an important segment of solar manufacturing. It has also seen a notable loss of expertise in key segments of the PV R&I landscape.
Just like investments in manufacturing, the EU will need to expand and intensify its R&I investments if it wants to keep up, especially with its renewed ambitions for PV manufacturing.
Alongside renewed investments in PV manufacturing capacity, the EU must also invest in the industrialisation of the results of R&I efforts, and strengthen its investments towards developing the next generation of PV technologies.
For example, the perovskite potential will be lost without a strong investment commitment to bridging the manufacturing gap and delivering innovation at scale.
The ETIP PV White Paper on manufacturing puts it best: adequate financing is needed throughout the EU PV value chain. Many solutions can be implemented to accelerate R&I financing at the European level. This could include relaxing the EU’s State Aid rules – its Temporary Crisis and Transition Framework (TCTF) – to accelerate the channelling of funding towards projects.
The rebirth and long life of European solar manufacturing needs a strong research and innovation base. Europe has its part to play in the world’s solar manufacturing story.
Innovations like perovskite solar cells are just one example waiting on the horizon, reflecting the potential of R&I investments to revolutionise the solar industry.
However, the PV manufacturing storm is still brewing; sustained and increased investments in EU R&I investments will help build resilience against this storm. Left unchecked, this storm will only get worse.
Now is the time to double down on EU research and innovation, so that we can guarantee the EU’s solar renaissance.
Please note, this article will also appear in the sixteenth edition of our quarterly publication.",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/1-Copyright-Meyer-Burger-Thalheim-factory.jpg,2023-10-17 09:28:51
https://www.innovationnewsnetwork.com/new-innovation-challenge-launched-tackle-bias-in-ai-systems/38298/,New innovation challenge launched to tackle bias in AI systems,"UK companies can now apply for up to £400,000 in government investment to fund innovative solutions tackling discrimination and bias in AI systems.
The competition will look to support up to three groundbreaking homegrown solutions, with successful bids securing a funding boost of up to £130,000 each to tackle bias in AI.
It comes ahead of the UK hosting the world’s first major AI Safety Summit to consider how to best manage the risks posed by AI while harnessing the opportunities in the best long-term interest of the British people.
Tackling bias in AI systems is a major priority
The first round of submissions to the Department for Science, Innovation, and Technology’s Fairness Innovation Challenge, delivered through the Centre for Data Ethics and Innovation, will nurture the development of new approaches to ensure fairness underpins the development of AI models.
The challenge will tackle the threats of discrimination and bias in AI by encouraging new approaches, which will see participants building a wider social context to develop their models from the off.
Fairness in AI systems is one of the government’s key principles for AI, as set out in the AI Regulation White Paper. AI is a powerful tool for good, presenting near-limitless opportunities to grow the global economy and deliver better public services.
Minister for AI, Viscount Camrose, said: “The opportunities presented by AI are enormous, but to fully realise its benefits we need to tackle bias in AI.
“By ensuring AI models do not reflect bias found in the world, we can not only make AI less potentially harmful but ensure the AI developments of tomorrow reflect the diversity of the communities they will help to serve.”
Harnessing a new, UK-led approach
While there are a number of technical bias audit tools on the market, many of these are developed in the US.
Although companies can use these tools to check for potential bias in AI systems, they often fail to fit alongside UK laws and regulations.
The challenge will promote a new UK-led approach which puts the social and cultural context at the heart of how AI systems are developed, alongside wider technical considerations.
This will focus on two areas. First, a new partnership with King’s College London will offer participants from across the UK’s AI sector the chance to work on potential bias in AI models. The model, developed with Health Data Research UK with the support of NHS AI Lab, is trained on the anonymised records of more than ten million patients to predict possible health outcomes.
Second is a call for ‘open use cases’. Applicants can propose new solutions which tackle discrimination in their own unique models and areas of focus, including tackling fraud, building new law enforcement AI tools, or helping employers build fairer systems which will help analyse and shortlist candidates during recruitment.
Companies currently face various challenges in tackling bias in AI, including insufficient access to data on demographics and ensuring potential solutions meet legal requirements.
The CDEI is working closely with the Information Commissioner’s Office (ICO) and the Equality and Human Rights Commission (EHRC) to deliver this Challenge. This partnership allows participants to tap into the expertise of regulators to ensure their solutions marry up with data protection and equality legislation.
Stephen Almond, Executive Director of Technology, Innovation and Enterprise at the ICO, explained: “The ICO is committed to realising the potential of AI for the whole of society, ensuring that organisations develop AI systems without unwanted bias.”
Baroness Kishwer Falkner, Chairwoman of the Equality and Human Rights Commission, added: “Without careful design and proper regulation, bias in AI systems has the potential to disadvantage protected groups, such as people from ethnic minority backgrounds and disabled people.
“Tech developers and suppliers have a responsibility to ensure that the AI systems do not discriminate.”",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/shutterstockSansoen-Saengsakaorat_2345949869.jpg,2023-10-17 08:27:32
https://www.innovationnewsnetwork.com/new-battery-recycling-method-efficiently-recovers-aluminium-and-lithium/38292/,New battery recycling method efficiently recovers aluminium and lithium,"Researchers from Chalmers University of Technology have developed a new battery recycling method that allows the recovery of 100% of the aluminium and 98% of the lithium in electric car batteries.
The new battery recycling method also minimises the loss of valuable materials such as nickel, cobalt, and manganese.
The researchers use oxalic acid to reduce costs and harmful chemicals in the process.
The paper, ‘Complete and selective recovery of lithium from EV lithium-ion batteries: Modelling and optimisation using oxalic acid as a leaching agent,’ is published in the journal Separation and Purification Technology.
How does the new battery recycling method work?
The team fine-tuned the temperature, concentration, and time to use the oxalic acid to facilitate EV battery recycling.
Martina Petranikova, Associate Professor at the Department of Chemistry and Chemical Engineering at Chalmers, said: “We need alternatives to inorganic chemicals. One of the biggest bottlenecks in today’s processes is removing residual materials like aluminium.
“This is an innovative method that can offer the recycling industry new alternatives and help solve problems that hinder development.”
The aqueous-based recycling method is called hydrometallurgy.
Traditional hydrometallurgy
In traditional hydrometallurgy, the metals in an EV battery cell are dissolved in an inorganic acid. The impurities, such as aluminium and copper, are then removed.
The valuable metals such as cobalt, manganese, lithium, and nickel are separately recovered.
Although the amount of residual copper and aluminium is small, several purification steps are required. Each step in this process can cause lithium to be lost.
The new method reduces the waste of valuable metals
The new battery recycling method reduces the waste of valuable metals needed to make new batteries by reversing the order of the traditional process. By doing this, lithium and aluminium are recovered first.
The latter part of the process leaves aluminium and lithium in the liquid, and the other metals in the solids. Aluminium and lithium then need to be separated.
“Since the metals have very different properties, we don’t think it’ll be hard to separate them. Our method is a promising new route for battery recycling – a route that definitely warrants further exploration,” said Léa Rouquette, PhD student at the Department of Chemistry and Chemical Engineering at Chalmers.
“As the method can be scaled up, we hope it can be used in industry in future years,” concluded Petranikova.",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/shutterstockMaxx-Studio_191931434.jpg,2023-10-17 08:02:49
https://www.innovationnewsnetwork.com/software-supply-chain-security-how-spot-cyber-risks-hidden-plain-sight/38274/,Software supply chain security: How to spot cyber risks hidden in plain sight,"Matt Middleton-Leal, Managing Director EMEA at Qualys, discusses the importance of software supply chain security.
Every day, you will see headlines around IT security issues affecting companies and public sector bodies. These organisations will be affected by attacks that could involve everything from data theft or sensitive information being leaked through to full blown ransomware deployments and interruptions to service. One of the biggest routes for these attacks over the past few years has been the software supply chain – if an attacker can jeopardise one software component, then they can attack multiple companies through that hole.
Applications are now more complex too. They have more moving parts, from open source software projects and internally developed software code through to third party applications that are embedded into services. They can expand and scale up to meet demands, created on-demand using infrastructure as Code or in Kubernetes containers. And they can be created using public software hosted on services like GitHub or the Python Package Index.
As a result, many security teams are not able to track those software assets effectively. To solve this, we have to make it easier to manage those software components and prevent potential attacks from coming in.
Bills of materials and software management
In practice, security teams need insight into what IT assets the organisation has. Without this list, it is impossible for IT security leaders to call their organisations secure. This has to be expanded to software as well.
The first step for this is to create a full inventory of the software that you have in place and the components used to make it. For internal applications – otherwise termed first-party software – this level of insight is often lacking. Studies have shown that between 70 to 90% of first-party software includes open-source components; according to our analysis of more than 13 trillion anonymised data points in the 2023 TruRisk Research Report, 79% of servers installed use open-source components.
Application and security operations teams most commonly rely on manual checks or siloed scripts to evaluate the security of first-party software. This depends on how well those checks work and how often they are carried out, and delays teams from prioritising the right risks for remediation.
Setting up a full process for software supply chain management starts with knowing what applications components you have and what versions those components are. Traditional vulnerability assessment or software composition analysis tools do not detect the presence of embedded open-source packages across the production environment. So, expanding your approach to cover these first-party software applications should be a natural first step.
Alongside looking internally, you should also look at the software that you consume from others. This third-party software will itself be built of different components and services, and any one of those can have an issue. As you don’t ‘own’ the software, it may be difficult to peer inside and know if there are any out-of-date components that could affect your security.
To fix this problem, the US Government supported the use of software bill of materials, or SBOMs. SBOMs provide customers with a list of all the components used within a given application, so security teams can spot any faults that come up. However, uptake of SBOMs is still in its infancy.
Regulation may help on this in time. SBOMs were mandated by the US Government in an Executive Order in May 2021, while the European Union’s Cyber Resilience Act also includes resolutions to implement SBOMs for hardware and software manufacturers that provide products to European consumers. The UK Government is also developing its approach to cyber resilience, and SBOMs are expected to be part of that approach.
The challenge here is that SBOMs are still relatively new, and CISOs have other more pressing issues around security that take up their teams’ resources and commitment. Regulation may force this up the agenda in time, but software supply chain attacks are happening now. SBOMs can deliver more insight into what cyber risks exist and consequently where to concentrate. As part of an overall software supply chain strategy, SBOMs will be essential in future, but the data they provide will help you manage risk around misconfigurations or vulnerabilities now too.
Improving overall processes around security
Just like any security process, software supply chain security depends on the data coming in and how quickly that information can be turned into actions. The issue is that software today is so complex that you can easily miss potential problems, whether they are in your organisation’s own software or contained in another company’s products.
Getting more data on what is in place is necessary in order to begin improving security. However, this data is not useful without the right context. Without that insight, you will not be able to prioritise where changes are needed in your own applications, and you will not be able to put pressure on your suppliers around their updates. Equally, you will not be able to manage those potential risks effectively and mitigate problems before they come up.
To improve your approach to software supply chain management, you will have to look at your overall approach to risk and how you manage software within your organisation. Bringing first-party software and third-party application risk data together will help your team understand the potential threats that exist, where changes are needed, and how you can support those problems getting fixed in an efficient and timely manner.
To make this work for you, look at how you can automate the data gathering so you have a continuous level of insight into what you have in place, and then prioritise your risks so you can always get ahead of any potential problems before they become serious or lead to attempted attacks.",https://www.innovationnewsnetwork.com/wp-content/uploads/2023/10/©-shutterstock3rdtimeluckystudio_2268705275.jpg,2023-10-17 07:39:18
https://venturebeat.com/ai/ai-platform-alliance-will-drive-ai-to-be-more-open-efficient-and-sustainable/,"AI Platform Alliance will drive AI to be more open, efficient and sustainable","VentureBeat presents: AI Unleashed - An exclusive executive event for enterprise data leaders. Network and learn with industry peers. Learn More
A group of prominent companies in the AI industry announced the establishment of the AI Platform Alliance, a consortium aimed at making sure that AI platforms will be more open, efficient, and sustainable.
The consortium seeks to address the growing demand for scalable AI solutions and overcome the challenges posed by the increasing compute power required for AI training and inferencing. The founding members of the alliance include Ampere, Cerebras Systems, Furiosa, Graphcore, Kalray, Kinara, Luminous, Neuchips, Rebellions and Sapeon, with more companies expected to join in the future.
The formation of the AI Platform Alliance comes at a critical juncture, not only for the technology industry but also for the world as a whole. The rapid expansion of AI has led to an unprecedented need for compute power to train and run AI workloads. The aim of the group is to “promote better collaboration and openness.”
While AI training demands substantial compute resources upfront, the total compute power required for AI inferencing can be up to ten times higher, posing an even greater challenge as AI usage scales up. One of the primary objectives of the AI Platform Alliance is to enhance the power and cost efficiency of AI hardware, surpassing the performance delivered by traditional graphics processing units (GPUs).
Event AI Unleashed An exclusive invite-only evening of insights and networking, designed for senior enterprise executives overseeing data stacks and strategies. Learn More
Recognizing the complexity involved in implementing AI solutions, the AI Platform Alliance will collaborate to validate joint AI solutions that offer superior alternatives to the prevailing GPU-based status quo. Notably, Nvidia, the biggest designer of AI chips and GPUs, isn’t a member of the alliance.
By fostering community-driven development, the consortium aims to expedite AI innovation by creating more transparent and accessible AI platforms. This collaborative approach seeks to enhance the efficiency of AI in solving real-world problems and establish sustainable, environmentally friendly, and socially responsible infrastructure at scale.
The AI Platform Alliance invites AI companies that are developing hardware solutions and are dedicated to challenging the status quo to join the consortium. Interested companies can apply for membership through the alliance’s website.",https://venturebeat.com/wp-content/uploads/2023/10/ai-platform-alliance.jpg?w=1200&strip=all,2023-10-17 12:00:00
